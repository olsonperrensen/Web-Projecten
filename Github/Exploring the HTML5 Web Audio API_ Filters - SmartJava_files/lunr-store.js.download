var store = [{
        "title": "First steps with Mule 3.0",
        "excerpt":"Been a while since I looked at the new versions of Mule, so took some time this weekend to look at the new version of Mule. After a rather unexpected small download of only 66 MB, getting Mule up and running has stayed as simple as it was in the previous versions. For me, that is on linux, it was just extracting the tar.gz, set MULE_HOME and run ./bin/mule. From clicking the download to having a running ESB within a minute.   So what exactly is new to Mule 3 since the previouse versions. Mulesoft provides a nice overview on their website (http://blogs.mulesoft.org/say-hello-to-mule-3/), but for me personally I especially like the following three new features:     Deployment model: One of the biggest gripes people had with previous versions of Mule, was the lack of support for hot deployment. In this version hot deployment is now supported.    Annotations: With annotations it's been made easier to create your own custom service components. It allows you to define part of your service component's configuration using annotations, and makes it a lot easier to define transformers.    Flow: Another issue I've heard a lot of complaints about was that for complex message flows, the use of services with explicit inbound and outbound phases wasn't the most flexible and easy to use way of defining message flows. In Mule 3, the concept of Flows is introduces which makes working with large and complex message flows a lot easier.   Let’s look somewhat closer at these three new features.   Deployment model In older Mule versions you were forces to restart the Mule container whenever you made changes to your routing configuration. Running multiple mule instances in a distributed manner could help you in minimizing the impact of your change to a single mule configuration, but it always felt a little bit forced. With this new deployment model we can just simply create a folder in the $MULE_HOME/apps directory and add our libs and configuration file. Mule will pick up this file and startup your application. If you make changes to your config file, Mule will also see this and reload this configuration and libraries. You can also zip up the complete structure and add it to the apps directory.   You can see this when you start Mule:    INFO  2010-10-03 17:00:07,974 [WrapperListener_start_runner] org.mule.module.launcher.DeploymentService: Application directory check interval: 5000   Deploying one of Mule’s own supplied examples, is thus as easy as just copying the zip to the apps directory and done.   The required format:   /  \\- classes                 // application-specific expanded resources (e.g. logging configuration files, properties, etc  |- lib                     // application-specific jars  |- mule-config.xml         // Main Mule configuration file, also monitored for changes  |- mule-deploy.properties  // Application deployment descriptor (optional)  |- mule-app.properties     // custom properties to be added to the registry instance used by the application (optional)    No more restarting Mule and specifying the configuration you want to use. A nice addition is the “anchor” you get when an application is deployed. If you remove the anchor, Mule will cleanly remove your deployed application. So after deploying the “hello” example, I can instantly use it without having to shutdown and startup Mule.   So basically we can just keep the Mule instances running and add additional applications or flows as we please. This is also very handy when your creating your flow, you can just add the components and simply build up your integration scenario. Note though that for this you still need to make sure you as a developer think about how you deal with your classes and cleaning up of resources. Even though Mule has an excellent hot deployment model, careless use of resources, classes, statics etc. can still lead to OutOfMemory and PermGen errors.   Annotations The best part with regards to annotations is that they make it very easy to follow the standard dependency injection patterns we’re used to from Spring and Guice. This time however we won’t inject beans into one another, but we can inject the message’s payload, message headers, attachments or by using XPath and Groovy even select part of the incoming payload to be injected. There isn’t much documentation and examples on this specific subject to be found yet, so let me show you how easy this is to do. Lets first look at how to configure a transformer:  public class MyTransformers {   @Transformer public String reverseTransformer(String content) { \treturn StringUtils.reverse(content); } }   Just by annotating this method with the @Transformer annotation will register this method automatically. Not that exciting I agree, but much better then extending some base class. Next up, some of the other annotations. First one we'll look at is the @Payload one. With this annotation you can implicitely tell Mule to transform the incoming payload to the type you've specified.  &lt;code lang=\"java\"&gt; package org.smartjava.mule.tryout;  import org.mule.api.annotations.param.Payload;  public class SimpleGreeter { \t     public Object process(@Payload String content) {     \tSystem.out.println(content);     \treturn content;     } }   So when an message is received by this component, Mule will lookup a transformer to make sure the content is of type String. Oh, should you want to test this for yourself, I made the simplest Mule config to test this with:               \t     \t\t     \t\t     \t     \t \t\t \t   By the way, if you don't want to use the Mule IDE, but still want to debug your flows without having to redeploy and add remote debugging. Mule still provides the old way of starting in embedded mode:    org.mule.MuleServer.main(new String[]{“-config”,”./config/mule-config.xml”});   So, on to the next one. The @function annotation. This annotation allows you to quickly inject the result from some simple functions. For example you could get the current time. On the function overview page (http://www.mulesoft.org/documentation/display/MULE3USER/Function+Annotation) you'll find a list of the possibilities. If we for instance want to get a timestamp and a counter as parameter to our component method. We can just change this method (from the previous example) to this: &lt;code lang=\"java\"&gt;     public Object process(@Payload String content, @Function(\"date\") Date date, @Function(\"count\") Long count) {     \tSystem.out.println(content);     \tSystem.out.println(date);     \tSystem.out.println(count);     \treturn content;     }   Note that, though I use these annotations on the component, you can just as easily use these in the transformer example I showed earlier. With the @Function annotation you can avoid having to create these helper methods for yourself, and they give you easy access to some generic functions. There are a lot of other annotations (http://www.mulesoft.org/documentation/display/MULE3USER/Annotations), I’d just like to show you one more, or better said two more: the @InboundHeaders and the @OutboundHeaders   When writing transformers or the more technical components, you often need access to message headers. You could do this by making your transformers and components aware of the mule context, or you could use these two annotations:      public Object process(@Payload String content, @InboundHeaders(\"*\") Map headers) {     \tfor (Object header : headers.keySet()) { \t\t\tSystem.out.println(header + \":\" + headers.get(header));   \t} \treturn content; }   In this one, we get instant access to all the headers (because of the \"*\" wildcard). If we want to add some extra headers, or overwrite existing ones, we can use the @OutboundHeaders annotation. &lt;code lang=\"java\"&gt;     public Object process(@Payload String content, @OutboundHeaders Map&lt;String, Object&gt;  outHeaders) {     \toutHeaders.put(\"aNewHeaderKey\", \"aNewValue\");     \treturn content;     }   Well that was a short example of how to use some of the annotations provided by Mule. With these annotations it’s much easier to keep your own custom code nice and clean. So far we’ve quickly looked at how hot-deploy works in Mule, how you can use annotations.   Flow The last item I’d like to quickly touch upon is the new flow based configuration. In the previous mule-config example, you’ve already seen an example of this. Basically what it is, is a shorthand way of wiring a set of components. It provides a basic pipeline where the output of one transport or component is used as input for the next one. Lets quickly look back at the mule-config I showed earlier:               \t     \t\t     \t\t     \t     \t \t \t   Here you can see that we do the following things:  1. Read from a directory 2. Pass the message to our own custom component 3. Write the output to a file  Easy as that, and compared with the 'old' way of configuration very easy. To show you just what a change this is, I'll show, as a nice wrap up to this short introduction to Mule 3, the old style of configuration for the same scenario: &lt;code lang=\"xml\"&gt; &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;mule xmlns=\"http://www.mulesoft.org/schema/mule/core\"       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"       xmlns:spring=\"http://www.springframework.org/schema/beans\"       xmlns:file=\"http://www.mulesoft.org/schema/mule/file\"     xsi:schemaLocation=\"        http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd        http://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/3.0/mule.xsd        http://www.mulesoft.org/schema/mule/file http://www.mulesoft.org/schema/mule/file/3.0/mule-file.xsd\"&gt;          &lt;model name=\"simpeFlowModel\"&gt;     \t&lt;service name=\"fileIn\"&gt;     \t\t&lt;inbound&gt;     \t\t\t&lt;file:inbound-endpoint path=\"./myInputDirectory\" &gt;     \t\t\t\t&lt;file:file-to-string-transformer/&gt;     \t\t\t\t&lt;file:filename-wildcard-filter pattern=\"*.xml\"/&gt;     \t\t\t&lt;/file:inbound-endpoint&gt;     \t\t&lt;/inbound&gt;     \t\t&lt;component class=\"org.smartjava.mule.tryout.SimpleGreeter\"/&gt;     \t\t&lt;outbound&gt;     \t\t\t&lt;pass-through-router&gt;     \t\t\t\t&lt;file:outbound-endpoint path=\"./myOutputDirectory\"/&gt;     \t\t\t&lt;/pass-through-router&gt;     \t\t&lt;/outbound&gt;     \t&lt;/service&gt;     &lt;/model&gt;     &lt;/mule&gt;    Even in such a simple configuration you can see the advantages of using the flow model. Using the flow model (http://www.mulesoft.org/documentation/display/MULE3USER/Flows) or the patterns based approach (http://www.mulesoft.org/documentation/display/MULE3USER/Configuration+Patterns), should basically be enough for almost all scenarios. If not you can always fallback to the old service based model.   So that’s it for this short introduction into Mule 3. I personally think that with these improvements Mule has once again proved to be one of the best (if not the best) opensource ESBs. I especially like the continuing focus on making integration as easy as possible for the developer.  ","categories": ["posts"],
        "tags": [],
        "url": "http://www.smartjava.org/content/first-steps-mule-30/",
        "teaser":null},{
        "title": "Mule and WSO2 Registry integration",
        "excerpt":"In this two part article we’ll look at how we can use the WSO2 registry (http://wso2.com/products/governance-registry/) as the service repository for services provided by the Mule ESB (http://www.mulesoft.org/). In this first part we’ll show how Mule can register its services with the WSO2 registry using WS-Discovery, in the second article we’ll look at how other clients can search the WSO2 registry and consume the services. For this article, as for the second one, all the sources can be downloaded. Just look at the bottom of this article for the links.   So let’s get started. For this to work and for you to test with, we need to do a couple of things:    Install WSO2 Registry and Mule Create a simple service that we expose through Mule Create a simple client that can register services with WSO2 Hook into the lifecycle of Mule so that the services are registered on startup   First things first, lets install these two applications.   Install WS02 Registry and Mule Installing the registry and the ESB is pretty easy. You can find the download for the WSO2 registry here (http://wso2.org/downloads/governance-registry) and for Mule here (http://www.mulesoft.org/download-mule-esb-community-edition). Both come packaged in various formats which can just extract into the directory of your choice. Now we’ll start these applications to see if they are installed correctly. For Mule you can use the following command: /bin/mule.bat, and for the registry: /bin/wso2server.bat. If you're not on windows there is also a /bin/mule and a /bin/wso2serversh which you can use.   Should there be any problems with the startup, the easiest thing to do is look at the information on the websites of these products. They both have extensive documentation and that should be able to solve and installation issues quickly.   Creating the simple service The simple service we’ll use is just a simple Hello Service as seen in most examples. We’ll create a java interface and an implementation class which we’ll expose using Mule’s CXF support. The interface for this service is shown below:   package manning.governance.wsdiscovery;  import javax.jws.WebService;  @WebService public interface HelloWorld { \t     @WebResult(name=\"response\")     String sayHi(@WebParam(name=\"hello\") String text); }   As you can see from this interface we’ve defined a very basic request/reply webservice, which just responds with an hello. The implementation is also very straightforward.   package manning.governance.wsdiscovery;  import javax.jws.WebService;  @WebService(endpointInterface = \"manning.governance.wsdiscovery.HelloWorld\") public class HelloWorldImpl implements HelloWorld {      public String sayHello(String text) {         return \"Hello \" + text;     } }   Now that we’ve created a jax-ws webservice, lets expose this service using Mule. For this we create a simple flow (a Mule 3 construct):   \t&lt;flow name=\"simpleFileFlow\"&gt;  \t\t&lt;inbound-endpoint address=\"http://localhost:18080/hello\" /&gt; \t    \t&lt;cxf:jaxws-service serviceClass=\"manning.governance.wsdiscovery.HelloWorldImpl\"/&gt;     \t\t&lt;component class=\"manning.governance.wsdiscovery.HelloWorldImpl\" /&gt;  \t&lt;/flow&gt;  If you start Mule with this configuration you’ll have this webservice running on http://localhost:18080/hello. You can now use SOAP-UI or any other tool you use for testing a webservice to invoke the operation.   Creating the WS-Discovery client That was the easy part, the next thing we need to do is find a way to register services with WSO2 SOA Registry. For this we can use a WS-* standard named WS-Discovery (http://docs.oasis-open.org/ws-dd/discovery/1.1/pr-01/wsdd-discovery-1.1-spec-pr-01.html). With WS-Discovery it’s possible to register services in a standard compliant way and it’s also possible to query this same repository. This latter part is something I’ll show in the second part of the article.   Before we dive into the creation of a WS-Discovery client, lets first look at how WS-Discovery works. In WS-Discovery you’ve got a simple set of operations you can use to register and search services. Basically the four services exposed are:    Hello: Register a service to a WS-Discovery server Bye: Deregister a service from a WS-Discovery server Probe: You can use a probe message to search for services with specific properties or simply return all the registered services Resolve: If you've already got an endpoint reference (either from an earlier probe or static configuration), you can use this endpoint reference to retrieve the details of this endpoint (for instance its transport address)   Besides these four operations WS-Discovery you can call it also defines two different ways you can call this service. You’ve got managed mode and ad-hoc mode. In managed mode, which is the one supported by the WSO-Registry, calls are made directly to the registry using unicast calls.   …   In this figure the registry plays the part of the “Discovery Proxy”. Besides the managed mode, there is also an ad-hoc mode.   …   In the ad-hoc mode initial discovery and registration are all done using multi-cast messages (for instance udp). There is an opensource java library (url of java ws-discovery) which supports this. But since we’re looking at how to integrate Mule and the WSO2 registry this is a bit out of scope.   Now back to the WS-Discovery client we need to write. To write a client for the unicase WS-Discovery proxy we need to have a WSDL. For this we have two options, we can use the WSDL provided by the WSO2 registry (can be found at http://localhost:9763/services/DiscoveryProxy?wsdl) or we can take the ones provided by the specification itself. Since the ones from the WSO2 registry uses xs-any elements, the created client stub won’t be that useful, since we still have to manually add the correct content.                &lt;xs:element name=\"Hello\"&gt;                  &lt;xs:complexType&gt;                      &lt;xs:sequence&gt;                          &lt;xs:element minOccurs=\"0\" name=\"Hello\" nillable=\"true\" type=\"xs:anyType\" /&gt;                      &lt;/xs:sequence&gt;                  &lt;/xs:complexType&gt;              &lt;/xs:element&gt;     Therefore I took the specifications from here (http://docs.oasis-open.org/ws-dd/ns/discovery/2009/01), removed the multicast operations, added a service and used that WSDL as the base to generate the client code from. If you’re interested in these final WSDLs you can download the complete project including all the files used at the end of this article.  Now that we’ve got a good WSDL to start from, lets use Maven and WSDL2Java to generate our client stub. To do this we just add the following to the Maven build file.    &lt;plugin&gt; \t&lt;groupId&gt;org.apache.cxf&lt;/groupId&gt; \t&lt;artifactId&gt;cxf-codegen-plugin&lt;/artifactId&gt; \t&lt;version&gt;2.2.3&lt;/version&gt; \t&lt;executions&gt; \t\t&lt;execution&gt; \t\t\t&lt;id&gt;generate-sources&lt;/id&gt; \t\t\t&lt;phase&gt;generate-sources&lt;/phase&gt; \t\t\t&lt;configuration&gt; \t\t\t&lt;sourceRoot&gt;${project.build.directory}/generated/cxf&lt;/sourceRoot&gt; \t\t\t\t&lt;wsdlOptions&gt; \t\t\t\t\t&lt;wsdlOption&gt; \t\t\t\t\t\t&lt;wsdl&gt;${basedir}/src/main/resources/wsdl/ws-discovery.wsdl&lt;/wsdl&gt;             \t\t\t\t\t\t&lt;extraargs&gt;  \t\t\t\t\t\t\t&lt;extraarg&gt;-client&lt;/extraarg&gt;         \t\t\t\t\t\t&lt;extraarg&gt;-noAddressBinding&lt;/extraarg&gt;     \t\t\t\t\t\t&lt;/extraargs&gt;     \t\t\t\t\t&lt;/wsdlOption&gt; \t\t\t\t&lt;/wsdlOptions&gt; \t\t\t&lt;/configuration&gt; \t\t\t&lt;goals&gt; \t\t\t\t&lt;goal&gt;wsdl2java&lt;/goal&gt; \t\t\t&lt;/goals&gt; \t\t&lt;/execution&gt; \t&lt;/executions&gt; &lt;/plugin&gt;    Running the build now will generate the client code (we’ve specified this with the -client option) which we’ll use to call the discovery proxy. If you look closely at the plugin configuration you’ll notice the use of the “-noAddressBinding” argument. The reason for this is the following schema type definition:    \t&lt;xs:element name=\"EndpointReference\" type=\"tns:EndpointReferenceType\" /&gt; \t&lt;xs:complexType name=\"EndpointReferenceType\" mixed=\"false\"&gt; \t\t&lt;xs:sequence&gt; \t\t\t&lt;xs:element name=\"Address\" type=\"tns:AttributedURIType\" /&gt; \t\t\t&lt;xs:element ref=\"tns:ReferenceParameters\" minOccurs=\"0\" /&gt; \t\t\t&lt;xs:element ref=\"tns:Metadata\" minOccurs=\"0\" /&gt; \t\t\t&lt;xs:any namespace=\"##other\" processContents=\"lax\" minOccurs=\"0\" \t\t\t\tmaxOccurs=\"unbounded\" /&gt; \t\t&lt;/xs:sequence&gt; \t\t&lt;xs:anyAttribute namespace=\"##other\" processContents=\"lax\" /&gt; \t&lt;/xs:complexType&gt;    This defines an endpoint reference and comes from the WS-Addressing schema. If you parse a WSDL which includes such an endpointReference, an internal java type will be generated: javax.xml.ws.EndpointReference. The problem is that we can’t access the address part of this class. This poses a problem if we want to dynamically create an endpoint using other techniques than jax-ws. If you specify the -NoAdrressBinding argument, the internal class won’t be used and a normal javabeans based class is created.   With the code generated, we can create a simple WS-Discovery client by directly using the generated client (we could also have used CXFs dynamic proxy). To do this we just instantiate this client and configure it with the url the discovery proxy is running on:   \tprivate DiscoveryProxy getPort(String dicoveryProxyUrl) { \t\tDiscoverProxyService service = new DiscoverProxyService(); \t\tDiscoveryProxy port = service.getDiscoverProxyPort(); \t\t((BindingProvider) port).getRequestContext().put(BindingProvider.ENDPOINT_ADDRESS_PROPERTY \t\t\t\t\t\t\t\t\t,dicoveryProxyUrl);  \t\treturn port; \t}  The port returned here provides the Hello, Bye, Probe and Resolve methods we can use to register and search for services. In this article we’ll focus on the registration part. For this we need to use the Hello operation. If we look at the WS-Discovery specification the Hello operation takes the following parameters.        &lt;d:Hello ... &gt;       &lt;a:EndpointReference&gt; ... &lt;/a:EndpointReference&gt;      [&lt;d:Types&gt;list of xs:QName&lt;/d:Types&gt;]?      [&lt;d:Scopes&gt;list of xs:anyURI&lt;/d:Scopes&gt;]?      [&lt;d:XAddrs&gt;list of xs:anyURI&lt;/d:XAddrs&gt;]?       &lt;d:MetadataVersion&gt;xs:unsignedInt&lt;/d:MetadataVersion&gt;     &lt;/d:Hello&gt;      - EndpointReference: Endpoint Reference for the Target Service and contains an \"address\" element (amongst others). This element however doesn't have to resolve to a network resolvable address. It's even recommended that this is a globally unique identifier which should remain constant regardless of network interfaces and lifecycle of this service. If this can't be resolved to a network address, which should be assumed, the network address should be specified in the d:XAddrs part of the Hello message. Types: describes the type of service this is. For instance d:employeeInformationService or d:accountServiceBasic and d:accountServiceAdvanced. You're pretty free to specify here whatever you want. Scopes: With scopes you can specify in which scopes your service should be used. For instance you could specify here that your service should be used only at a specific location, in a specific timeframe, or only for testing purposes. Once again there is no strict format for this, so you can easily adapt this to your specific scenario. XAddrs: This is the element which contains the endpoint addresses on which your service can be called. In contrast with the endpoint reference we saw earlier, this should be a network resolvable address. MetadataVersion: the version of this service. If for instance the interface changes the next time this service registers itself   Back to the source code and let’s see how we specify this in our own ws-discovery client.   \tpublic void hello(String endpointReference, List&lt;String&gt; addresses,  List&lt;QName&gt; types, String scope, long version) { \t\ttry { \t\t\t// create the parameter \t\t\tHelloType parameters = new HelloType(); \t\t\t \t\t\t// first the endpointReference \t\t\tEndpointReferenceType reference = new EndpointReferenceType(); \t\t\tAttributedURIType attr = new AttributedURIType(); \t\t\tattr.setValue(endpointReference); \t\t\treference.setAddress(attr); \t\t\tparameters.setEndpointReference(reference); \t\t\t \t\t\t// then the types \t\t\tList&lt;QName&gt; typesList = parameters.getTypes(); \t\t\ttypesList.addAll(types); \t\t\t \t\t\t// and the addresses \t\t\tList&lt;String&gt; addressesList = parameters.getXAddrs(); \t\t\taddressesList.addAll(addresses); \t\t\t \t\t\t// and the scope \t\t\tScopesType scopesType = new ScopesType(); \t\t\tscopesType.setMatchBy(scope); \t\t\tparameters.setScopes(scopesType); \t\t\t \t\t\t// and finally the version \t\t\tparameters.setMetadataVersion(version); \t\t\t \t\t\t// now get the port and fire the operation \t\t\tgetPort(PROXY_URL).helloOp(parameters);  \t\t} catch (Exception e) { \t\t\t// handle exception, for now just print it out \t\t\te.printStackTrace(); \t\t} \t} \t \tprivate DiscoveryProxy getPort(String dicoveryProxyUrl) { \t\tDiscoverProxyService service = new DiscoverProxyService(); \t\tDiscoveryProxy port = service.getDiscoverProxyPort(); \t\t((BindingProvider) port).getRequestContext().put(BindingProvider.ENDPOINT_ADDRESS_PROPERTY, dicoveryProxyUrl);  \t\treturn port; \t}   The interesting code here is the one in the hello operation. This operation takes as parameters the information we’ve just discussed. All it does is fill in the generated HelloType parameter and uses the generated client to fire of the request. Nothing to special here. One thing to notice though is the way we get the port for a specific url. Normally when we get a generated client, the client uses the address specified in the WSDL, often though we need to be able to manually set the destination of the webservice. We can do this by manually changing the BindingProvider.ENDPOINT_ADDRESS_PROPERTY as is shown in this code sample.   At this point we’ve got a working WS-Discovery client, which we can already use from our unit tests to register an endpoint in the WSO2 registry. What is missing here though is integration with Mule. The scenario which we’ll follow in this example is that once Mule is started we want to register all the available endpoint in the service registry. In the next section we’ll show you how you can use this client from Mule to register services.   Hook into the lifecycle of Mule so that the services are registered on startup   The last part we need to do is hook into Mule’s lifecycle and register the endpoints when Mule is started. For this example we’ll register the services when Mule has started its context. In other words when we receive the MuleContextNotification.CONTEXT_STARTED event, we’ll get a list of all the endpoints and register those in the WSO2 registry. Mule provides a number of options to hook into the lifecycle, we can register a notification listener using a simple Spring Bean, change the way Mule is bootstrapped, or in this case create an agent that listens to specific events. To create an agent all we need to add to the mule configuration is the following:    \t&lt;custom-agent name=\"test-custom-agent\" class=\"org.smartjava.agent.notifications.DiscoveryAgent\"&gt; \t\t&lt;spring:property name=\"discoveryUrl\" value=\"http://localhost:9763/services/DiscoveryProxy\"/&gt; \t&lt;/custom-agent&gt;    This will register an agent which is started when Mule itself is started. The code for this agent isn’t that special. We implement a couple of interfaces to get the mule context and the notification and for the rest we just use the client code we discussed earlier. The interesting parts of the code are shown below.   public class DiscoveryAgent implements Agent, MuleContextAware, MuleContextNotificationListener&lt;MuleContextNotification&gt; {  \tprivate String discoveryUrl;\t\t \tprivate MuleContext context; \tprivate List&lt;String&gt; registeredEndpoints = new ArrayList&lt;String&gt;(); \tprivate DiscoveryClient client = new DiscoveryClient(); \t \t@Override \tpublic void setMuleContext(MuleContext muleContext) { \t\tthis.context = muleContext; \t\ttry{ \t\t\tmuleContext.registerListener(this,\"*\"); \t\t} catch (NotificationException e) { \t\t\te.printStackTrace(); \t\t} \t} \t \t@Override \tpublic void onNotification(MuleContextNotification notification) { \t\t \t\tif (notification.getAction() == MuleContextNotification.CONTEXT_STARTED) { \t\t\tCollection&lt;ImmutableEndpoint&gt; endpoints = context.getRegistry().getEndpoints(); \t\t\tfor (ImmutableEndpoint immutableEndpoint : endpoints) { \t\t\t\tif (immutableEndpoint.getProperties() != null &amp;&amp; immutableEndpoint.getProperties().containsKey(\"wsdiscovery.enableDiscovery\")) { \t\t\t\t\tString enableValue = (String) immutableEndpoint.getProperties().get(\"wsdiscovery.enableDiscovery\"); \t\t\t\t\tif (enableValue.equals(\"true\")) { \t\t\t\t\t\tif (!registeredEndpoints.contains(immutableEndpoint.getAddress())) { \t\t\t\t\t\t\t// get the version \t\t\t\t\t\t\tString value = (String) immutableEndpoint.getProperties().get(\"wsdiscovery.version\"); \t\t\t\t\t\t\tlong version = Long.valueOf(value); \t\t\t\t\t\t\t \t\t\t\t\t\t\t// get the endpoint name \t\t\t\t\t\t\tString endpointReference = immutableEndpoint.getName(); \t\t\t\t\t\t\t \t\t\t\t\t\t\t// get the endpoint address \t\t\t\t\t\t\tList&lt;String&gt; addresses = Arrays.asList(immutableEndpoint.getAddress()); \t\t\t\t\t\t\t \t\t\t\t\t\t\t// get the scope \t\t\t\t\t\t\tString scope = (String) immutableEndpoint.getProperties().get(\"wsdiscovery.scope\"); \t\t\t\t\t\t\t \t\t\t\t\t\t\t// get the types as qname \t\t\t\t\t\t\tList&lt;QName&gt; types = stringToQName((String) immutableEndpoint.getProperties().get(\"wsdiscovery.types\"));  \t\t\t\t\t\t\t// and make the call to the client \t\t\t\t\t\t\tclient.hello(endpointReference, addresses, types, scope, version); \t\t\t\t\t\t\t \t\t\t\t\t\t\t// make sure we only register each endpoint one time \t\t\t\t\t\t\tregisteredEndpoints.add(immutableEndpoint.getAddress()); \t\t\t\t\t\t} \t\t\t\t\t} \t\t\t\t} \t\t\t}\t\t\t \t\t} \t} \t \t... }  In the class definition you can see that we implement the Agent, MuleContextAware and the MuleContextNotificationListener interfaces. The Agent interfaces is required since we’re implementing a Mule Agent. The MuleContextAware interface is used to get access to the MuleContext which we use later on to get the endpoints. The last interface, the MuleContextNotificationListener, allows us to receive certain context related events. When a context event occurs the method onNotification is called. In this method, as you can see from the code, we do the following things:    We check whether the event indicates the context is started If it's the correct event, we use the Mule context to get the list of registered endpoints We then check if the property \"wsdiscovery.enableDiscovery\" is set to true. If it is, we collect a number of other properties of the endpoint and use the client to call the hello method Finally we keep track of the registered addresses to avoid registring the same endpoint twice.  That’s it for the agent. The final part we need to do is configure Mule to start up our agent and configure the endpoints we want to register with the SOA Repository. The next listing shows the Mule configuration required for this.    &lt;mule xmlns=\"http://www.mulesoft.org/schema/mule/core\"       xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"       xmlns:spring=\"http://www.springframework.org/schema/beans\"       xmlns:file=\"http://www.mulesoft.org/schema/mule/file\"       xmlns:http=\"http://www.mulesoft.org/schema/mule/http\"       xmlns:cxf=\"http://www.mulesoft.org/schema/mule/cxf\"     xsi:schemaLocation=\"        http://www.mulesoft.org/schema/mule/cxf http://www.mulesoft.org/schema/mule/cxf/3.0/mule-cxf.xsd        http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans-3.0.xsd        http://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/3.0/mule.xsd        http://www.mulesoft.org/schema/mule/http http://www.mulesoft.org/schema/mule/http/3.0/mule-http.xsd        http://www.mulesoft.org/schema/mule/file http://www.mulesoft.org/schema/mule/file/3.0/mule-file.xsd\"&gt;       \t&lt;custom-agent name=\"test-custom-agent\" class=\"org.smartjava.agent.notifications.DiscoveryAgent\"&gt; \t\t&lt;spring:property name=\"discoveryUrl\" value=\"http://localhost:9763/services/DiscoveryProxy\"/&gt; \t&lt;/custom-agent&gt; \t \t&lt;notifications dynamic=\"true\"/&gt;      \t&lt;flow name=\"simpleFileFlow\"&gt;  \t\t&lt;inbound-endpoint address=\"http://localhost:18080/hello\" &gt;  \t\t\t&lt;properties&gt;  \t\t\t\t&lt;spring:entry key=\"wsdiscovery.enableDiscovery\" value=\"true\"/&gt;  \t\t\t\t&lt;spring:entry key=\"wsdiscovery.version\" value=\"1\"/&gt;  \t\t\t\t&lt;spring:entry key=\"wsdiscovery.scope\" value=\"e:QA\"/&gt;  \t\t\t\t&lt;spring:entry key=\"wsdiscovery.types\" value=\"{http://example.org}Hello,{http://discovery.org}Example\"/&gt;  \t\t\t&lt;/properties&gt;  \t\t&lt;/inbound-endpoint&gt;     \t\t&lt;cxf:jaxws-service serviceClass=\"manning.governance.wsdiscovery.HelloWorldImpl\"/&gt;     \t\t&lt;component class=\"manning.governance.wsdiscovery.HelloWorldImpl\" /&gt;  \t&lt;/flow&gt; &lt;/mule&gt;   As you can see we add a simple properties element to the inbound endpoint on which we configure the ws-discovery specifics we’ve discussed earlier. That’s all you need to do. Now we can run this example (after you’ve started the WSO2 SOA Registry), and Mule will automatically register its services into the WSO2 Registry. Do check the registered service go to the SOA Registry admin page (https://localhost:9443/carbon/), log in and click on the metadata tab you can view the services that are available. Here you’ll see a service listed with a DiscoveredService_1286732385985 like name. This is the service that was addes from Mule. In the current view you won’t see much about the service that was registered. If you click on the ‘standard view’ link and then on the ‘Display as text’ link you can see the information we’ve registered.   …   Next week I’ll post the follow to this article where we will see how to discover services using WS-Discovery from Mule to the WSO2 Service registry.  ","categories": ["posts","mule","wso2"],
        "tags": [],
        "url": "http://www.smartjava.org/content/mule-and-wso2-registry-integration/",
        "teaser":null},{
        "title": "Using d3.js to visualize GIS",
        "excerpt":" I've been playing around with d3.js  a bit the last couple of days and it's a great little javascript library to visualize information. I needed a simple library to easily create a SVG element from a couple of paths stored in postgis. This library will transfom this set of coordinates (in WGS84) to an SVG that can be directly rendered in your browser.    I really like the example they showed here:       So I wanted to recreate this, but then based on a set geo paths (shape format) I downloaded from http://www.cbs.nl. After some messing around I only had to do the following:      Convert and read in the shape files in postgis.   Make sure the coordinates were transformed from the RD format used in the Netherlands to WGS84.   Export this information from postgis in geo-json format.   Use d3.js to create SVG nodes using the mercator projection.   Scale and transform the SVG to center on the Netherlands   Convert the shape files and read into postgis  You can do this very easily using the shp2pgsql command:    /opt/local/lib/postgresql91/bin/shp2pgsql -W ISO-8859-1 Bevolkingskern_2008_def.shp geodata.kernen &gt; Bevolkingskern_2008_def.sql   This will create a simple SQL file you can read into postgis.   Convert the coordinates to the correct EPSG projection. If you know the source projection and the target projection you can easily do this using postgis:    ST_Transform(ST_SetSRID(the_geom,28992),4326))   When I loaded the information in postgis I didn’t specify which projection was used (which SRID). So before I can covert it I first have to explicitly set the SRID we’re working with. In this case its SRID 28992, which tells postgis that our data in the database is stored in Rijks Driehoek format. Then with the ST_Transform operation we convert to SRID 4326 which is WGS84, and which we can use in our mercator projection.   Export to geo-json  The next step is to get the information in a format we can use. D3.js expects the data in the geo-json format. Luckily postgis also supports this format. If we expand the query we just saw, we get the following to get a set of geo-json ready data:    SELECT gm_naam,ST_AsGeoJson(ST_Transform(ST_SetSRID(the_geom,28992),4326)), \t\t                          \t\tbev_dichth,aant_inw,aant_man,aant_vrouw,  \t\t                          \t\tp_ongehuwd,p_gehuwd,p_gescheid, p_verweduw \t\t                          \t\tFROM geodata.gem   With this select statement I retrieve some general information from the database (jn this case some census information for all the municipalities in the Netherlands) and I retrieve the geo coordinates as geo-json. We almost got the format we want to sent to d3.js. We just need to wrap the geo-json string in another json element to get this:    {   \"type\": \"FeatureCollection\",   \"features\":   [         {       \"type\": \"Feature\",       \"bev_dichth\": \"364\",       \"gm_naam\": \"Gennep\",       \"geometry\":       {         \"type\": \"MultiPolygon\",         \"coordinates\": [[        [                     [             5.951878,             51.742153           ],                     [             5.9547286,             51.73854           ],                     [             5.9590716,             51.73852           ],            ....    Use d3.js to show and scale the geo information  Now we can load this json structure in d3.js and show the map:  var xym = d3.geo.mercator(); var path = d3.geo.path().projection(xym);  // the variable that holds our translate, centers on the netherlands var translate = xym.translate(); translate[0] = -500; translate[1] = 10640  // center on the netherlands and zoom all the way in\t xym.translate(translate); xym.scale(60000);  var svg = d3.select(\"#chart\")     .append(\"svg\").attr(\"id\",\"svgoriginal\");  var gemeentes = svg.append(\"g\")     .attr(\"id\", \"gemeentes\")     .attr(\"class\", \"RdYlGn\");  d3.json(\"geoinformation.json\", function (json) {     gemeentes.selectAll(\"path\")   // select all the current path nodes         .data(json.features)      // bind these to the features array in json         .enter().append(\"path\")   // if not enough elements create a new path         .attr(\"class\", quantize)  // add attribute class and fill with result from quantize         .attr(\"id\", named)         .attr(\"d\", path)          // transform the supplied jason geo path to svg });   And that’s pretty much it. The result of this exercise is the following svg map of the Netherlands.      The interesting aspect is that everything is standard SVG, so you can very easily attach mouselisteners, use css and javascript for effect, and pretty much do everything with these element you can do with other elements in the DOM tree!   ","categories": ["posts","javascript","postgis","gis","d3.js"],
        "tags": [],
        "url": "http://www.smartjava.org/content/using-d3js-visualize-gis/",
        "teaser":null},{
        "title": "Embedded Jetty, Vaadin and Weld",
        "excerpt":"When I develop web applications I like to be able to quickly start them from Eclipse without having to rely on all kinds of heavy-weight tomcat or glassfish plugins. So what I often do is just create a simple Java based Jetty launcher I can run directly from Eclipse. This launcher starts up within a couple of seconds so this makes developing a lot more pleasant.   However, sometimes, getting everything setup correctly is a bit hard. So in this article I’ll give you a quick overview of how you can setup Jetty together with Weld for CDI and Vaadin as the web framework.   To get everything setup correctly we’ll need to take the following steps:      Setup the maven pom for the required dependencies   Create a java based Jetty Launcher   Setup web.xml   Add Weld placeholders   Setup the maven pom for the required dependencies   I use the following pom.xml file. You might not need everything if you for instance don’t use custom components. But it should serve as a good reference of what should be in there.   &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \txsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"&gt; \t&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; \t&lt;groupId&gt;group.id&lt;/groupId&gt; \t&lt;artifactId&gt;artifact.id&lt;/artifactId&gt; \t&lt;packaging&gt;war&lt;/packaging&gt; \t&lt;version&gt;1.0&lt;/version&gt; \t&lt;name&gt;Vaadin Web Application&lt;/name&gt;  \t&lt;properties&gt; \t\t&lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; \t\t&lt;vaadin.version&gt;6.7.1&lt;/vaadin.version&gt; \t\t&lt;gwt.version&gt;2.3.0&lt;/gwt.version&gt; \t\t&lt;gwt.plugin.version&gt;2.2.0&lt;/gwt.plugin.version&gt; \t&lt;/properties&gt;  \t&lt;build&gt; \t\t&lt;plugins&gt; \t\t\t&lt;plugin&gt; \t\t\t\t&lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; \t\t\t\t&lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; \t\t\t\t&lt;configuration&gt; \t\t\t\t\t&lt;source&gt;1.5&lt;/source&gt; \t\t\t\t\t&lt;target&gt;1.5&lt;/target&gt; \t\t\t\t&lt;/configuration&gt; \t\t\t&lt;/plugin&gt; \t\t\t&lt;plugin&gt; \t\t\t\t&lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; \t\t\t\t&lt;artifactId&gt;gwt-maven-plugin&lt;/artifactId&gt; \t\t\t\t&lt;version&gt;${gwt.plugin.version}&lt;/version&gt; \t\t\t\t&lt;configuration&gt; \t\t\t\t\t&lt;webappDirectory&gt;${project.build.directory}/${project.build.finalName}/VAADIN/widgetsets&lt;/webappDirectory&gt; \t\t\t\t\t&lt;extraJvmArgs&gt;-Xmx512M -Xss1024k&lt;/extraJvmArgs&gt; \t\t\t\t\t&lt;runTarget&gt;cvgenerator-web&lt;/runTarget&gt; \t\t\t\t\t&lt;hostedWebapp&gt;${project.build.directory}/${project.build.finalName}&lt;/hostedWebapp&gt; \t\t\t\t\t&lt;noServer&gt;true&lt;/noServer&gt; \t\t\t\t\t&lt;port&gt;8080&lt;/port&gt; \t\t\t\t\t&lt;compileReport&gt;false&lt;/compileReport&gt; \t\t\t\t&lt;/configuration&gt; \t\t\t\t&lt;executions&gt; \t\t\t\t\t&lt;execution&gt; \t\t\t\t\t\t&lt;goals&gt; \t\t\t\t\t\t\t&lt;goal&gt;resources&lt;/goal&gt; \t\t\t\t\t\t\t&lt;goal&gt;compile&lt;/goal&gt; \t\t\t\t\t\t&lt;/goals&gt; \t\t\t\t\t&lt;/execution&gt; \t\t\t\t&lt;/executions&gt; \t\t\t\t&lt;dependencies&gt; \t\t\t\t\t&lt;dependency&gt; \t\t\t\t\t\t&lt;groupId&gt;com.google.gwt&lt;/groupId&gt; \t\t\t\t\t\t&lt;artifactId&gt;gwt-dev&lt;/artifactId&gt; \t\t\t\t\t\t&lt;version&gt;${gwt.version}&lt;/version&gt; \t\t\t\t\t&lt;/dependency&gt; \t\t\t\t\t&lt;dependency&gt; \t\t\t\t\t\t&lt;groupId&gt;com.google.gwt&lt;/groupId&gt; \t\t\t\t\t\t&lt;artifactId&gt;gwt-user&lt;/artifactId&gt; \t\t\t\t\t\t&lt;version&gt;${gwt.version}&lt;/version&gt; \t\t\t\t\t&lt;/dependency&gt; \t\t\t\t&lt;/dependencies&gt; \t\t\t&lt;/plugin&gt; \t\t\t&lt;plugin&gt; \t\t\t\t&lt;groupId&gt;com.vaadin&lt;/groupId&gt; \t\t\t\t&lt;artifactId&gt;vaadin-maven-plugin&lt;/artifactId&gt; \t\t\t\t&lt;version&gt;1.0.2&lt;/version&gt; \t\t\t\t&lt;executions&gt; \t\t\t\t\t&lt;execution&gt; \t\t\t\t\t\t&lt;configuration&gt; \t\t\t\t\t\t&lt;/configuration&gt; \t\t\t\t\t\t&lt;goals&gt; \t\t\t\t\t\t\t&lt;goal&gt;update-widgetset&lt;/goal&gt; \t\t\t\t\t\t&lt;/goals&gt; \t\t\t\t\t&lt;/execution&gt; \t\t\t\t&lt;/executions&gt; \t\t\t&lt;/plugin&gt; \t\t&lt;/plugins&gt; \t&lt;/build&gt;         &lt;!-- extra repositories for Vaadin extensions --&gt; \t&lt;repositories&gt; \t\t&lt;repository&gt; \t\t\t&lt;id&gt;vaadin-snapshots&lt;/id&gt; \t\t\t&lt;url&gt;http://oss.sonatype.org/content/repositories/vaadin-snapshots/&lt;/url&gt; \t\t\t&lt;releases&gt; \t\t\t\t&lt;enabled&gt;false&lt;/enabled&gt; \t\t\t&lt;/releases&gt; \t\t\t&lt;snapshots&gt; \t\t\t\t&lt;enabled&gt;true&lt;/enabled&gt; \t\t\t&lt;/snapshots&gt; \t\t&lt;/repository&gt; \t\t&lt;repository&gt; \t\t\t&lt;id&gt;vaadin-addons&lt;/id&gt; \t\t\t&lt;url&gt;http://maven.vaadin.com/vaadin-addons&lt;/url&gt; \t\t&lt;/repository&gt; \t&lt;/repositories&gt;          &lt;!-- repositories for the plugins --&gt; \t&lt;pluginRepositories&gt; \t\t&lt;pluginRepository&gt; \t\t\t&lt;id&gt;codehaus-snapshots&lt;/id&gt; \t\t\t&lt;url&gt;http://nexus.codehaus.org/snapshots&lt;/url&gt; \t\t\t&lt;snapshots&gt; \t\t\t\t&lt;enabled&gt;true&lt;/enabled&gt; \t\t\t&lt;/snapshots&gt; \t\t\t&lt;releases&gt; \t\t\t\t&lt;enabled&gt;false&lt;/enabled&gt; \t\t\t&lt;/releases&gt; \t\t&lt;/pluginRepository&gt; \t\t&lt;pluginRepository&gt; \t\t\t&lt;id&gt;vaadin-snapshots&lt;/id&gt; \t\t\t&lt;url&gt;http://oss.sonatype.org/content/repositories/vaadin-snapshots/&lt;/url&gt; \t\t\t&lt;snapshots&gt; \t\t\t\t&lt;enabled&gt;true&lt;/enabled&gt; \t\t\t&lt;/snapshots&gt; \t\t\t&lt;releases&gt; \t\t\t\t&lt;enabled&gt;false&lt;/enabled&gt; \t\t\t&lt;/releases&gt; \t\t&lt;/pluginRepository&gt; \t&lt;/pluginRepositories&gt;         &lt;!-- minimal set of dependencies --&gt; \t&lt;dependencies&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;com.vaadin&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;vaadin&lt;/artifactId&gt; \t\t\t&lt;version&gt;${vaadin.version}&lt;/version&gt; \t\t&lt;/dependency&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.vaadin.addons&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;stepper&lt;/artifactId&gt; \t\t\t&lt;version&gt;1.1.0&lt;/version&gt; \t\t&lt;/dependency&gt;                 &lt;!-- the jetty version we'll use --&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.eclipse.jetty.aggregate&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;jetty-all-server&lt;/artifactId&gt; \t\t\t&lt;version&gt;8.0.4.v20111024&lt;/version&gt; \t\t\t&lt;type&gt;jar&lt;/type&gt; \t\t\t&lt;scope&gt;compile&lt;/scope&gt; \t\t\t&lt;exclusions&gt; \t\t\t\t&lt;exclusion&gt; \t\t\t\t\t&lt;artifactId&gt;mail&lt;/artifactId&gt; \t\t\t\t\t&lt;groupId&gt;javax.mail&lt;/groupId&gt; \t\t\t\t&lt;/exclusion&gt; \t\t\t&lt;/exclusions&gt; \t\t&lt;/dependency&gt;                   &lt;!-- vaadin custom field addon --&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.vaadin.addons&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;customfield&lt;/artifactId&gt; \t\t\t&lt;version&gt;0.9.3&lt;/version&gt; \t\t&lt;/dependency&gt;                 &lt;!-- with cdi utils plugin you can use Weld --&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.vaadin.addons&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;cdi-utils&lt;/artifactId&gt; \t\t\t&lt;version&gt;0.8.6&lt;/version&gt; \t\t&lt;/dependency&gt;                                &lt;!-- we'll use this version of Weld --&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.jboss.weld.servlet&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;weld-servlet&lt;/artifactId&gt; \t\t\t&lt;version&gt;1.1.5.Final&lt;/version&gt; \t\t\t&lt;type&gt;jar&lt;/type&gt; \t\t\t&lt;scope&gt;compile&lt;/scope&gt; \t\t&lt;/dependency&gt;                                 &lt;!-- normally following are provided, but not if you run within jetty --&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;javax.servlet&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;servlet-api&lt;/artifactId&gt; \t\t\t&lt;version&gt;2.5&lt;/version&gt; \t\t\t&lt;type&gt;jar&lt;/type&gt; \t\t\t&lt;scope&gt;provided&lt;/scope&gt; \t\t&lt;/dependency&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;javax.servlet.jsp&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;jsp-api&lt;/artifactId&gt; \t\t\t&lt;version&gt;2.2&lt;/version&gt; \t\t\t&lt;type&gt;jar&lt;/type&gt; \t\t\t&lt;scope&gt;provided&lt;/scope&gt; \t\t&lt;/dependency&gt;  \t\t&lt;dependency&gt; \t\t\t&lt;artifactId&gt;el-api&lt;/artifactId&gt; \t\t\t&lt;groupId&gt;javax.el&lt;/groupId&gt; \t\t\t&lt;version&gt;2.2&lt;/version&gt; \t\t\t&lt;scope&gt;provided&lt;/scope&gt; \t\t&lt;/dependency&gt; \t&lt;/dependencies&gt;  &lt;/project&gt;    Create the java launcher  With this pom we have all the dependencies we need to run Jetty, Vaadin and Weld together. Lets look at the Jetty Launcher.   import javax.naming.InitialContext; import javax.naming.Reference;  import org.eclipse.jetty.plus.jndi.Resource; import org.eclipse.jetty.server.Server; import org.eclipse.jetty.webapp.WebAppContext;  /**  * Simple jetty launcher, which launches the webapplication from the local  * resources and reuses the projects classpath.  *   * @author jos  */ public class Launcher {  \t/** run under root context */ \tprivate static String contextPath = \"/\"; \t/** location where resources should be provided from for VAADIN resources */ \tprivate static String resourceBase = \"src/main/webapp\"; \t/** port to listen on */ \tprivate static int httpPort = 8081;      private static String[] __dftConfigurationClasses =     {         \"org.eclipse.jetty.webapp.WebInfConfiguration\",         \"org.eclipse.jetty.webapp.WebXmlConfiguration\",         \"org.eclipse.jetty.webapp.MetaInfConfiguration\",          \"org.eclipse.jetty.webapp.FragmentConfiguration\",                 \"org.eclipse.jetty.plus.webapp.EnvConfiguration\",         \"org.eclipse.jetty.webapp.JettyWebXmlConfiguration\"     } ; \t \t/** \t * Start the server, and keep waiting. \t */ \tpublic static void main(String[] args) throws Exception { \t\t \t\tSystem.setProperty(\"java.naming.factory.url\",\"org.eclipse.jetty.jndi\"); \t\tSystem.setProperty(\"java.naming.factory.initial\",\"org.eclipse.jetty.jndi.InitialContextFactory\"); \t\t \t\tInitialContext ctx = new InitialContext(); \t\tctx.createSubcontext(\"java:comp\"); \t\t \t\tServer server = new Server(httpPort); \t\tWebAppContext webapp = new WebAppContext(); \t\twebapp.setConfigurationClasses(__dftConfigurationClasses); \t\t \t\twebapp.setDescriptor(\"src/main/webapp/WEB-INF/web.xml\"); \t\twebapp.setContextPath(contextPath); \t\twebapp.setResourceBase(resourceBase); \t\twebapp.setClassLoader(Thread.currentThread().getContextClassLoader());  \t\tserver.setHandler(webapp); \t\tserver.start(); \t\t \t\tnew Resource(\"BeanManager\", new Reference(\"javax.enterprise.inject.spi.BeanMnanager\", \t\t\t\t\"org.jboss.weld.resources.ManagerObjectFactory\", null)); \t\t \t\tserver.join(); \t} }   This code will start a Jetty server that uses the web.xml from the project to launch the Vaadin web-app. Take note that we explicitly use the  setConfigurationClasses  operation. This is needed to make sure we have a JNDI context we can use to register the Weld beanmanager in.   Setup the web.xml   Next we’ll look at the web.xml. The one I use in this example is shown next:   &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;web-app xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \txmlns=\"http://java.sun.com/xml/ns/javaee\" xmlns:web=\"http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd\" \txsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd\" \tid=\"WebApp_ID\" version=\"2.5\"&gt; \t&lt;display-name&gt;Vaadin Web Application&lt;/display-name&gt; \t&lt;context-param&gt; \t\t&lt;description&gt;Vaadin production mode&lt;/description&gt; \t\t&lt;param-name&gt;productionMode&lt;/param-name&gt; \t\t&lt;param-value&gt;false&lt;/param-value&gt; \t&lt;/context-param&gt; \t&lt;servlet&gt; \t\t&lt;servlet-name&gt;example&lt;/servlet-name&gt; \t\t&lt;servlet-class&gt;ServletSpecifiedByTheCDIVaadinPlugin&lt;/servlet-class&gt; \t\t&lt;init-param&gt; \t\t\t&lt;description&gt;Vaadin application class to start&lt;/description&gt; \t\t\t&lt;param-name&gt;application&lt;/param-name&gt; \t\t\t&lt;param-value&gt;VaadinApplicationClassName&lt;/param-value&gt; \t\t&lt;/init-param&gt; \t\t&lt;init-param&gt; \t\t\t&lt;param-name&gt;widgetset&lt;/param-name&gt; \t\t\t&lt;param-value&gt;customwidgetsetnameifyouuseit&lt;/param-value&gt; \t\t&lt;/init-param&gt; \t&lt;/servlet&gt; \t&lt;servlet-mapping&gt; \t\t&lt;servlet-name&gt;example&lt;/servlet-name&gt; \t\t&lt;url-pattern&gt;/example/*&lt;/url-pattern&gt; \t&lt;/servlet-mapping&gt; \t&lt;welcome-file-list&gt; \t\t&lt;welcome-file&gt;index.html&lt;/welcome-file&gt; \t&lt;/welcome-file-list&gt; \t&lt;listener&gt; \t\t&lt;listener-class&gt;org.jboss.weld.environment.servlet.Listener&lt;/listener-class&gt; \t&lt;/listener&gt; \t&lt;resource-env-ref&gt; \t\t&lt;description&gt;Object factory for the CDI Bean Manager&lt;/description&gt; \t\t&lt;resource-env-ref-name&gt;BeanManager&lt;/resource-env-ref-name&gt; \t\t&lt;resource-env-ref-type&gt;javax.enterprise.inject.spi.BeanManager&lt;/resource-env-ref-type&gt; \t&lt;/resource-env-ref&gt; &lt;/web-app&gt;   At the bottom of the web.xml you can see the resource-env we define for Weld and the required listener to make sure Weld is started and our beans are injected. You can also see that we specified a different servlet name instead of the normal Vaadin servlet. For the details on this see the CDI plugin page: https://vaadin.com/directory#addon/cdi-utils   The main steps are (taken from that page):      Add empty beans.xml -file (CDI marker file) to your project under WEB-INF dir   Add cdiutils*.jar to your project under WEB-INF/lib   Create your Application class by extending AbstractCdiApplication   Extend AbstractCdiApplicationServlet and annotate it with @WebServlet(urlPatterns = \"/*\")   Deploy to JavaEE/Web profile -compatible container (CDI apps can also be run on servlet containers etc. but some further configuration is required)   Add weld placeholder   At this point we have all the dependencies, we created a launcher that can be directly used from Eclipse, and we made sure Weld is loaded on startup. We’ve also configured the CDI plugin for Vaadin. At this point we’re almost done. We only need to add empty beans.xml files in the location we want to be included by the beans discovery of Weld.    &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;beans xmlns=\"http://java.sun.com/xml/ns/javaee\"   xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"   xsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/beans_1_0.xsd\"&gt; &lt;/beans&gt;    I had to add these to the  src/main/java/META-INF  library and to the  WEB-INF  directory for Weld to pickup all the annotated beans. And that’s it. You can now start the launcher and you should see all kind of Weld and Vaadin logging appearing.  ","categories": ["posts"],
        "tags": [],
        "url": "http://www.smartjava.org/content/embedded-jetty-vaadin-and-weld/",
        "teaser":null},{
        "title": "Maven and wildcard exclusions",
        "excerpt":"Once in a while I run into the problem where I don’t want to exclude a single dependency from a maven dependency, but all dependencies. For instance I recently needed a client library that, due to its parent pom, included all kinds of different framework components and dependencies it didn’t really need and me neither. And extra problem was that the transitive dependencies pointed to corrupt maven artifacts, that took a lot of time to resolve and finally fail the build.   If you want to exclude these dependencies, according to the documentation, you have to add each exclusions separately. According to the maven documentation you have to do it like this:    &lt;project&gt;   &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;   &lt;groupId&gt;sample.ProjectA&lt;/groupId&gt;   &lt;artifactId&gt;Project-A&lt;/artifactId&gt;   &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;   &lt;packaging&gt;jar&lt;/packaging&gt;   ...   &lt;dependencies&gt;     &lt;dependency&gt;       &lt;groupId&gt;sample.ProjectB&lt;/groupId&gt;       &lt;artifactId&gt;Project-B&lt;/artifactId&gt;       &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;       &lt;exclusions&gt;         &lt;exclusion&gt;           &lt;groupId&gt;sample.ProjectD&lt;/groupId&gt; &lt;!-- Exclude Project-D from Project-B --&gt;           &lt;artifactId&gt;Project-D&lt;/artifactId&gt;         &lt;/exclusion&gt;       &lt;/exclusions&gt;     &lt;/dependency&gt;   &lt;/dependencies&gt; &lt;/project&gt;    And repeat it for each dependency. But what if you don’t need all the dependencies, or some transitive dependencies are broken, you either have to add all the dependencies to the exclusion elements or put an altered version of the artifact in your local repository that doesn’t include any of the conflicting dependencies.   Wouldn’t it be nice to be able to exclude all transitive dependencies in one go? If you look at Maven’s Jira you’re not the only one to have this same idea.   http://jira.codehaus.org/browse/MNG-2315 http://jira.codehaus.org/browse/MNG-3832   If you look at these comments, the final comment of MNG-3832 from about a week ago seems to indicate that this functionality is already in Maven! I added:           &lt;exclusion&gt;           &lt;groupId&gt;*&lt;/groupId&gt;           &lt;artifactId&gt;*&lt;/artifactId&gt;         &lt;/exclusion&gt;    And magically all my transitive dependencies were excluded and I could build my project since the corrupt external artifacts weren’t included in the dependency graph anymore. But how could this work. Issue MNG-3832 points us into the direction where Maven supposedly determines whether to include an artifact or not. The ExludesArtifactFilter class:   public class ExcludesArtifactFilter     extends IncludesArtifactFilter {     public ExcludesArtifactFilter( List&lt;String&gt; patterns )     {         super( patterns );     }      public boolean include( Artifact artifact )     {         return !super.include( artifact );     } }   Which uses, not so surprising, the inverse of the IncudesArtificatFilter:       public boolean include( Artifact artifact )     {         String id = artifact.getGroupId() + \":\" + artifact.getArtifactId();          boolean matched = false;         for ( Iterator&lt;String&gt; i = patterns.iterator(); i.hasNext() &amp; !matched; )         {             // TODO: what about wildcards? Just specifying groups? versions?             if ( id.equals( i.next() ) )             {                 matched = true;             }         }         return matched;     }   Did you see the TODO in the last listing. Someone thought the same as the rest of us, but for now this filter checks only whether the id matches. Nothing more.   I wanted to know why this was working, and whether it was an accident it worked, or it was by design. I created a test project and started a Maven build in debug mode:    MAVEN_OPTS=\"-Xdebug -Xnoagent -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=8000\"; export MAVEN_OPTS mvn install    With the remote debugger attached and a lot of stepping through the code I saw that the ExludesArtifactFilter wasn’t even called. What the newer versions of maven (don’t know from which version) apparently use to filter out exclusions is the org.sonatype.aether.util.graph.selector.ExclusionDependencySelector. This class uses the following two methods to determine whether a dependency needs to be excluded or not:       private boolean matches( Exclusion exclusion, Artifact artifact )     {         if ( !matches( exclusion.getArtifactId(), artifact.getArtifactId() ) )         {             return false;         }         if ( !matches( exclusion.getGroupId(), artifact.getGroupId() ) )         {             return false;         }         if ( !matches( exclusion.getExtension(), artifact.getExtension() ) )         {             return false;         }         if ( !matches( exclusion.getClassifier(), artifact.getClassifier() ) )         {             return false;         }         return true;     }      private boolean matches( String pattern, String value )     {         return \"*\".equals( pattern ) || pattern.equals( value );     }   And there it is. The wildcard “*” we use and that works. You can not only use it on groupid and artifactid but also on extension and classifier.   So now you know that when you want to exclude some troublesome set of transitive dependencies you can use wildcards. Whether you should use wildcards is a whole other discussion….  ","categories": ["posts","exclusion","wildcard","java","maven"],
        "tags": [],
        "url": "http://www.smartjava.org/content/maven-and-wildcard-exclusions/",
        "teaser":null},{
        "title": "Using Querulous with Scala on postgresql",
        "excerpt":"I’ve started diving deeper into scala and as a language I’m starting to like it. Combine it with the Play framework and you’ve got a nice, statically typed, rapid development environment. Especially for creating REST services this setup works great:      Play to host the application   Scala as a powerful language to tie everything together   lift-json as an easy way to serialze objects to json   But what was missing was an easy, flexible way to talk to the database. I didn’t want to have to create a complete object model and use ORM to map this to the database, I just wanted a simple basic framework that hid the ugly part of JDBC from me. I have to admit, I started out with Anorm, but that wasn’t a success. I couldn’t get Anorm to play nice with numeric fields in postgresql, and the mapping-convention wasn’t working for me neither, since I had some GIS data in the database for which I needed to call specific postGis functions.   After getting frustated with Anorm, I started looking at the various database frameworks that were out there for Scala. The scala site lists a couple of them on its wiki here. A lot of these libraries focus on getting type-safety into the way you access a database. Good goal, but most of the libraries I’ve looked into, map to specific tables and column names. This wasn’t going to work for me, because of the postGis functions I needed.   One of the libraries listed, however, presented itself as a simple wrapper around JDBC: Querulous. Looking at the sparse documentation this library looked pretty simple to use, and nicely matched my requirements.   Get querulous   I’ve developed this service using play, and so to get this library I needed to alter the dependencies.yml file to include the querulous libs. My total depenencies.yml file looks like this:    require:     - play     - play -&gt; scala 0.9.1     - play -&gt; jersey 0.1     - com.sun.jersey -&gt; jersey-server 1.4     - net.liftweb -&gt; lift-json_2.8.1 2.4     - com.twitter -&gt; querulous 2.6.5       repositories:      - devjava:         type: iBiblio         root: \"http://download.java.net/maven/2/\"         contains:             - com.sun -&gt; *                  - twitter:         type: iBiblio         root: \"http://maven.twttr.com/\"         contains:             - com.twitter -&gt; *    Nothing to fancy. I include a small set of depedencies for jax-rs, json and querulous. I’ve also added a couple of extra repositories since not all the artifacts can be found in the default repositories. After this the depedencies need to be added to play. You do this by just running the “play dependencies –sync” command. Depending on which IDE you use, you might need to manually add the querulous (and the other) libraries to your classpath to get your IDE to pick them up.   Using Querulous   Using Querulous really isn’t that hard. You need access to a QueryEvaluator, and using that QueryEvaluator you can execute SQL statements. I do this using the following scala object:    object Database {     private val queryFactory = new SqlQueryFactory   private val apachePoolingDatabaseFactory = new ApachePoolingDatabaseFactory(     1,     5,     Duration.fromTimeUnit(1000, TimeUnit.MILLISECONDS),     Duration.fromTimeUnit(1000, TimeUnit.MILLISECONDS),     true,     Duration.fromTimeUnit(1000, TimeUnit.MILLISECONDS))    private val queryEvaluatorFactory = new StandardQueryEvaluatorFactory(apachePoolingDatabaseFactory, queryFactory)   private val queryEvaluator = queryEvaluatorFactory(\"localhost\", \"&lt;db_name&gt;\", \"&lt;user_name&gt;\", \"&lt;password&gt;\", Map[String, String](), \"jdbc:postgresql\")      def getQueryEvaluator(): QueryEvaluator = {     Logger.info(\"Getting evaluator: \" + queryEvaluator);     return queryEvaluator;   } }    This object creates a pooled database factory, which we use to create a QueryEvaluatorFactory. From this second factory we create the queryEvaluator for our specific database. Accessing the database is very easy, you just pass in a query to the QueryEvaluator and process the results. For my service I’ve created a set of commands that each execute and process a specific query. This command is executed from within the receive method of an Akka Actor:    class JsonActor extends Actor {       /**    * Based on the type recieved we determine what command to execute, most case classes    * can be executed using the normal two steps. Execute a query, convert result to    * a set of json data and return this result.    */   def receive = {          // when we receive a Command we process it and return the result     case some: Command =&gt; {              // execute the query from the FULL command and process the results using the       // processRows function       var records:Seq[GeoRecord] = null;              // if the match parameter is null we do the normal query, if not we pass in a set of varargs       some.parameters match {         case null =&gt;  records = Database.getQueryEvaluator.select(some.query) {some.processRows}         case _ =&gt; records = Database.getQueryEvaluator.select(some.query, some.parameters:_*) {some.processRows}       }               // return the result as a json string       self.reply(some.toJson(records))     }          case _ =&gt; self.reply(null)   } }    In this code you can see that we use the  Database.getQueryEvaluator function to get  QueryEvaluator. We then call the “select” operation on this QueryEvaluator with the some.query parameter, which holds the SQL Query. The result is passed on to the some.processRows function. To determine whether we need to pass in arguments to the select function, we use a case statement to check if the command provides us with a set of parameters to add. After the SQL query is processed we let the command format this result to JSon and return from the receive function.   To get a complete overview of how this works together lets look at the commands we pass to our actor.    sealed trait Command {   def name: String;   def query: String   var parameters: List[Any] = null;    /**    * Default function that maps georecords to json    */   def toJson(records: Seq[GeoRecord]): String = {     // map the object to json     val json =       (\"type\" -&gt; \"featureCollection\") ~         (\"features\" -&gt; records.map(r =&gt;           ((\"type\" -&gt; \"Feature\") ~             (\"gm_naam\" -&gt; r.name) ~             (\"geometry\" -&gt; parse(r.geojson)) ~             (\"properties\" -&gt; ({               // create an empty object               var obj = JNothing(0)               // iterate over the properties               r.properties.foreach(entry =&gt; (                 // add each property to the object, the reason                 // we do this is, that else it results in an                  // arraylist, not a list of seperate properties                 obj = concat(obj, JField(entry._1, entry._2))))               obj             })))))      return pretty(render(json))   }    /**    * Default function that processes resultsets to a georecord    */   def processRows(row: ResultSet): GeoRecord = {     // filter out the gid, the_geom and geo from the      // result, the rest is put into a map     val map = HashMap[String, String]()     val metadata = row.getMetaData();     for (i &lt;- 0 until metadata.getColumnCount()) {       map += (metadata.getColumnName(i + 1) -&gt; row.getString(i + 1));     }     // remove elements     val geo = map.get(\"geo\").get;     map -= (\"geo\", \"the_geom\", \"gid\")      new GeoRecord(map(\"gm_naam\"), geo, map)   }  }  /**  * Command to get all the information from the database.  */ case class FULL extends Command {   val name = \"full\";   val query = \"\"\"               SELECT *, ST_AsGeoJson(ST_Transform(ST_SetSRID(gem.the_geom,28992),4326)) as geo               FROM geodata.gem;           \"\"\" }  case class SINGLE extends Command {    val name = \"single\";   val query = \"\"\"             SELECT *, ST_AsGeoJson(ST_Transform(ST_SetSRID(gem.the_geom,28992),4326)) as geo               FROM geodata.gem WHERE gm_code = ?;           \"\"\" }    In the code above you see the FULL and SINGLE command. The FULL command retrieves all the records and the SINGLE command retrieves a single one. Since both these commands need to process the resultset in the same manner a trait is defined that contains the logic for processing the resultset ( def processRows(row: ResultSet): GeoRecord ) and for the transformation to json (def toJson(records: Seq[GeoRecord]): String).   To get the results of a specific command I only have to pass in the command itself and the rest is handled by Akka and Querulous. Creating new (select) queries is as easy as creating a basic new command.   ","categories": ["posts","querulous","postgresql","scala"],
        "tags": [],
        "url": "http://www.smartjava.org/content/using-querulous-scala-postgresql/",
        "teaser":null},{
        "title": "How should REST services be documented?",
        "excerpt":"REST has become the standard way of creating APIs and exposing resources on the internet. Traditional web services (using SOAP and various sets of WS-* standards) are still used a lot within enterprises, but have pretty much disappeared completely from the public API area and are replaced (or deprecated in favor of) REST based APIs.   REST based APIs are generally easier to use and get started with the SOAP based services and usually don’t require all kinds of code generation to create the messages and the envelopes. However, one thing that is often missing or overlooked, when creating REST based APIs or services is the documentation part. REST doesn’t have a WSDL that is used to describe to service (see section on WADL further down in the article) and often it is said REST resources should be self-documenting. Even though this is easy to say, it’s generally a good idea to provide additional documentation. In this article I’ll show you what you should document and how you can provide this documentation together with your resource.   Lets first look at an example resource. This resource, shown next, represent a report you can make to your local munipality. For instance you notice that a traffic light isn’t functioning, or there is a hole in the road. Your very modern municipality offers a REST based API you can use to report such events.    Content-Type: application/vnd.opengov.org.report+json;charset=UTF-8  {\"report\": {      \"self\": \"report-1\",                                \"status\": \"New\",      \"location\": \"Corner of ninth street\",      \"x-coordinate\": 52.34,      \"y-coordinate\":  4.34,      \"description\": \"There is ugly graffiti                       sprayed on the mailbox at the corner                      on ninth street\",      \"date\": \"25-11-2010\",      \"time\": \"15:46\"      \"images\": [       {\"href\": \"images/image1.png\"},                      {\"href\": \"images/image1.png\"}                      ],      \"related\":[        {\"href\": \"../report-4\"},                            {\"href\": \"../report-7\"},                            {\"href\": \"../report-9\"}                           ]      \"links\": [         {\"relation\": \"invalidation\",                                          \"href\": \"http://localhost:9002/opengov/invalidations/\"},           {\"relation\": \"duplication\",                                           \"href\": \"http://localhost:9002/opengov/duplications/\"}             {\"relation\": \"relation\",                                              \"href\": \"http://localhost:9002/opengov/relations/\"}              ]      \"comments\": []                                                   } }    REST services are often said to be self-documenting. If you look at this resource you can quickly pretty much already understand what this resource represents. It contains some general information about the resource:         \"self\": \"report-1\",                                \"status\": \"New\",      \"location\": \"Corner of ninth street\",      \"x-coordinate\": 52.34,      \"y-coordinate\":  4.34,      \"description\": \"There is ugly graffiti                       sprayed on the mailbox at the corner                      on ninth street\",      \"date\": \"25-11-2010\",      \"time\": \"15:46\"    Shows where you can find some related resources such as images related to this report:        \"images\": [       {\"href\": \"images/image1.png\"},                      {\"href\": \"images/image1.png\"}                      ]    Or other reports related to this report:         \"related\":[        {\"href\": \"../report-4\"},                            {\"href\": \"../report-7\"},                            {\"href\": \"../report-9\"}                           ]    Finally from this resource you can also see how you can traverse the links in this resource to report a duplication, invalidate this report or add a related report:         \"links\": [         {\"relation\": \"invalidation\",                                          \"href\": \"http://localhost:9002/opengov/invalidations/\"},           {\"relation\": \"duplication\",                                           \"href\": \"http://localhost:9002/opengov/duplications/\"}             {\"relation\": \"relation\",                                              \"href\": \"http://localhost:9002/opengov/relations/\"}              ]    As you can see this REST resource explains itself pretty well. For instance if we want to add a comment to this report we could just use a PUT with a comment message to the /reports/report-1/comments/ URL . But what does this comment message look like? How can we use the links? For this we do need some additional documentation to make our intent clear to the users of our service.   What you should describe are the following items:     URLs used to access or search for a report;   Links relations that describe how various resources are linked together.   Media Types that are used by this service;   Let’s make such a description for this service. The first thing we describe is the URL on which this service can be accessed:    URLs: http://localhost:9002/opengov/reports?location=xPos,yPos&amp;radius=r Submit a GET request to this URL to search for Reports. You can optionally specify a location and a radius to only return reports for a specific area. If no location and radius are specified the first 100 reports, sorted by date (newest first), are returned. The reports that are returned have the application/vnd.opengov.org.report+json  media type. xPos: x-coordinate of the location. Accepts GPS coordinates. yPos: y-coordinate of the location. Accepts GPS coordinates. r: radius to search for in meters.    This piece of documentation describes how to use our search service. We also explicitly define the media-type that this service returns. This way consumers already know how to act with the results from this search service.  Another important aspect of the documentation are the links we’ve defined:    Links: self: identifies the current resource. This (relative) URL can be used to directly access or modify a report.  http://localhost:9002/opengov/invalidations/: This URL can be used to invalidate this resource. Use an HTTP PUT operation on this URL with media type application/vnd.opengov.org.invalidation+json.   http://localhost:9002/opengov/duplications/: This URL can be used to mark a report as a duplicate. Use an HTTP PUT operation on this URL with media type application/vnd.opengov.org.duplication+json.  http://localhost:9002/opengov/relations/: This URL can be used to relate two reports to each other. Use an HTTP PUT operation on this URL with media type application/vnd.opengov.org.invalidation+json.    Here we describe what a specific link implies, how to use it, and what kind of media-types this link expects. That leaves us with describing the resources themselves. If you’re doing REST with XML, you should describe the message using an XSD. If you’re doing JSON you could use JSON-Schema, but I prefer to just describe the fields per media-type.    Media types: application/vnd.opengov.org.report+json - status: The status of this report - location: Readable description of the location of this report - etc.    If there are any restrictions on these values, this is a good place to describe them. Remember, we write this documentation for human consumption, we don’t require it to be parsable. With the items described above, you’ve got a very good starting point for creating documentation for REST services.   What you should keep in mind are the following items:     Follow the basic REST principles for the HTTP PUT, GET, DELETE and POST operations.   Use href/links when linking to other resources. It doesn’t really matter if you use relative links or absolute links for this, although relative links are more flexible should you relocate your resource.   Use media types to inform your consumer the type of resource they are dealing with.   Use links with a specific relation property to tell your consumers what you can do with this resource.   Add a simple description of the URLs, media types and links that are supported by your service.   Since I expect someone coming with this question, I’ll answer it beforehand. Why not use a WADL to describe your REST API?   A couple of reasons:     A WADL focusses more on machine-to-machine communication. REST services are more often created to be explored through human interaction. Not by generating a client stub WADL is a good concept but leans to much towards WSDLs. WSDLs are often used to create RPC based services that hide the underlying protocol. REST APIs are used in a different manner where this approach doesn't fit that well. An old article, but explains the why/why not of a WADL pretty well: http://bitworking.org/news/193/Do-we-need-WADL   In a follow up article I’ll show you how you can easily provide this documentation to your customers by storing it in a WSO2 Registry and serve it through a http://hostname/service?documentation link.  ","categories": ["posts","rest","governance","documentation"],
        "tags": [],
        "url": "http://www.smartjava.org/content/how-should-rest-services-be-documented/",
        "teaser":null},{
        "title": "Client certificates with HTTPClient 4",
        "excerpt":"I just wrote a small post on how to setup Jetty to support client-side certificates (http://www.smartjava.org/content/embedded-jetty-client-certificates). To easily test the configuration I use the latest HTTPClient from from http://hc.apache.org/httpcomponents-client-ga/. This is a great client, but, once again, the documentation on how to configure this client for two-way ssl isn’t that easy to be found. The following piece of java code uses HTTPClient to make a GET call using client-side certificates. In this example I haven’t defined a specific truststore for this client, since the server certificate is already trusted by my cacerts file.   \t\t// read in the keystore from the filesystem, this should contain a single keypair \t\tKeyStore clientKeyStore = KeyStore.getInstance(\"JKS\"); \t\tclientKeyStore.load(new FileInputStream(KEYSTORE_LOCATION), \t\t\t\tKEYSTORE_PASS.toCharArray());  \t\t// set up the socketfactory, to use our keystore for client authentication. \t\tSSLSocketFactory socketFactory = new SSLSocketFactory( \t\t\t\tSSLSocketFactory.TLS, \t\t\t\tclientKeyStore, \t\t\t\tKEYSTORE_PASS, \t\t\t\tnull, \t\t\t\tnull, \t\t\t\tnull, \t\t\t\t(X509HostnameVerifier) SSLSocketFactory.ALLOW_ALL_HOSTNAME_VERIFIER);  \t\t// create and configure scheme registry \t\tSchemeRegistry registry = new SchemeRegistry(); \t\tregistry.register(new Scheme(\"https\", 443, socketFactory));  \t\t// create a client connection manager to use in creating httpclients \t\tThreadSafeClientConnManager mgr = new ThreadSafeClientConnManager(registry);  \t\t// create the client based on the manager, and use it to make the call \t\thttpClient = new DefaultHttpClient(mgr);  \t\t// create the method to execute \t\tHttpGet m = new HttpGet(path);  \t\t// execute the method \t\tHttpResponse response = httpClient.execute(m);   A note on this section:   (X509HostnameVerifier) SSLSocketFactory.ALLOW_ALL_HOSTNAME_VERIFIER);   This tells the SSLSocketFactory to accept the certificate even if the hostname doesn’t match the information from the certificate. Especially useful when testing using self-signed certificates or changing ip-addresses. Note that you shouldn’t, of course, use this in production.  ","categories": ["posts"],
        "tags": [],
        "url": "http://www.smartjava.org/content/client-certificates-httpclient-4/",
        "teaser":null},{
        "title": "Embedded Jetty with client certificates",
        "excerpt":"Every time I start with an embedded Jetty server, I can’t find any up to date documentation and have to mess around with old, obsolete and deprecated documentation to get things working.   Today was no different… I wanted to create a simple embedded HTTPS server that required the clients to present a certificate. Most documentation I could find was relevant for older versions and either wasn’t working, or presented me with a whole load of deprecation warnings. However, after some messing around, I managed to get this working, and, as with most things, it wasn’t really that hard and using the javadocs, actually very straightforward.   So for those of you using Jetty 8 and wanting to use client-side ssl, you can use the following java class to start a HTTPS server, which in this example contains a single servlet.   import org.eclipse.jetty.server.Connector; import org.eclipse.jetty.server.Server; import org.eclipse.jetty.server.ssl.SslSocketConnector; import org.eclipse.jetty.servlet.ServletContextHandler; import org.eclipse.jetty.util.ssl.SslContextFactory;  public class IdentityForwardingProxy {  \t// the keystore (with one key) we'll use to make the connection with the \t// broker \tprivate final static String KEYSTORE_LOCATION = \"src/main/resources/client_keystore.jks\"; \tprivate final static String KEYSTORE_PASS = \"secret\"; \t \t// the truststore we use for our server. This keystore should contain all the keys \t// that are allowed to make a connection to the server \tprivate final static String TRUSTSTORE_LOCATION = \"src/main/resources/truststore.jks\"; \tprivate final static String TRUSTSTORE_PASS = \"secret\";  \t/** \t * Simple starter for a jetty HTTPS server. \t *  \t * @param args \t * @throws Exception  \t */ \tpublic static void main(String[] args) throws Exception {  \t\t// create a jetty server and setup the SSL context \t\tServer server = new Server(); \t\tSslContextFactory sslContextFactory = new SslContextFactory(KEYSTORE_LOCATION); \t\tsslContextFactory.setKeyStorePassword(KEYSTORE_PASS); \t\tsslContextFactory.setTrustStore(TRUSTSTORE_LOCATION); \t\tsslContextFactory.setTrustStorePassword(TRUSTSTORE_PASS); \t\tsslContextFactory.setNeedClientAuth(true); \t\t \t\t// create a https connector \t\tSslSocketConnector connector = new SslSocketConnector(sslContextFactory); \t\tconnector.setPort(8443); \t \t\t// register the connector \t\tserver.setConnectors(new Connector[] { connector }); \t\t \t\tServletContextHandler scHandler = new ServletContextHandler(server,\"/\"); \t\tscHandler.addServlet(NameOfServlet.class, \"/\"); \t\tserver.start(); \t\tserver.join(); \t} }  ","categories": ["posts","client-certificates","java","ssl","jetty"],
        "tags": [],
        "url": "http://www.smartjava.org/content/embedded-jetty-with-client-certificates/",
        "teaser":null},{
        "title": "Play 2.0: Akka, Rest, Json and dependencies",
        "excerpt":"I’ve been diving more and more into scala the last couple of months. Scala together with the “Play Framework” provides you with a very effective and quick development environment (as soon as you’ve grasped the idiosyncrasies of the Scala language, that is). The guys behind the Play framework have been hard at work at the new version Play 2.0. In Play 2.0 scala plays a much more important role, and especially the complete build process has been immensely improved. The only problem so far, I’ve encountered with Play 2.0 is the lack of good documentation. The guys are hard at work at updating the wiki, but its often still a lot of trial and error to get what you want. Note though, that often this isn’t just caused by Play, I also sometimes still struggle with the more exotic Scala constructs ;-)   In this article, I’ll give you an introduction into how you can accomplish some common tasks in Play 2.0 using Scala. More specifically I’ll show you how to create an application that:      uses sbt based dependency management to configure external dependencies   is edited in Eclipse (with the Scala-ide plugin) using the play eclipsify command   provides a Rest API using Play's routes   uses Akka 2.0 (provided by the Play framework) to asynchronously call the database and generate Json (just because we can)   convert Scala objects to Json using the Play provided Json functionality (based on jerkson)   I won’t show the database access using Querulous, if you want to know more about that look at this article. I’d like to convert the Querulous code to using Anorm. But since my last experiences with Anorm were, how do I put this, not convincingly positive, I’m saving that for a later day.   Creating an application with Play 2.0  Getting up and running with Play 2.0 is very easy and is well documented, so I won’t spent too much time on this. For complete instruction see the Play 2.0 Wiki. To get up and running, after you you have downloaded and extracted Play 2.0, take the following steps:   Execute the following command from the console:    $play new FirstStepsWithPlay20    This will create a new project, and show you something like the following output:           _            _   _ __ | | __ _ _  _| | | '_ \\| |/ _' | || |_| |  __/|_|\\____|\\__ (_) |_|            |__/                play! 2.0-RC2, http://www.playframework.org  The new application will be created in /Users/jos/Dev/play-2.0-RC2/FirstStepsWithPlay20  What is the application name?  &gt; FirstStepsWithPlay20  Which template do you want to use for this new application?     1 - Create a simple Scala application   2 - Create a simple Java application   3 - Create an empty project  &gt; 1  OK, application FirstStepsWithPlay20 is created.  Have fun!    You’ve now got an application you can run. Change to the just created directory and execute play run.    $ play run  [info] Loading project definition from /Users/jos/Dev/play-2.0-RC2/FirstStepsWithPlay2/project [info] Set current project to FirstStepsWithPlay2 (in build file:/Users/jos/Dev/play-2.0-RC2/FirstStepsWithPlay2/)  --- (Running the application from SBT, auto-reloading is enabled) ---  [info] play - Listening for HTTP on port 9000...  (Server started, use Ctrl+D to stop and go back to the console...)    If you navigate to http://localhost:9000, you can see your first Play 2.0 application. And you’re done with the basic installation of Play 2.0.   Dependency management  I mentioned in the introduction that I didn’t start this project from scratch. I rewrote a Rest service I made with Play 1.2.4, Akka 1.x, JAX-RS and Json-Lift to the components provided by the Play 2.0 framework. Since dependency management changed between Play 1.2.4 and Play 2.0, I needed to configure my new project with the dependencies I needed. In Play 2.0 you do this in a file called build.scala, which you can find in the project folder in your project. After adding the dependencies from my previous project, this file looked like this:    import sbt._ import Keys._ import PlayProject._  object ApplicationBuild extends Build {      val appName         = \"FirstStepsWithPlay2\"     val appVersion      = \"1.0-SNAPSHOT\"      val appDependencies = Seq(       \"com.twitter\" % \"querulous\" % \"2.6.5\" ,       \"net.liftweb\" %% \"lift-json\" % \"2.4\" ,       \"com.sun.jersey\" % \"jersey-server\" % \"1.4\" ,       \"com.sun.jersey\" % \"jersey-core\" % \"1.4\" ,        \"postgresql\" % \"postgresql\" % \"9.1-901.jdbc4\"     )      val main = PlayProject(appName, appVersion, appDependencies, mainLang = SCALA).settings(       // Add extra resolver for the twitter           resolvers += \"Twitter repo\" at \"http://maven.twttr.com/\" ,         resolvers += \"DevJava repo\" at \"http://download.java.net/maven/2/\"     ) }    How to use this file is rather straightforward, once you’ve read the sbt documentation (http://code.google.com/p/simple-build-tool/wiki/LibraryManagement). Basically we define the libraries we want, using appDependencies, and we define some extra repositories where sbt should download its dependencies from (using resolvers). A nice thing to mention is that you can specify a %% when defining dependencies. This implies that we also want to search for a library that matches our version of scala. SBT looks at our current configured version and adds a qualifier for that version. This makes sure we get a version that works for our Scala version.  Like I mentioned, I wanted to replace most external libraries I used with functionality from Play 2.0. After removing the stuff I didn’t use anymore this file looks like this:    import sbt._ import Keys._ import PlayProject._  object ApplicationBuild extends Build {      val appName         = \"FirstStepsWithPlay2\"     val appVersion      = \"1.0-SNAPSHOT\"      val appDependencies = Seq(       \"com.twitter\" % \"querulous\" % \"2.6.5\" ,       \"postgresql\" % \"postgresql\" % \"9.1-901.jdbc4\"     )      val main = PlayProject(appName, appVersion, appDependencies, mainLang = SCALA).settings(       // Add extra resolver for the twitter           resolvers += \"Twitter repo\" at \"http://maven.twttr.com/\"     ) }    With the dependencies configured, I can configure this project for my IDE. Even though all my colleagues are big IntelliJ proponents, I’m still coming back to what I’m used to: Eclipse. So lets see what you need to do to get this project up and running in Eclipse.   Work from Eclipse  In my Eclipse version I’ve got the scala plugin installed, and the Play 2.0 framework nicely works together with this plugin. To get your project in eclipse all you have to do is run the following command: play eclipsify    jos@Joss-MacBook-Pro.local:~/dev/play-2.0-RC2/FirstStepsWithPlay2$ ../play eclipsify [info] Loading project definition from /Users/jos/Dev/play-2.0-RC2/FirstStepsWithPlay2/project [info] Set current project to FirstStepsWithPlay2 (in build file:/Users/jos/Dev/play-2.0-RC2/FirstStepsWithPlay2/) [info] About to create Eclipse project files for your project(s). [info] Compiling 1 Scala source to /Users/jos/Dev/play-2.0-RC2/FirstStepsWithPlay2/target/scala-2.9.1/classes... [info] Successfully created Eclipse project files for project(s): FirstStepsWithPlay2 jos@Joss-MacBook-Pro.local:~/dev/play-2.0-RC2/FirstStepsWithPlay2$     Now you can use “import project” from Eclipse, and you can edit your Play 2.0 / Scala project directly from Eclipse. Its possible to start the Play environment directly from Eclipse, but I haven’t used that. I just start the Play project from the command line, once, and all the changes I make in Eclipse are immediately visible. For those of you who’ve worked with Play longer, this is probably not so special anymore. For me, personally, I still am amazed by the productivity of this environment.   provides a Rest API using Play's routes  In my previous Play project I used the jersey module to be able to use JAX-RS annotations to specify my Rest API. Since Play 2.0 contains a lot of breaking API changes and is pretty much a rewrite from the ground up, you can’t expect all the old modules to work. This was also the case for the Jersey module. I did dive into the code of this module, to see if the changes were trivial, but since I couldn’t find any documentation on how to create a plugin for Play 2.0 that allows you to interact with the route processing, I decided to just switch to the way Play 2.0 does Rest. And using the “routes” file, it was very easy to connect the (just) two operations I exposed to a simple controller:    # Routes # This file defines all application routes (Higher priority routes first) # ~~~~  GET     /resources/rest/geo/list    controllers.Application.processGetAllRequest GET     /resources/rest/geo/:id     controllers.Application.processGetSingleRequest(id:String)    The corresponding controller looks like this:    package controllers  import akkawebtemplate.GeoJsonService import play.api.mvc.Action import play.api.mvc.Controller  object Application extends Controller {      val service = new GeoJsonService()      def processGetSingleRequest(code: String) = Action {     val result = service.processGetSingleRequest(code)     Ok(result).as(\"application/json\")   }      def processGetAllRequest() = Action {     val result = service.processGetAllRequest;     Ok(result).as(\"application/json\");   }  }    As you can see I’ve just created to very simple, basic actions. Haven’t look at fault and exception handling yet, but the Rest API offered by Play really makes using additional Rest framework unnecessary. Thats the first of the frameworks. The next part of my original application that needed to change was the Akka code. Play 2.0 includes the latest version of the Akka library (2.0-RC1). Since my original Akka code was written against 1.2.4, there were a lot of conflicts. Updating the original code, wasn’t so easy though.   Using Akka 2.0   I won’t dive into all the problems I had with Akka 2.0. Biggest problem was the very crappy documentation on the Play Wiki and the crappy documentation on the Akka website my crappy skills at locating the correct information in the Akka documentation. Together with me only using Akka for about three or four months, doesn’t make it the best combination. After a couple of frustation hours though, I just removed all the existing Akka code, and started from scratch. 20 minutes later I got everything working with Akka 2, and using the master configuration from Play. In the next listing you can see the corresponding code (I’ve intentionally left the imports, since in a lot of examples you can find, they are omitted, which makes an easy job, that much harder)    import akka.actor.actorRef2Scala import akka.actor.Actor import akka.actor.Props import akka.dispatch.Await import akka.pattern.ask import akka.util.duration.intToDurationInt import akka.util.Timeout import model.GeoRecord import play.libs.Akka import resources.commands.Command import resources.commands.FULL import resources.commands.SINGLE import resources.Database  /**  * This actor is responsible for returning JSON objects from the database. It uses querulous to   * query the database and parses the result into the GeoRecord class.  */ class JsonActor extends Actor {       /**    * Based on the type recieved we determine what command to execute, most case classes    * can be executed using the normal two steps. Execute a query, convert result to    * a set of json data and return this result.    */   def receive = {          // when we receive a Command we process it and return the result     case some: Command =&gt; {              // execute the query from the FULL command and process the results using the       // processRows function       var records:Seq[GeoRecord] = null;              // if the match parameter is null we do the normal query, if not we pass in a set of varargs       some.parameters match {         case null =&gt;  records = Database.getQueryEvaluator.select(some.query) {some.processRows}         case _ =&gt; records = Database.getQueryEvaluator.select(some.query, some.parameters:_*) {some.processRows}       }       // return the result as a json string       sender ! some.toJson(records)     }           case _ =&gt; sender ! null   } }  /**  * Handle the specified path. This rest service delegates the functionality to a specific actor  * and if the result from this actor isn't null return the result  */ class GeoJsonService {     def processGetSingleRequest(code: String) = {       val command = SINGLE();       command.parameters = List(code);       runCommand(command);   }     /**    * Operation that handles the list REST command. This creates a command    * that forwards to the actor to be executed.    */   def processGetAllRequest:String = { \t  runCommand(FULL());   }      /**    * Function that runs a command on one of the actors and sets the response    */   private def runCommand(command: Command):String =  {      // get the actor     val actor = Akka.system.actorOf(Props[JsonActor])     implicit val timeout = Timeout(5 seconds)     val result = Await.result(actor ? command, timeout.duration).asInstanceOf[String]     // return result as String     result   } }    A lot of code, but I wanted to show you the actor definition and how to use them. Summarizing, the Akka 2.0 code you need to use, to execute a request/reply pattern with Akka is this:      private def runCommand(command: Command):String =  {      // get the actor     val actor = Akka.system.actorOf(Props[JsonActor])     implicit val timeout = Timeout(5 seconds)     val result = Await.result(actor ? command, timeout.duration).asInstanceOf[String]     // return result as String     result   }    This uses the global Akka configuration to retrieve an actor of the required type. We then send a command to the actor, and are returned a Future, on which we wait 5 seconds for a result, which we cast to a String. This Future waits for our Actor to send a reply. This is done in the actor itself:     sender ! some.toJson(records)    With Akka replaced I finally got a working system again. When looking through the documentation on Play 2.0 I noticed that they provided their own Json library, starting from 2.0. Since I used Json-Lift in the previous version, I thought it would be a nice exercise to move this code to the Json library, named Jerkson, provided by Play.   Moving to Jerkson   The move to the new library was a fairly easy one. Both Lift-Json and Jerkson use pretty much the same concept of building Json objects. In the old version I didn’t use any automatic marshalling (since I had to comply with the jsongeo format) so in this version I also did the marshalling manually. In the next listing you can see the old version and the new version together, As you can see the concepts used in both are pretty much the same.    #New version using jerkson     val jsonstring = JsObject(       List(\"type\" -&gt; JsString(\"featureCollection\"),         \"features\" -&gt; JsArray(           records.map(r =&gt;             (JsObject(List(               \"type\" -&gt; JsString(\"Feature\"),               \"gm_naam\" -&gt; JsString(r.name),               \"geometry\" -&gt; Json.parse(r.geojson),               \"properties\" -&gt; ({\t                 var toAdd = List[(String, play.api.libs.json.JsValue)]()                 r.properties.foreach(entry =&gt; (toAdd ::= entry._1 -&gt; JsString(entry._2)))                 JsObject(toAdd)               })))))             .toList)))  #Old version using Lift-Json     val json =       (\"type\" -&gt; \"featureCollection\") ~         (\"features\" -&gt; records.map(r =&gt;           ((\"type\" -&gt; \"Feature\") ~             (\"gm_naam\" -&gt; r.name) ~             (\"geometry\" -&gt; parse(r.geojson)) ~             (\"properties\" -&gt; ({               // create an empty object               var obj = JNothing(0)               // iterate over the properties               r.properties.foreach(entry =&gt; (                 // add each property to the object, the reason                 // we do this is, that else it results in an                  // arraylist, not a list of seperate properties                 obj = concat(obj, JField(entry._1, entry._2))))               obj             })))))    And after all this, I have exactly the same as I already had. But now with Play 2.0 and not using any external libraries (except Querulous). So far my experiences with Play 2.0 have been very positive. The lack of good concrete examples and documentation can be annoying sometimes, but is understandable. They do provide a couple of extensive examples in their distribution, but nothing that matched my use cases. So hats off to the guys who are responsible for Play 2.0. What I’ve seen so far, great and comprehensive framework, lots of functionality and a great environment to program scala in. In the next couple of weeks I’ll see if I can get enough courage up to start with Anorm and I’ll look at what Play has to offer on the client side. So far I’ve looked at LESS which I really like, so I’ve got my hopes up for their template solution ;-)  ","categories": ["posts","json","scala","rest","play"],
        "tags": [],
        "url": "http://www.smartjava.org/content/play-20-akka-rest-json-and-dependencies/",
        "teaser":null},{
        "title": "How to analyze Java SSL errors",
        "excerpt":"In my recent projects I’ve had to do a lot with certificates, java and HTTPS with client-side authentication. In most of these projects, either during testing, or setting up a new environment, I’ve run into various SSL configuration errors that often resulted in a rather uncomprehensive error such as:    javax.net.ssl.SSLPeerUnverifiedException: peer not authenticated \tat com.sun.net.ssl.internal.ssl.SSLSessionImpl.getPeerCertificates(SSLSessionImpl.java:352) \tat org.apache.http.conn.ssl.AbstractVerifier.verify(AbstractVerifier.java:128) \tat org.apache.http.conn.ssl.SSLSocketFactory.connectSocket(SSLSocketFactory.java:397) \tat org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:148) \tat org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:150) \tat org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:121) \tat org.apache.http.impl.client.DefaultRequestDirector.tryConnect(DefaultRequestDirector.java:575) \tat org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:425) \tat org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:820) \tat org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:754) \tat org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:732)    In most of the cases it was misconfiguration where keystores didn’t containt the correct certificates, the certificate chain was incomplete or the client didn’t supply a valid certificate. So in the last project I decided to document what was happening and what caused specific errors during the SSL handshake.   In this article I’ll show you why specific SSL errors occur, how you can detect them by analyzing the handshake information, and how to solve them. For this I use the following scenario:      Server uses a certificate issued by a CA and requires client authentication. The server uses a simple truststore that lists this CA as trusted.   Client connects using a certificate issued by this single trusted CA and has it's own trustore that also contains this certificate from the server.   Not a very complicated situation, but one you often see. Note that the following information can also be used to identify problems when you don’t work with client certificates or use self-signed certificates. The way to determine the problem in those cases, is pretty much the same.   Happy Flow   First we’ll look at the happy flow, what happens in the handshake when we use client certificates. We won’t look at the complete negotiation phase, but only until both the client and the server have exchanged their certificates and have validated the received certificate. If everything goes well until that point, the rest should work. The following is what you see when you run the client and the server using the java VM parameter: -Djavax.net.debug=ssl:handshake.   The first thing that happens is that the client sends a ClientHello message using the TLS protocol version he supports, a random number and a list of suggested cipher suites and compression methods. From our client this looks like this:   Client sends:    *** ClientHello, TLSv1 RandomCookie:  GMT: 1331663143 bytes = { 141, 219, 18, 140, 148, 60, 33, 241, 10, 21, 31, 90, 88, 145, 34, 153, 238, 105, 148, 72, 163, 210, 233, 49, 99, 224, 226, 64 } Session ID:  {} Cipher Suites: [SSL_RSA_WITH_RC4_128_MD5, SSL_RSA_WITH_RC4_128_SHA, TLS_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_256_CBC_SHA, TLS_DHE_RSA_WITH_AES_128_CBC_SHA, TLS_DHE_RSA_WITH_AES_256_CBC_SHA, TLS_DHE_DSS_WITH_AES_128_CBC_SHA, TLS_DHE_DSS_WITH_AES_256_CBC_SHA, SSL_RSA_WITH_3DES_EDE_CBC_SHA, SSL_DHE_RSA_WITH_3DES_EDE_CBC_SHA, SSL_DHE_DSS_WITH_3DES_EDE_CBC_SHA, SSL_RSA_WITH_DES_CBC_SHA, SSL_DHE_RSA_WITH_DES_CBC_SHA, SSL_DHE_DSS_WITH_DES_CBC_SHA, SSL_RSA_EXPORT_WITH_RC4_40_MD5, SSL_RSA_EXPORT_WITH_DES40_CBC_SHA, SSL_DHE_RSA_EXPORT_WITH_DES40_CBC_SHA, SSL_DHE_DSS_EXPORT_WITH_DES40_CBC_SHA, TLS_EMPTY_RENEGOTIATION_INFO_SCSV] Compression Methods:  { 0 } ***    The server responds, very originally, with a ServerHello message, that contains the choices made based on the information provided by the client another random number and (optionally) a session id.   Server sends:    *** ServerHello, TLSv1 RandomCookie:  GMT: 1331663143 bytes = { 172, 233, 79, 197, 14, 21, 187, 161, 114, 206, 7, 38, 188, 228, 120, 102, 115, 214, 155, 86, 211, 41, 156, 179, 138, 2, 230, 81 } Session ID:  {79, 96, 145, 39, 203, 136, 206, 69, 170, 46, 194, 17, 154, 175, 13, 138, 143, 199, 162, 193, 110, 86, 113, 109, 248, 187, 220, 169, 47, 180, 44, 68} Cipher Suite: SSL_RSA_WITH_RC4_128_MD5 Compression Method: 0 Extension renegotiation_info, renegotiated_connection: &lt;empty&gt; ***    So in this case we’re going to use SSL_RSA_WITH_RC4_128_MD5 as Cipher Suite. The next step is also done by the server. The server next sends a Certificate message that contains its complete certificate chain:   Server sends:    *** Certificate chain chain [0] = [ [   Version: V1   Subject: CN=server, C=NL   Signature Algorithm: SHA1withRSA, OID = 1.2.840.113549.1.1.5    Key:  Sun RSA public key, 1024 bits   modulus: 143864428144045085986129639694300995179398936575198896494655652087658861594939489453166811774109137006267822033915476680673848164790815913192075840268069822357600376998775923266017630332239546722181180383155088413406178660120548292599278819762883993031950564327152510982887716901499177102158407884939613382007   public exponent: 65537   Validity: [From: Wed Mar 14 13:32:04 CET 2012,                To: Thu Mar 14 13:32:04 CET 2013]   Issuer: CN=Application CA, OU=GKD, O=Smartjava, L=Maasland, ST=ZH, C=NL   SerialNumber: [    a881d144 5e631f21]  ]   Algorithm: [SHA1withRSA]   Signature: 0000: C3 56 81 7F 33 91 8A FF   84 5E 0B BA 7A 01 D8 41  .V..3....^..z..A 0010: 6B 47 B2 F7 8F FB B5 77   23 D8 FB B2 35 19 6E C4  kG.....w#...5.n. 0020: A4 6A BC 23 BB 69 92 F6   85 5A 1E CB FE 23 C6 98  .j.#.i...Z...#.. 0030: A0 57 F8 FB E9 DB B0 40   BD 8E F8 35 F8 77 E1 09  .W.....@...5.w.. 0040: 5A 2E 45 71 80 F6 89 E7   0B 93 E2 48 EB 40 92 13  Z.Eq.......H.@.. 0050: 14 AA 1F 59 AA 98 67 46   9B 52 33 49 9A 3C 91 9B  ...Y..gF.R3I.&lt;.. 0060: F1 CB 8A BD 7D D4 DD 76   C4 15 00 36 A3 B2 87 A7  .......v...6.... 0070: D5 FF 52 E3 68 D4 F0 E0   32 86 74 02 DD 92 EC 1D  ..R.h...2.t.....  ] chain [1] = [ [   Version: V3   Subject: CN=Application CA, OU=SL, O=SmartJava, L=Waalwijk, ST=ZH, C=NL   Signature Algorithm: SHA1withRSA, OID = 1.2.840.113549.1.1.5    Key:  Sun RSA public key, 1024 bits   modulus: 159927271510058538658170959055540487654246676457579822126433656091883150307639380685203152841988861440546492270915750324654620063428634486478674507234742748515614639629692189315918046446256610037776978028900716455223387878926383828815082154427031884246429239077082613371662803582187768145965112751392402313823   public exponent: 65537   Validity: [From: Mon Mar 12 13:35:16 CET 2012,                To: Wed Apr 11 14:35:16 CEST 2012]   Issuer: CN=Application CA, OU=CA, O=Blaat, L=Waalwijk, ST=ZH, C=NL   SerialNumber: [    fe7636c5 6804e69c]  Certificate Extensions: 3 [1]: ObjectId: 2.5.29.14 Criticality=false SubjectKeyIdentifier [ KeyIdentifier [ 0000: 6C CC 48 03 E4 BE 07 D6   9E F6 4C 78 53 54 A2 B8  l.H.......LxST.. 0010: 7B DA 40 09                                        ..@. ] ]  [2]: ObjectId: 2.5.29.35 Criticality=false AuthorityKeyIdentifier [ KeyIdentifier [ 0000: 6C CC 48 03 E4 BE 07 D6   9E F6 4C 78 53 54 A2 B8  l.H.......LxST.. 0010: 7B DA 40 09                                        ..@. ]  ]  [3]: ObjectId: 2.5.29.19 Criticality=false BasicConstraints:[   CA:true   PathLen:2147483647 ]  ]   Algorithm: [SHA1withRSA]   Signature: 0000: 1A 30 08 15 01 8E A6 36   5F 38 22 C6 81 5E 69 B1  .0.....6_8\"..^i. 0010: 42 9A 1E FF 0F C4 D7 40   5F 85 0E 42 35 E0 CC 00  B......@_..B5... 0020: 6E A5 2E 70 6B 79 64 C5   99 AE A4 29 CB 26 DE 60  n..pkyd....).&amp;.` 0030: 0B A6 AB 19 06 6F 19 54   6C 1A 88 9E 3A 6A D4 BB  .....o.Tl...:j.. 0040: CB 28 85 2F 72 4D DE 35   C0 9B F4 2F EF 8E 6D E8  .(./rM.5.../..m. 0050: 30 AC 12 7D B4 0D A3 08   DA D4 60 46 94 BD 12 AF  0.........`F.... 0060: 44 F7 C3 B8 9D 69 2D 6A   32 C8 4D AE 12 60 05 09  D....i-j2.M..`.. 0070: FE AE D0 1A 72 6D 91 CE   DA 7C 8E D5 31 14 31 4C  ....rm......1.1L  ]    In this message you can see that the issuer of this certificate is our example CA. Our client checks to see if this certificate is trusted, which it is in this case. Since we require the client to authenticate itself the server requests a certificate from the client and after that sends a helloDone.   Server sends:    *** CertificateRequest Cert Types: RSA, DSS Cert Authorities: &lt;CN=Application CA, OU=CA, O=Blaat, L=Waalwijk, ST=ZH, C=NL&gt; *** ServerHelloDone    In this message you can see that the server provides a list of Cert Authorities it trusts. The client will use this information to determine if it has a keypair that matches this CA. In our happy flow, it has one and responds with a Certificate message.   Client sends:    *** Certificate chain chain [0] = [ [   Version: V1   Subject: CN=Application 3, OU=Smartjava, O=Smartjava, L=NL, ST=ZH, C=NL   Signature Algorithm: SHA1withRSA, OID = 1.2.840.113549.1.1.5    Key:  Sun RSA public key, 1024 bits   modulus: 90655907749318585147523875906892969031300830816947226352221659107570169820452561428696751943383590982109524990627182456571533992582229229163232831159652561902456847954385746762477844009336466314872376131553489447601649924116778337873632641536164462534398137791450495316700015095054427027256393580022887087767   public exponent: 65537   Validity: [From: Mon Mar 12 15:13:24 CET 2012,                To: Tue Mar 12 15:13:24 CET 2013]   Issuer: CN=Application CA, OU=Smartjava, O=Smartjava, L=Maasland, ST=ZH, C=NL   SerialNumber: [    b247ffb2 ce060768]  ]   Algorithm: [SHA1withRSA]   Signature: 0000: 97 58 36 C5 28 87 B3 16   9B DD 31 0C E0 C6 23 76  .X6.(.....1...#v 0010: 72 82 5B 13 4D 23 B6 0E   A9 2F 9F 0C 3F 97 15 6E  r.[.M#.../..?..n 0020: 7B 38 EC DE E2 57 D7 AA   07 12 E3 98 B7 86 A7 CE  .8...W.......... 0030: 57 8E A1 29 96 C9 F0 30   57 67 C7 F1 F2 98 90 64  W..)...0Wg.....d 0040: 6C B9 6C 05 24 8B 56 3F   B1 FF 03 62 3D 81 DB 45  l.l.$.V?...b=..E 0050: D3 1F C1 B2 DD 77 CF 74   54 EB 9D 82 23 89 1A 70  .....w.tT...#..p 0060: F8 C4 68 6A B7 41 C7 DE   7B B6 3A 0C 17 E7 FA 98  ..hj.A....:..... 0070: 19 0C D8 91 FB 5E FE D2   B3 92 FD 2D 2A 6B 51 10  .....^.....-*kQ.  ] chain [1] = [ [   Version: V3   Subject: CN=Application CA, OU=Smartjava, O=Smartjava, L=Maasland, ST=ZH, C=NL   Signature Algorithm: SHA1withRSA, OID = 1.2.840.113549.1.1.5    Key:  Sun RSA public key, 1024 bits   modulus: 159927271510058538658170959055540487654246676457579822126433656091883150307639380685203152841988861440546492270915750324654620063428634486478674507234742748515614639629692189315918046446256610037776978028900716455223387878926383828815082154427031884246429239077082613371662803582187768145965112751392402313823   public exponent: 65537   Validity: [From: Mon Mar 12 13:35:16 CET 2012,                To: Wed Apr 11 14:35:16 CEST 2012]   Issuer: CN=Application CA, OU=Smartjava, O=Smartjava, L=Maasland, ST=ZH, C=NL   SerialNumber: [    fe7636c5 6804e69c]  Certificate Extensions: 3 [1]: ObjectId: 2.5.29.14 Criticality=false SubjectKeyIdentifier [ KeyIdentifier [ 0000: 6C CC 48 03 E4 BE 07 D6   9E F6 4C 78 53 54 A2 B8  l.H.......LxST.. 0010: 7B DA 40 09                                        ..@. ] ]  [2]: ObjectId: 2.5.29.35 Criticality=false AuthorityKeyIdentifier [ KeyIdentifier [ 0000: 6C CC 48 03 E4 BE 07 D6   9E F6 4C 78 53 54 A2 B8  l.H.......LxST.. 0010: 7B DA 40 09                                        ..@. ]  ]  [3]: ObjectId: 2.5.29.19 Criticality=false BasicConstraints:[   CA:true   PathLen:2147483647 ]  ]   Algorithm: [SHA1withRSA]   Signature: 0000: 1A 30 08 15 01 8E A6 36   5F 38 22 C6 81 5E 69 B1  .0.....6_8\"..^i. 0010: 42 9A 1E FF 0F C4 D7 40   5F 85 0E 42 35 E0 CC 00  B......@_..B5... 0020: 6E A5 2E 70 6B 79 64 C5   99 AE A4 29 CB 26 DE 60  n..pkyd....).&amp;.` 0030: 0B A6 AB 19 06 6F 19 54   6C 1A 88 9E 3A 6A D4 BB  .....o.Tl...:j.. 0040: CB 28 85 2F 72 4D DE 35   C0 9B F4 2F EF 8E 6D E8  .(./rM.5.../..m. 0050: 30 AC 12 7D B4 0D A3 08   DA D4 60 46 94 BD 12 AF  0.........`F.... 0060: 44 F7 C3 B8 9D 69 2D 6A   32 C8 4D AE 12 60 05 09  D....i-j2.M..`.. 0070: FE AE D0 1A 72 6D 91 CE   DA 7C 8E D5 31 14 31 4C  ....rm......1.1L  ]    This certificate is checked on the server side and if all is well, the final steps in the handshake are executed to setup the secured connection. Note that there is a CertificateVerify step. In this step the client signs a message with its private key. This is done so the server can verify the client has access to its private key. This might seem a step where things can go wrong in an incorrectly configured environment. In the default java implementation this won’t happen. In the phase where the client has to determine which certificate to present to the server, the java implementation already checks if the privatekey is available.   What could possibly go wrong   So what could possibly go wrong in this handshake? In the next couple of sections we’ll look at some scenarios, and how to detect them.   Passwords   Now that we’ve seen what happens when things go right, lets look at a couple of scenarios where things go wrong. We’ll start simple with the following exception, that we get at the moment we start up the client application:   Exception in thread “main” java.security.UnrecoverableKeyException: Cannot recover key \tat sun.security.provider.KeyProtector.recover(KeyProtector.java:311) \tat sun.security.provider.JavaKeyStore.engineGetKey(JavaKeyStore.java:121) \tat sun.security.provider.JavaKeyStore$JKS.engineGetKey(JavaKeyStore.java:38) \tat java.security.KeyStore.getKey(KeyStore.java:763) \tat com.sun.net.ssl.internal.ssl.SunX509KeyManagerImpl.(SunX509KeyManagerImpl.java:113) \tat com.sun.net.ssl.internal.ssl.KeyManagerFactoryImpl$SunX509.engineInit(KeyManagerFactoryImpl.java:48) \tat javax.net.ssl.KeyManagerFactory.init(KeyManagerFactory.java:239) \tat org.apache.http.conn.ssl.SSLSocketFactory.createSSLContext(SSLSocketFactory.java:186) \tat org.apache.http.conn.ssl.SSLSocketFactory.(SSLSocketFactory.java:260)   This very helpful message is thrown when (from the javadoc) “ .. a key in the keystore cannot be recovered”. There are a couple of reasons this can happen, but normally this occurs when the key in the keystore is accessed with the wrong password. Usually when you use the keytool to create and manage your keys, the keystore password is usually the same as the key password. However, if you import keys from a PKCS#12 type keystore, the password of the keystore can be easily set to a different value. Not all the SSL client allow you to specify a different password for the key and the keystore. If that is the case you can use the following command, to change the password of the key:    keytool -keypasswd -alias &lt;keyalias&gt; -keystore &lt;keystore&gt;    It is also possible to set an incorrect password for the keystore. Luckily in that case the error message that is thrown is much more helpful:    Exception in thread \"main\" java.io.IOException: Keystore was tampered with, or password was incorrect \tat sun.security.provider.JavaKeyStore.engineLoad(JavaKeyStore.java:771) \tat sun.security.provider.JavaKeyStore$JKS.engineLoad(JavaKeyStore.java:38) \tat java.security.KeyStore.load(KeyStore.java:1185)     ... Caused by: java.security.UnrecoverableKeyException: Password verification failed \tat sun.security.provider.JavaKeyStore.engineLoad(JavaKeyStore.java:769) \t... 3 more    If this occurs at the server side, we can see the same message when the SSL listener is being set up.   Incomplete CA Chains   Now lets look at the first of the “peer not authenticated” exceptions. In the logging we see this exception at the client side:    javax.net.ssl.SSLPeerUnverifiedException: peer not authenticated \tat com.sun.net.ssl.internal.ssl.SSLSessionImpl.getPeerCertificates(SSLSessionImpl.java:352) \tat org.apache.http.conn.ssl.AbstractVerifier.verify(AbstractVerifier.java:128) \tat org.apache.http.conn.ssl.SSLSocketFactory.connectSocket(SSLSocketFactory.java:397) \tat org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:148) \tat org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:150)    So enable SSL logging, run again, and we’ll start with analyzing the handshake. We’ll start by looking from the client side. If we look through the logging we find the following CertificateRequest message from the server and the ServerHelloDone.    *** CertificateRequest Cert Types: RSA, DSS Cert Authorities: *** ServerHelloDone    So thus far, everything went ok. The server has already sent its certificate, and since our client doesn’t throw an error on that part, we can assume it is trusted by the client. So something seems to be wrong with the steps that come after this message from the server. If you look closer at this message, you can see that the server doesn’t specify a set of Cert Authorities it trusts. This could be a misconfiguration at the server side, or it could just be that the server expects one of the trusted Root CAs. In any case, the client is free to send any certificate he wants. So the client sends the following certificate:    *** Certificate chain chain [0] = [ [   Version: V1   Subject: CN=Application4, OU=Smartjava, O=Smartjava, L=NL, ST=NB, C=NL   Signature Algorithm: SHA1withDSA, OID = 1.2.840.10040.4.3  ... ] chain [1] = [ [   Version: V3   Subject: EMAILADDRESS=jos.dirksen@gmail.com, CN=CA2, OU=Smartjava, O=Smartjava, L=Waalwijk, ST=NB, C=NL   Signature Algorithm: SHA1withDSA, OID = 1.2.840.10040.4.3   ... ]    According to the specification the client now continues with the key exchange and generates secrets to exchange. Somewhere along the lines we can see the following:    pool-1-thread-1, WRITE: TLSv1 Handshake, length = 32 pool-1-thread-1, READ: TLSv1 Alert, length = 2 pool-1-thread-1, RECV TLSv1 ALERT:  fatal, internal_error pool-1-thread-1, called closeSocket()    This means we’ve received an internal error. So something at the server side went wrong. Looking at the server we see the following in the SSL dump:    *** Certificate chain chain [0] = [ [   Version: V1   Subject: CN=Application4, OU=Smartjava, O=Smartjava, L=NL, ST=NB, C=NL   Signature Algorithm: SHA1withDSA, OID = 1.2.840.10040.4.3   ... ] chain [1] = [ [   Version: V3   Subject: EMAILADDRESS=jos.dirksen@gmail.com, CN=CA2, OU=Smartjava, O=Smartjava, L=Waalwijk, ST=NB, C=NL   Signature Algorithm: SHA1withDSA, OID = 1.2.840.10040.4.3   ... ] *** qtp1735121130-17, handling exception: java.lang.RuntimeException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty qtp1735121130-17, SEND TLSv1 ALERT:  fatal, description = internal_error qtp1735121130-17, WRITE: TLSv1 Alert, length = 2    You can see that we received the certificate from the client, and directly after that we get this error. This error however doesn’t really tell us anything. We do however have enough information to at least limit the possible errors. We know that the server didn’t sent a list of CAs, we can see that the client sent a valid certificate, and that server somehow isn’t able to process it. It looks like a problem with the server truststore. In this case the best approach is to look at the certificates the server trusts. Either in the cacerts file or in it’s own truststore. Validate whether the CA certificate our client sends is in the server’s truststore, and the server actually loads the stores we expect.   It’s of course also possible that the client has an incomplete chain of trust for the certificate received from the server. In that case we once again get the “peer not authenticated” error at the client side. If we look at the SSL debug logging, we see the following exception occuring at the client side:    pool-1-thread-1, handling exception: java.lang.RuntimeException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty pool-1-thread-1, SEND TLSv1 ALERT:  fatal, description = internal_error pool-1-thread-1, WRITE: TLSv1 Alert, length = 2    This exception occured directly after the server has sent its certificate using a “Certificate message”:    *** Certificate chain chain [0] = [ [   Version: V1   Subject: CN=server, C=NL   Signature Algorithm: SHA1withRSA, OID = 1.2.840.113549.1.1.5    Following the same reasoning as for the server we can conclude that there is something wrong with the client side truststore. For completeness sake, the server receives this error message when this situation occurs at the client:    qtp1500389297-17, READ: TLSv1 Alert, length = 2 qtp1500389297-17, RECV TLSv1 ALERT:  fatal, internal_error qtp1500389297-17, called closeSocket() qtp1500389297-17, handling exception: javax.net.ssl.SSLException: Received fatal alert: internal_error qtp1500389297-17, called close() qtp1500389297-17, called closeInternal(true)    Invalid keys   For the next exercise lets look at the following error that occurs during this handshake. In the logging at the client side we see the following error message in the SSL output:    ool-1-thread-1, WRITE: TLSv1 Handshake, length = 32 pool-1-thread-1, READ: TLSv1 Alert, length = 2 pool-1-thread-1, RECV TLSv1 ALERT:  fatal, internal_error pool-1-thread-1, called closeSocket() pool-1-thread-1, handling exception: javax.net.ssl.SSLException: Received fatal alert: internal_error pool-1-thread-1, IOException in getSession():  javax.net.ssl.SSLException: Received fatal alert: internal_error    Which results in the very unhelpful:    javax.net.ssl.SSLPeerUnverifiedException: peer not authenticated \tat com.sun.net.ssl.internal.ssl.SSLSessionImpl.getPeerCertificates(SSLSessionImpl.java:352) \tat org.apache.http.conn.ssl.AbstractVerifier.verify(AbstractVerifier.java:128) \tat org.apache.http.conn.ssl.SSLSocketFactory.connectSocket(SSLSocketFactory.java:397) \tat org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:148) \tat org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:150) \tat org.apache.http.impl.conn.AbstractPooledConnAdapter.open(AbstractPooledConnAdapter.java:121) \tat org.apache.http.impl.client.DefaultRequestDirector.tryConnect(DefaultRequestDirector.java:575) \tat org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:425) \tat org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:820) \tat org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:754) \tat org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:732)    When you receive an internal error, there is usually something wrong at the server side. So looking at the serverside, lets see what caused this error.    *** qtp2044601711-16, handling exception: java.lang.RuntimeException: Unexpected error: java.security.InvalidAlgorithmParameterException: the trustAnchors parameter must be non-empty qtp2044601711-16, SEND TLSv1 ALERT:  fatal, description = internal_error    Hmm.. somewhat more useful. It seems that there is something wrong with the algorithm we used, the client seems to have provided an incorrect certificate. But what is wrong? If you look back at the happy flow, you can send that at a certain time the server asks the client for a certificate using a “Certificate” message. Lets look a bit closer at this message and the response:    *** CertificateRequest Cert Types: RSA, DSS Cert Authorities: &lt;EMAILADDRESS=jos.dirksen@gmail.com, CN=CA2, OU=Smartjava, O=Smartjava, L=Waalwijk, ST=NB, C=NL&gt; *** ServerHelloDone matching alias: application4 *** Certificate chain chain [0] = [ [   Version: V1   Subject: CN=Application4, OU=Smartjava, O=Smartjava, L=NL, ST=NB, C=NL   Signature Algorithm: SHA1withDSA, OID = 1.2.840.10040.4.3    Key:  Sun DSA Public Key   ...    What you can see here is that the server specifies the cert types it accepts, and the authorities it accepts. The client responses in this case however with a DSA public key. Depending on the server implementation this can cause this strange message. Another possible scenario I’ve seen (especially with self-signed certificates) is that with a “CertificateRequest” message like this:    *** CertificateRequest Cert Types: RSA, DSS Cert Authorities: *** ServerHelloDone    This client won’t respond with a certificate at all, if you only have DSA based keys in your keystore. It won’t throw an error on the client side, but will cause a “null certificate chain” message as the server side. I haven’t seen this scenario, though, when you don’t use self-signed certificates.   Certificate expiration   So far we’ve seen how you can analyze the SSL handshake to determine where to look for configuration errors. In this last example we’ll look at what happens when a certificate expires. In this case we once again see the very cryptic message at the client side:    pool-1-thread-1, READ: TLSv1 Alert, length = 2 pool-1-thread-1, RECV TLSv1 ALERT:  fatal, certificate_unknown pool-1-thread-1, called closeSocket() pool-1-thread-1, handling exception: javax.net.ssl.SSLHandshakeException: Received fatal alert: certificate_unknown pool-1-thread-1, IOException in getSession():  javax.net.ssl.SSLHandshakeException: Received fatal alert: certificate_unknown pool-1-thread-1, called close() pool-1-thread-1, called closeInternal(true) pool-1-thread-1, called close() pool-1-thread-1, called closeInternal(true) javax.net.ssl.SSLPeerUnverifiedException: peer not authenticated \tat com.sun.net.ssl.internal.ssl.SSLSessionImpl.getPeerCertificates(SSLSessionImpl.java:352) \tat org.apache.http.conn.ssl.AbstractVerifier.verify(AbstractVerifier.java:128) \tat org.apache.http.conn.ssl.SSLSocketFactory.connectSocket(SSLSocketFactory.java:397) \tat org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:148) \tat org.apache.http.impl.conn.AbstractPoolEntry.open(AbstractPoolEntry.java:150)    If we look at the phase of the SSL handshake we’re in, we can see that we’ve already sent our client certificate and finishing up the handshake when we receive this error. The error on the serverside is actually pretty helpful. After receiving the invalid certificate, in the debug logging, it shows us the following:    *** qtp1735121130-17, SEND TLSv1 ALERT:  fatal, description = certificate_unknown qtp1735121130-17, WRITE: TLSv1 Alert, length = 2 [Raw write]: length = 7 0000: 15 03 01 00 02 02 2E                               ....... qtp1735121130-17, called closeSocket() qtp1735121130-17, handling exception: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path validation failed: java.security.cert.CertPathValidatorException: timestamp check failed qtp1735121130-17, called close() qtp1735121130-17, called closeInternal(true)    It tells us that during the validation of the certificate, a timestamp check failed. This tells us that we should look at the validity of the certificates in our certificate chain to see what is happening.   Summary  In this article you’ve seen a couple of common causes for SSL exceptions and ways to identify the exception. Their can be many causes for these kind of exceptions, the most common though are the following:      Incorrect certificate chains in the client truststore   Incorrect certificate chains in the server truststore   Invalid key algorithm used for private keys   Expired certificate or expired CA certificate   Incorrect passwords used to access the keys   Multiple private keys to choose from   If you’re presented with a such an exception a good general approach is this. You first check the keystores that are involved. Use the java keytool for this:    keytool -list -v -keystore &lt;location_of_keystore&gt;    This will print out all the certificates and keys in the keystore. Check whether the keys are of a supported type, the required CA certificates are stored and that your application is using the correct one (spent hours figuring out an issue because I was looking into a truststore for my private key). If everything seems to be OK at first glance it’s time to enable ssl debugging (-Djavax.net.debug=ssl:handshake) and check the handshake messages that are sent. Wikipedia has a nice overview of which message is sent at a specific time. For more information on the content of the messages look at the RFC 5246 (or the one of the SSL/TLS version you’re using, but the handshake changes are minimal between versions). Using the messages and the handshake, determine at what place in the handshake things go wrong, taking into account that the client will continue with the handshake, while the server is processing it’s certificate.  ","categories": ["posts","java","ssl"],
        "tags": [],
        "url": "http://www.smartjava.org/content/how-analyze-java-ssl-errors/",
        "teaser":null},{
        "title": "Protect a REST service using HMAC (Play 2.0)",
        "excerpt":"We have HTTPS, what more do we need?  When you talk about security for REST based APIs, people often point to HTTPS. With HTTPS you can easily protect your services from prying eyes using methods everybody is familiar with. When you, however, require an additional level of security, or HTTPS just isn’t available, you need an alternative. For instance you might need to track the usage of your API for each customer, or need to know exactly who is makking all these calls. You could use HTTPS together with client authentication, but that would require setting up a complete PKI Infrastructure and a secure way to identify your customers and exchange private keys.  And in contrast with WS-Security for SOAP based service, there isn’t a standard we can use for REST.   A common way to solve this (Microsoft, Amazon, Google and Yahoo take this approach), is by signing your message based on a shared secret between the client and the service. Note that with this approach we only sign the data, we don’t encrypt it. The signature we’re talking about in this case, is something that is usually called a Hash-based Message Authentication Code (or HMAC for short). With an HMAC we create a message authentication code (MAC) for a request based on a secret key we’ve exchanged.   In this article I’ll show you how you can implement this algorithm for a Play 2.0 based REST service. If you use a different technology the steps will be pretty much the same way though.   HMAC Scenario   For the client part, I’ll just use a simple HTTPClient based application. To implement this we’ll have to take the following steps:      First, we need to exchange a shared secret with out client. Often this is sent by the API provider to the client using an email message, or the provider has a website where you can lookup the shared secret. Note that this secret is shared only between you and service, each client will have a different shared secret. This isn't something you share like a public key,    To make sure the client and the service calculate the signature over the same content, we need to normalize the request that is to be signed. If we don't do this, the server might interpret whitespace in a different manner as the client did and conclude that the signature is invalid.    Based on this normalized message, the client creates an HMAC value using the shared secret.   Now the client is ready to sent the request to the service. He adds the HMAC value to the headers, and also something that identifies him as user. For instance a username or some other public value.    When the service receives the request it extracts the username and the HMAC value from the headers.    Based on the username, the service knows which shared secret should have been used to sign the message. The service would, for instance, retrieve this from a datastore somewhere.     The service now normalizes the request in the same manner as the client did, and calculates the HMAC value for itself.     If the HMAC from the client matches the calculated HMAC from the server, you know that the message's integrity is guaranteed, and that the client is who he says he is. If either the wrong username was supplied, or an incorrect secret was used to calculate the headers, the HMAC values wouldn't match.   What do we need to do, to implement HMAC? In the following section we’ll at the following subjects.      Determine the fields to use for input.   Create client code that can calculate this HMAC and add the corresponding headers   Create Play 2.0 based interceptor that checks the HMAC headers   Determine the input fields  The first thing we need to do is determine the input for our HMAC calculation. The following table describes the elements we’ll include:   FieldDescription HTTP MethodWith REST the kind of HTTP method we execute defines the behavior on the server side. A DELETE to a specific URL is handled differently than a GET to that URL.&lt;/tr&gt; Content-MD5This HTTP header is a standard HTTP header. This is an MD5 hash of the body of the request. If we include this header into the HMAC code generation we get an HMAC value that changes as the request body changes. Content-Type headerThe Content-Type header is an important header when making REST calls. Depending on the media-type the server can respond differently to a request, therefore it should be included in the HMAC.&lt;/tr&gt; Date headerWe also include the date the request was created to calculate the HMAC. On the server side we can make sure the date wasn’t changed in transit. Besides this we can add message expiration functionality on the server.&lt;/tr&gt; PathThe Path part of the URL that was invoked is also used in HMAC calculation, since an URI identifies a resource within REST.&lt;/tr&gt;   What we’ll include is pretty much the following information from a request:    PUT /example/resource/1 Content-Md5: uf+Fg2jkrCZgzDcznsdwLg== Content-Type: text/plain; charset=UTF-8 Date: Tue, 26 Apr 2011 19:59:03 CEST    Client code that can be used to create an HMAC signature   Below you can see the client code with which we’ll make the calls to the HMAC protected service. This is just a quick HTTPClient based client with which we can test our Service.   public class HMACClient {  \tprivate final static String DATE_FORMAT = \"EEE, d MMM yyyy HH:mm:ss z\"; \tprivate final static String HMAC_SHA1_ALGORITHM = \"HmacSHA1\"; \t \tprivate final static String SECRET = \"secretsecret\"; \tprivate final static String USERNAME = \"jos\"; \t \tprivate static final Logger LOG = LoggerFactory.getLogger(HMACClient.class);  \tpublic static void main(String[] args) throws HttpException, IOException, NoSuchAlgorithmException { \t\tHMACClient client = new HMACClient(); \t\tclient.makeHTTPCallUsingHMAC(USERNAME); \t}  \tpublic void makeHTTPCallUsingHMAC(String username) throws HttpException, IOException, NoSuchAlgorithmException { \t\tString contentToEncode = \"{\\\"comment\\\" : {\\\"message\\\":\\\"blaat\\\" , \\\"from\\\":\\\"blaat\\\" , \\\"commentFor\\\":123}}\"; \t\tString contentType = \"application/vnd.geo.comment+json\"; \t\t//String contentType = \"text/plain\"; \t\tString currentDate = new SimpleDateFormat(DATE_FORMAT).format(new Date());  \t\tHttpPost post = new HttpPost(\"http://localhost:9000/resources/rest/geo/comment\"); \t\tStringEntity data = new StringEntity(contentToEncode,contentType,\"UTF-8\"); \t\tpost.setEntity(data); \t\t \t\tString verb = post.getMethod(); \t\tString contentMd5 = calculateMD5(contentToEncode); \t\tString toSign = verb + \"\\n\" + contentMd5 + \"\\n\" \t\t\t\t+ data.getContentType().getValue() + \"\\n\" + currentDate + \"\\n\" \t\t\t\t+ post.getURI().getPath(); \t\t \t\tString hmac = calculateHMAC(SECRET, toSign);  \t\tpost.addHeader(\"hmac\", username + \":\" + hmac); \t\tpost.addHeader(\"Date\", currentDate); \t\tpost.addHeader(\"Content-Md5\", contentMd5);  \t\tHttpClient client = new DefaultHttpClient(); \t\tHttpResponse response = client.execute(post); \t\t \t\tSystem.out.println(\"client response:\" + response.getStatusLine().getStatusCode()); \t}  \tprivate String calculateHMAC(String secret, String data) { \t\ttry { \t\t\tSecretKeySpec signingKey = new SecretKeySpec(secret.getBytes(),\tHMAC_SHA1_ALGORITHM); \t\t\tMac mac = Mac.getInstance(HMAC_SHA1_ALGORITHM); \t\t\tmac.init(signingKey); \t\t\tbyte[] rawHmac = mac.doFinal(data.getBytes()); \t\t\tString result = new String(Base64.encodeBase64(rawHmac)); \t\t\treturn result; \t\t} catch (GeneralSecurityException e) { \t\t\tLOG.warn(\"Unexpected error while creating hash: \" + e.getMessage(),\te); \t\t\tthrow new IllegalArgumentException(); \t\t} \t} \t \tprivate String calculateMD5(String contentToEncode) throws NoSuchAlgorithmException { \t\tMessageDigest digest = MessageDigest.getInstance(\"MD5\"); \t\tdigest.update(contentToEncode.getBytes()); \t\tString result = new String(Base64.encodeBase64(digest.digest())); \t\treturn result; \t} }    We won’t dive into too much detail here, since the code isn’t that interesting. The only interesting part is where we create an HMAC value from the fields we discussed earlier. We do this in the following couple of lines:   We first create the string we’re going to sign:  \t\tString verb = post.getMethod(); \t\tString contentMd5 = calculateMD5(contentToEncode); \t\tString toSign = verb + \"\\n\" + contentMd5 + \"\\n\" \t\t\t\t+ data.getContentType().getValue() + \"\\n\" + currentDate + \"\\n\" \t\t\t\t+ post.getURI().getPath();   And then use the HMAC algorithm to create a signature based on a shared secret.   \tprivate String calculateHMAC(String secret, String data) { \t\ttry { \t\t\tSecretKeySpec signingKey = new SecretKeySpec(secret.getBytes(),\tHMAC_SHA1_ALGORITHM); \t\t\tMac mac = Mac.getInstance(HMAC_SHA1_ALGORITHM); \t\t\tmac.init(signingKey); \t\t\tbyte[] rawHmac = mac.doFinal(data.getBytes()); \t\t\tString result = new String(Base64.encodeBase64(rawHmac)); \t\t\treturn result; \t\t} catch (GeneralSecurityException e) { \t\t\tLOG.warn(\"Unexpected error while creating hash: \" + e.getMessage(),\te); \t\t\tthrow new IllegalArgumentException(); \t\t} \t}   After we’ve calculcated the HMAC value, we need to send it to the server. We do this by providing a custom header:   \t\tpost.addHeader(\"hmac\", username + \":\" + hmac);   As you can see, we also add our username. This is needed by the server to determine which secret to use to calculate the HMAC value on the server side. When we now run this code, a simple POST operation will be executed that sends the following request to the server:    POST /resources/rest/geo/comment HTTP/1.1[\\r][\\n] hmac: jos:+9tn0CLfxXFbzPmbYwq/KYuUSUI=[\\r][\\n] Date: Mon, 26 Mar 2012 21:34:33 CEST[\\r][\\n] Content-Md5: r52FDQv6V2GHN4neZBvXLQ==[\\r][\\n] Content-Length: 69[\\r][\\n] Content-Type: application/vnd.geo.comment+json; charset=UTF-8[\\r][\\n] Host: localhost:9000[\\r][\\n] Connection: Keep-Alive[\\r][\\n] User-Agent: Apache-HttpClient/4.1.3 (java 1.5)[\\r][\\n] [\\r][\\n] {\"comment\" : {\"message\":\"blaat\" , \"from\":\"blaat\" , \"commentFor\":123}}    Implementing in Scala / Play   So far we’ve seen what the client needs to do to provide us with the correct headers. Service providers often offer specific libraries, in multiple languages, that handle the details of signing the message. But as you can see, doing it by hand, isn’t that difficult. Now, let’s look at the server side, where we use scala together with the Play 2.0 framework to check whether the supplied header contains the correct information. For more information on setting up the correct scala environment to test this code look at my previous post on scala (http://www.smartjava.org/content/play-20-akka-rest-json-and-dependencies).  The first thing to do is setup the correct routes to support this POST operation. We do this in the conf/routes file    POST\t/resources/rest/geo/comment\t\t\tcontrollers.Application.addComment    This is basic Play functionality. All POST calls to the /resource/rest/geo/comment URL will be passed on to the specified controller. Let’s look at what this operation looks like:      def addComment() = Authenticated {     (user, request) =&gt; {     \t// convert the supplied json to a comment object     \tval comment = Json.parse(request.body.asInstanceOf[String]).as[Comment]     \t     \t// pass the comment object to a service for processing     \tcommentService.storeComment(comment)     \tprintln(Json.toJson(comment))         Status(201)       }   }    Now it gets a bit more complicated. As you can see in the listing above, we’ve defined an addComment operation. But, instead of directly defining an Action like this:      def processGetAllRequest() = Action {     val result = service.processGetAllRequest;     Ok(result).as(\"application/json\");   }    We, instead, define it like this:      def addComment() = Authenticated {     (user, request) =&gt; {    What we do here is create a composite action (http://www.playframework.org/documentation/2.0/ScalaActionsComposition). We can easily do this, since Scala is a functional language. The ‘Authenticated’ reference you see here is just a simple reference to a simple function, that takes another function as its argument. In the ‘Authenticated’ function we’ll check the HMAC signature. You can read this as using annotations, but now without the need for any special constructs. So, what does our HMAC check look like.    import play.api.mvc.Action import play.api.Logger import play.api.mvc.RequestHeader import play.api.mvc.Request import play.api.mvc.AnyContent import play.api.mvc.Result import controllers.Application._ import java.security.MessageDigest import javax.crypto.spec.SecretKeySpec import javax.crypto.Mac import org.apache.commons.codec.binary.Base64 import play.api.mvc.RawBuffer import play.api.mvc.Codec  /**  * Obejct contains security actions that can be applied to a specific action called from  * a controller.  */ object SecurityActions {    val HMAC_HEADER = \"hmac\"   val CONTENT_TYPE_HEADER = \"content-type\"   val DATE_HEADER = \"Date\"        val MD5 = \"MD5\"   val HMACSHA1 = \"HmacSHA1\"    /**    * Function authenticated is defined as a function that takes as parameter    * a function. This function takes as argumens a user and a request. The authenticated    * function itself, returns a result.    *    * This Authenticated function will extract information from the request and calculate    * an HMAC value.    *    *    */   def Authenticated(f: (User, Request[Any]) =&gt; Result) = {     // we parse this as tolerant text, since our content type     // is application/vnd.geo.comment+json, which isn't picked     // up by the default body parsers. Alternative would be     // to parse the RawBuffer manually     Action(parse.tolerantText) {              request =&gt;         {           // get the header we're working with           val sendHmac = request.headers.get(HMAC_HEADER);            // Check whether we've recevied an hmac header           sendHmac match {              // if we've got a value that looks like our header              case Some(x) if x.contains(\":\") &amp;&amp; x.split(\":\").length == 2 =&gt; {                // first part is username, second part is hash               val headerParts = x.split(\":\");               val userInfo = User.find(headerParts(0))                // Retrieve all the headers we're going to use, we parse the complete                // content-type header, since our client also does this               val input = List(                 request.method,                 calculateMD5(request.body),                 request.headers.get(CONTENT_TYPE_HEADER),                 request.headers.get(DATE_HEADER),                 request.path)                // create the string that we'll have to sign               val toSign = input.map(                 a =&gt; {                   a match {                     case None =&gt; \"\"                     case a: Option[Any] =&gt; a.asInstanceOf[Option[Any]].get                     case _ =&gt; a                   }                 }).mkString(\"\\n\")                // use the input to calculate the hmac               val calculatedHMAC = calculateHMAC(userInfo.secret, toSign)                // if the supplied value and the received values are equal               // return the response from the delegate action, else return               // unauthorized               if (calculatedHMAC == headerParts(1)) {                 f(userinfo, request)               } else {                  Unauthorized               }             }              // All the other possibilities return to 401              case _ =&gt; Unauthorized           }         }     }   }    /**    * Calculate the MD5 hash for the specified content    */   private def calculateMD5(content: String): String = {     val digest = MessageDigest.getInstance(MD5)     digest.update(content.getBytes())     new String(Base64.encodeBase64(digest.digest()))   }    /**    * Calculate the HMAC for the specified data and the supplied secret    */   private def calculateHMAC(secret: String, toEncode: String): String = {     val signingKey = new SecretKeySpec(secret.getBytes(), HMACSHA1)     val mac = Mac.getInstance(HMACSHA1)     mac.init(signingKey)     val rawHmac = mac.doFinal(toEncode.getBytes())     new String(Base64.encodeBase64(rawHmac))   } }    That’s a lot of code, but most of it will be pretty easy to understand. The ‘calculateHMAC’ and the ‘calculateMD5’ methods are just basic scala wrappers around Java functionality. The documentation inside this class should be enough to understand what is happening. I do, however, want to highlight a couple of interesting concepts in this code. The first thing is the method signatue:     def Authenticated(f: (User, Request[Any]) =&gt; Result) = {    What this means is that the Authenticated method itself, takes as it’s arguments another method (or function if you want to call it that). If you look back at the target of our route, you can see that we do just that:    def addComment() = Authenticated {     (user, request) =&gt; ...    Now what happens when this ‘Authenticated’ method is called? The first thing we do, is check whether the HMAC header exists and is in the correct format:      val sendHmac = request.headers.get(HMAC_HEADER);   sendHmac match {              // if we've got a value that looks like our header              case Some(x) if x.contains(\":\") &amp;&amp; x.split(\":\").length == 2 =&gt; {             ...             }                 // All the other possibilities return to 401              case _ =&gt; Unauthorized    We do this by using a match against the HMAC header. If it contains a value that is of the correct format, we process the header and calculate the HMAC value in the same manner as our client did. If not we return a 401. If the HMAC value is correct we delegate to the provided function using this code:                  if (calculatedHMAC == headerParts(1)) {                 f(userInfo, request)               } else {                  Unauthorized               }    And that pretty much is it. With this code you can easily use an HMAC to check whether the message has changed in transit, and whether your client is really known to you. Very easy as you can see.   Just a small sidenote on JSON usage from Play 2.0. If you look at the action code, you can see I use the standard JSON functionality:      def addComment() = Authenticated {     (user, request) =&gt; {     \t// convert the supplied json to a comment object     \tval comment = Json.parse(request.body.asInstanceOf[String]).as[Comment]     \t     \t// pass the comment object to a service for processing     \tcommentService.storeComment(comment)     \tprintln(Json.toJson(comment))         Status(201)       }   }    First we parse the received JSON using ‘json.parse’ to a ‘comment’ class, then store the comment, and convert the command object back to a string value. Not the most useful code, but it does nicely demonstrate some of the JSON functionality provided by Play 2.0. To convert from JSON to an object and back again, something called “Implicit Conversions” is used. I won’t dive too much in the details, but a good explanation can be found here: http://www.codecommit.com/blog/ruby/implicit-conversions-more-powerful-than-dynamic-typing. What happens here is that the JSON.parse and the Json.toJson method look for a specific method on the Comment class. And if it can’t find it there, it looks for the specific operation in its scope. To see how this works for the JSON parsing let’s look a the Comment class and its companion object:     import play.api.libs.json.Format import play.api.libs.json.JsValue import play.api.libs.json.JsObject import play.api.libs.json.JsString import play.api.libs.json.JsNumber import play.api.libs.json.JsArray  object Comment {    implicit object CommentFormat extends Format[Comment] {      def reads(json: JsValue): Comment = {       val root = (json \\ \"comment\")        Comment(         (root \\ \"message\").as[String],         (root \\ \"from\").as[String],         (root \\ \"commentFor\").as[Long])      }      def writes(comment: Comment): JsValue = {       JsObject(List(\"comment\" -&gt;         JsObject(Seq(           \"message\" -&gt; JsString(comment.message),           \"from\" -&gt; JsString(comment.message),           \"commentFor\" -&gt; JsNumber(comment.commentFor)))))     }   }  }  case class Comment(message: String, from: String, commentFor: Long) {}    What you see here is that in the companion object we create a new ‘Format’ object. The ‘reads’ and ‘writes’ operations in this object will now be used by the JSON operation to convert from and to JSON when working with the ‘Comment’ class. Very powerful stuff, even though it’s a bit magic ;-)   For more information on the Scala/Play environment I used for this example see my previous posts:   http://www.smartjava.org/content/play-20-akka-rest-json-and-dependencies http://www.smartjava.org/content/using-querulous-scala-postgresql   I hoped you liked the article, any question feel free to contact met via mail or twitter.  ","categories": ["posts","rest","security","hmac","json","play","scala"],
        "tags": [],
        "url": "http://www.smartjava.org/content/protect-rest-service-using-hmac-play-20/",
        "teaser":null},{
        "title": "How to use SPDY with Jetty",
        "excerpt":"SPDY is a new protocol proposed by Google as a new protocol for the web. SPDY is compatible with HTTP but tries to reduce web page loading by using compression, mulitplexing and prioritization.To be more precise, the goals for speedy are: (http://dev.chromium.org/spdy/spdy-whitepaper)    The SPDY project defines and implements an application-layer protocol for the web which greatly reduces latency.   The high-level goals for SPDY are:          To target a 50% reduction in page load time. Our preliminary results have come close to this target (see below).       To minimize deployment complexity. SPDY uses TCP as the underlying transport layer, so requires no changes to existing networking infrastructure.         To avoid the need for any changes to content by website authors. The only changes required to support SPDY are in the client user agent and web server applications.       To bring together like-minded parties interested in exploring protocols as a way of solving the latency problem. We hope to develop this new protocol in partnership with the open-source community and industry specialists    Some specific technical goals are:         To allow many concurrent HTTP requests to run across a single TCP session.       To reduce the bandwidth currently used by HTTP by compressing headers and eliminating unnecessary headers.       To define a protocol that is easy to implement and server-efficient. We hope to reduce the complexity of HTTP by cutting down on edge cases and defining easily parsed message formats.       To make SSL the underlying transport protocol, for better security and compatibility with existing network infrastructure. Although SSL does introduce a latency penalty, we believe that the long-term future of the web depends on a secure network connection. In addition, the use of SSL is necessary to ensure that communication across existing proxies is not broken.         To enable the server to initiate communications with the client and push data to the client whenever possible.  &lt;/em&gt;   Setup maven  In this article we won’t look too much into the technical implementation of this protocol, but we’ll show you how you can start using and experimenting with SPDY yourself. For this we’ll use Jetty that has a SPDY implementation available in it’s latest release (http://wiki.eclipse.org/Jetty/Feature/SPDY).   So let’s get started. For this example we’ll let Maven handle the dependencies. And we’ll use the following POM.    &lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \txsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; \t&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; \t&lt;groupId&gt;smartjava.jetty.spdy&lt;/groupId&gt; \t&lt;artifactId&gt;SPDY-Example&lt;/artifactId&gt; \t&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; \t&lt;dependencies&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.eclipse.jetty.aggregate&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;jetty-all-server&lt;/artifactId&gt; \t\t\t&lt;version&gt;8.1.2.v20120308&lt;/version&gt; \t\t\t&lt;type&gt;jar&lt;/type&gt; \t\t\t&lt;scope&gt;compile&lt;/scope&gt; \t\t\t&lt;exclusions&gt; \t\t\t\t&lt;exclusion&gt; \t\t\t\t\t&lt;artifactId&gt;mail&lt;/artifactId&gt; \t\t\t\t\t&lt;groupId&gt;javax.mail&lt;/groupId&gt; \t\t\t\t&lt;/exclusion&gt; \t\t\t&lt;/exclusions&gt; \t\t&lt;/dependency&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.eclipse.jetty.spdy&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;spdy-jetty&lt;/artifactId&gt; \t\t\t&lt;version&gt;8.1.2.v20120308&lt;/version&gt; \t\t&lt;/dependency&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.eclipse.jetty.spdy&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;spdy-core&lt;/artifactId&gt; \t\t\t&lt;version&gt;8.1.2.v20120308&lt;/version&gt; \t\t&lt;/dependency&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.eclipse.jetty.spdy&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;spdy-jetty-http&lt;/artifactId&gt; \t\t\t&lt;version&gt;8.1.2.v20120308&lt;/version&gt; \t\t&lt;/dependency&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.eclipse.jetty.npn&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;npn-api&lt;/artifactId&gt; \t\t\t&lt;version&gt;8.1.2.v20120308&lt;/version&gt;                         &lt;scope&gt;provided&lt;/scope&gt; \t\t&lt;/dependency&gt; \t&lt;/dependencies&gt; &lt;/project&gt;    NTP TLS extension   With this POM the correct libraries are loaded, so we can start using the specific SPDY classes in Jetty. Before we can really use SPDY though, we also need to configure Java to use an extension to the TLS protocol: TLS Next Protocol Negotiation or NPN for short. The details of this extension can be found on a googles technote (http://technotes.googlecode.com/git/nextprotoneg.html), but in short it comes down to this problem. What if we want to use a different protocol than HTTP when we’re making a connection to a server via TLS. We don’t know whether the server supports this protocol, and, since SPDY is focussed on speed, we don’t want the added latency of making a round trip. Even though there are a couple of different solutions , most suffer from unpredictability, extra roundtrips or breaks existing proxies (see (http://www.ietf.org/proceedings/80/slides/tls-1.pdf for more info).   The proposed solution by Google is use the TLS’s extension mechanism to determine the protocol to be used. This is called “Next Protocol Negotiation” or NPN for short. With this extension the following steps are taken during the TLS handshake:      Client shows support for this extension   Server responds with this support and includes a list of supported protocols   Client sends the protocol he wants to use, which doesn't have to be one offerede by the server.   This results in the following TLS handshake:    Client                                               Server  ClientHello (NP extension)   --------&gt;                                                 ServerHello (NP extension &amp; list of protocols)                                                Certificate*                                          ServerKeyExchange*                                         CertificateRequest*                              &lt;--------      ServerHelloDone Certificate* ClientKeyExchange CertificateVerify* [ChangeCipherSpec] NextProtocol Finished                     --------&gt;                                          [ChangeCipherSpec]                              &lt;--------             Finished Application Data             &lt;-------&gt;     Application Data    For more information on TLS/SSL handshakes look at my previous article on how to analyze Java SSL errors: http://www.smartjava.org/content/how-analyze-java-ssl-errors.   So we need NPN to quickly determine the protocol we want to use. Since this isn’t standard TLS we need to configure Java to use NPN. Standard Java doesn’t (yet) support NPN so we can’t run SPDY on the standard JVM. To solve this Jetty has created a NPN implementation that can be used together with OpenJDK 7 (see http://wiki.eclipse.org/Jetty/Feature/NPN for more details). You can download this implementation from here: http://repo2.maven.org/maven2/org/mortbay/jetty/npn/npn-boot/ and you have to add it to your boot classpath as such:    java -Xbootclasspath/p:&lt;path_to_npn_boot_jar&gt; ...    Wrap HTTP request in SPDY   Now you can start using SPDY from Jetty. Jetty has support for this feature in two different ways. You can use it to transparently convert from SPDY to HTTP and back again or you can use it to directly talk SPDY. Let’s create a simple server configuration that hosts some static content using a SPDY enabled connection. For this we’ll use the following Jetty configuration:  import org.eclipse.jetty.server.Connector; import org.eclipse.jetty.server.Server; import org.eclipse.jetty.server.handler.ContextHandler; import org.eclipse.jetty.server.handler.ResourceHandler; import org.eclipse.jetty.spdy.http.HTTPSPDYServerConnector; import org.eclipse.jetty.util.ssl.SslContextFactory;   public class SPDYServerLauncher {  \tpublic static void main(String[] args) throws Exception { \t\t \t\t// the server to start \t\tServer server = new Server(); \t\t \t\t// the ssl context to use \t\tSslContextFactory sslFactory = new SslContextFactory(); \t\tsslFactory.setKeyStorePath(\"src/main/resources/spdy.keystore\"); \t\tsslFactory.setKeyStorePassword(\"secret\"); \t\tsslFactory.setProtocol(\"TLSv1\"); \t\t \t\t// simple connector to add to serve content using spdy \t\tConnector connector = new HTTPSPDYServerConnector(sslFactory); \t\tconnector.setPort(8443); \t\t \t\t// add connector to the server \t\tserver.addConnector(connector); \t\t \t\t// add a handler to serve content \t\tContextHandler handler = new ContextHandler(); \t\thandler.setContextPath(\"/content\"); \t\thandler.setResourceBase(\"src/main/resources/webcontent\"); \t\thandler.setHandler(new ResourceHandler()); \t\t \t\tserver.setHandler(handler); \t\t \t\tserver.start(); \t\tserver.join(); \t} }   Since Jetty also has a very flexible XML configuration language, you can do the same thing using the following XML configuration.    &lt;Configure id=\"Server\" class=\"org.eclipse.jetty.server.Server\"&gt;       &lt;New id=\"sslContextFactory\" class=\"org.eclipse.jetty.util.ssl.SslContextFactory\"&gt;         &lt;Set name=\"keyStorePath\"&gt;src/main/resources/spdy.keystore&lt;/Set&gt;         &lt;Set name=\"keyStorePassword\"&gt;secret&lt;/Set&gt;         &lt;Set name=\"protocol\"&gt;TLSv1&lt;/Set&gt;     &lt;/New&gt;       &lt;Call name=\"addConnector\"&gt;         &lt;Arg&gt;             &lt;New class=\"org.eclipse.jetty.spdy.http.HTTPSPDYServerConnector\"&gt;                 &lt;Arg&gt;                     &lt;Ref id=\"sslContextFactory\" /&gt;                 &lt;/Arg&gt;                 &lt;Set name=\"Port\"&gt;8443&lt;/Set&gt;             &lt;/New&gt;         &lt;/Arg&gt;     &lt;/Call&gt;     // Use standard XML configuration for the other handlers and other   // stuff you want to add   &lt;/Configure&gt;    As you can see in thia listing we specify a SSL context. This is needed since SPDY works over TLS. When we run this configuration Jetty will start listening on port 8443 for SPDY connections. Not all browser yet support SPDY, I’ve tested this example using the latest chrome browser. If you browse to https://localhost:8443/dummy.html (a file I created to test with) you’ll see the content of this file, just like you requested it using HTTPS. So what is happening here? Let’s first look at the SPDY session view that Chrome provides to determine whether we’re really using SPDY. If you navigate to the following url:  chrome://net-internals/#events&amp;q=type:SPDY_SESSION%20is:active. You’ll see something like the following figure.      In this view you can see all the current SPDY sessions. If everything was configured correctly you can also see a SPDY session connected to localhost. An additional check to see if everything is working as intended it to enable debugging of the NPN extension. You can do this by adding the following line to the Java code you use to start up the server:   NextProtoNego.debug = true;   Use the SPDY protocol directly   Now that we’ve got the HTTP over SPDY working, let’s look at the other option Jetty provides that allows us to directly send and recieve SPDY messages. For this example we’ll just create a client that sends a message to the server every 5 seconds. The server sends responses to a connected client with the number of received messages every  second. First we create the server code.   import java.util.concurrent.Executors; import java.util.concurrent.ScheduledExecutorService; import java.util.concurrent.TimeUnit;  import org.eclipse.jetty.server.Server; import org.eclipse.jetty.spdy.SPDYServerConnector; import org.eclipse.jetty.spdy.api.DataInfo; import org.eclipse.jetty.spdy.api.ReplyInfo; import org.eclipse.jetty.spdy.api.Stream; import org.eclipse.jetty.spdy.api.StreamFrameListener; import org.eclipse.jetty.spdy.api.StringDataInfo; import org.eclipse.jetty.spdy.api.SynInfo; import org.eclipse.jetty.spdy.api.server.ServerSessionFrameListener;  public class SPDYListener {  \tpublic static void main(String[] args) throws Exception { \t\t \t\t// Frame listener that handles the communication over speedy\t\t \t\tServerSessionFrameListener frameListener = new ServerSessionFrameListener.Adapter() { \t\t\t\t \t\t\t/** \t\t\t * As soon as we receive a syninfo we return the handler for the stream on  \t\t\t * this session \t\t\t */ \t\t\t@Override \t\t\tpublic StreamFrameListener onSyn(final Stream stream, SynInfo synInfo) { \t\t\t\t \t\t\t\t// Send a reply to this message \t\t\t\tstream.reply(new ReplyInfo(false)); \t\t\t\t \t\t\t\t// and start a timer that sends a request to this stream every 5 seconds \t\t\t\tScheduledExecutorService executor = Executors.newSingleThreadScheduledExecutor(); \t\t\t\tRunnable periodicTask = new Runnable() { \t\t\t\t\t\tprivate int i = 0; \t\t\t\t\t    public void run() { \t\t\t\t\t    \t// send a request and don't close the stream \t\t\t\t\t        stream.data(new StringDataInfo(\"Data from the server \" + i++, false)); \t\t\t\t\t    } \t\t\t\t\t}; \t\t\t\texecutor.scheduleAtFixedRate(periodicTask, 0, 1, TimeUnit.SECONDS); \t\t\t\t \t\t\t\t// Next create an adapter to further handle the client input from specific stream. \t\t\t\treturn new StreamFrameListener.Adapter() { \t\t\t\t\t \t\t\t\t\t/** \t\t\t\t\t * We're only interested in the data, not the headers in this \t\t\t\t\t * example \t\t\t\t\t */ \t\t\t\t\tpublic void onData(Stream stream, DataInfo dataInfo) { \t\t\t\t\t\tString clientData = dataInfo.asString(\"UTF-8\", true); \t\t\t\t\t\tSystem.out.println(\"Received the following client data: \" + clientData); \t\t\t\t\t} \t\t\t\t}; \t\t\t} \t\t};  \t\t// Wire up and start the connector \t\torg.eclipse.jetty.server.Server server = new Server(); \t\tSPDYServerConnector connector = new SPDYServerConnector(frameListener); \t\tconnector.setPort(8181); \t\t \t\tserver.addConnector(connector); \t\tserver.start(); \t\tserver.join(); \t} }   And the client code looks like this:   import java.net.InetSocketAddress; import java.util.concurrent.Executors; import java.util.concurrent.ScheduledExecutorService; import java.util.concurrent.TimeUnit;  import org.eclipse.jetty.spdy.SPDYClient; import org.eclipse.jetty.spdy.api.DataInfo; import org.eclipse.jetty.spdy.api.SPDY; import org.eclipse.jetty.spdy.api.Session; import org.eclipse.jetty.spdy.api.Stream; import org.eclipse.jetty.spdy.api.StreamFrameListener; import org.eclipse.jetty.spdy.api.StringDataInfo; import org.eclipse.jetty.spdy.api.SynInfo;  /**  * Calls the server every couple of seconds.  *   * @author jos  */ public class SPDYCaller {  \tpublic static void main(String[] args) throws Exception { \t\t \t\t// this listener receives data from the server. It then prints out the data \t\tStreamFrameListener streamListener = new StreamFrameListener.Adapter() { \t\t     \t\t    public void onData(Stream stream, DataInfo dataInfo)  { \t\t        // Data received from server \t\t        String content = dataInfo.asString(\"UTF-8\", true); \t\t        System.out.println(\"SPDY content: \" + content); \t\t    } \t\t}; \t\t \t\t// Create client \t\tSPDYClient.Factory clientFactory = new SPDYClient.Factory(); \t\tclientFactory.start(); \t\tSPDYClient client = clientFactory.newSPDYClient(SPDY.V2); \t\t  \t\t// Create a session to the server running on localhost port 8181 \t\tSession session = client.connect(new InetSocketAddress(\"localhost\", 8181), null).get(5, TimeUnit.SECONDS); \t\t  \t\t// Start a new session, and configure the stream listener \t\tfinal Stream stream = session.syn(new SynInfo(false), streamListener).get(5, TimeUnit.SECONDS); \t\t \t\t//start a timer that sends a request to this stream every second \t\tScheduledExecutorService executor = Executors.newSingleThreadScheduledExecutor(); \t\tRunnable periodicTask = new Runnable() { \t\t\t\tprivate int i = 0; \t\t\t \t\t\t    public void run() { \t\t\t    \t// send a request, don't close the stream \t\t\t    \tstream.data(new StringDataInfo(\"Data from the client \" + i++, false)); \t\t\t    } \t\t\t}; \t\texecutor.scheduleAtFixedRate(periodicTask, 0, 1, TimeUnit.SECONDS); \t} }   This shows the following output on the client, and on the server:    client: .. SPDY content: Data from the server 3 SPDY content: Data from the server 4 SPDY content: Data from the server 5 SPDY content: Data from the server 6 ..  server: ... Received the following client data: Data from the client 2 Received the following client data: Data from the client 3 Received the following client data: Data from the client 4 Received the following client data: Data from the client 5 ...    The code itself should be easy to understand from the inline comments. The only thing to remember, when you want to sent more then one data message over a stream is to make sure the second parameter of the constructor to StringDataInfo is set to false. If set to true, the stream will be closed after the data has been sent.     \tstream.data(new StringDataInfo(\"Data from the client \" + i++, false));   This just shows a simple use case of how you can use the SPDY protocol directly. More information and examples can be found at the Jetty wiki and the SPDY API documentation.  ","categories": ["posts","spdy","ssl","jetty"],
        "tags": [],
        "url": "http://www.smartjava.org/content/how-use-spdy-jetty/",
        "teaser":null},{
        "title": "Election site part 1: Basics with Knockout.js, Bootstrap and d3.js",
        "excerpt":"This article shows how you can use knockout.js, Twitter’s Bootstrap and d3.js together to create a javascript application that creates visually pleasing graphics and diagrams to your users. This is complete client side solution that only communicates with REST to the server. This is part 1 of a series of articles discussing these concepts. In this first article we’ll look at setting up the basic application and we’ll demonstrate how to use knockout.js, bootstrap and d3.js. It’s a bit too much to show all the javascript in this article. You can find the demo for this version (with a couple more features) at this url: http://www.smartjava.org/examples/election/part1.html. So if you want to see the some of the util functions I use look at the javascript used in that example.  The case  In the Netherlands, where I live, we have a tendency of our chosen leaders not serving their terms. More often then not, through a breach of trust, or some other big (or sometimes minor) issue the cabinet collapses and we have new elections. During election time we’re spammed with all kinds of predictions, charts and people telling us how the Dutch people are going to vote. And when we finally vote we can see real-time updates throughout the evening, and on many websites showing us the cast votes for each municipality. &lt;/p&gt;  Creating an election map   These updates, and the way they are presented is kind of interesting. Usually a map is shown with all the municipalities where colors depict how they voted, the presenter then zooms in on a specific municipality and shows some more detailled results. And this isn't something they do just in the Netherlands:   US elections    Which tools will we use?   Now hard would it be to build an application that allows you to view the results per district, using just standard web technologies. No Flash, no plugins, just basic HTML and javascript. In this first article of a sample election site, we'll focus on getting the basic components together and working. To be more specific we'll create a site where we use the following tools:      Knockout.js as our javascript application framework.   Twitter's Bootstrap as basis for the application and layout.   D3.js to make working with SVG and maps easier.  Besides these tools we’ll be connecting to a Play 2.0 Rest service that provides us with the information that we’ll show. All in all we’ll be creating a simple application that looks like this:   Show complete map, with colors representing a specific value     Show the details of a specific municipality      Prerequisites: JSON Data  In this example we’ll use a set of geo json elements as the data for our application. In a previous article  I already showed the basics of how to convert data to this format. For more information on how to create a REST service to expose this information see my other articles on this subject. For this example we’ll use a set of GeoJSON records. These records contain information about the geography of a specific city and it contains some statistics on the residents (such as age distribution, percentage foreigners, widowers etc.). For instance the record for Amsterdam looks like this:    { \"geometry\" : { \"coordinates\" : [ [ [ [ 4.750558564252043,                 52.429385009319844               ],               [ 4.983615108367763,                 52.36497825292634               ],              ...               [ 4.9846764177139704,                 52.36565814884794               ]             ] ]         ],       \"type\" : \"MultiPolygon\"     },   \"gm_code\" : \"GM0363\",   \"gm_naam\" : \"Amsterdam\",   \"properties\" : { \"aant_inw\" : \"779810\",       \"aant_man\" : \"383800\",       \"aant_vrouw\" : \"396005\",       \"aantal_hh\" : \"431060\",       \"bev_dichth\" : \"4704\",       \"gem_hh_gr\" : \"1.8\",       \"gm_code\" : \"GM0363\",       \"gm_naam\" : \"Amsterdam\",       \"opp_land\" : \"16576\",       \"opp_tot\" : \"21932\",       \"opp_water\" : \"5356\",       \"p_00_14_jr\" : \"16\",       \"p_15_24_jr\" : \"13\",       \"p_25_44_jr\" : \"35\",       \"p_45_64_jr\" : \"25\",       \"p_65_eo_jr\" : \"11\",       \"p_ant_aru\" : \"2\",       \"p_eenp_hh\" : \"56\",       \"p_gehuwd\" : \"26\",       \"p_gescheid\" : \"9\",       \"p_hh_m_k\" : \"24\",       \"p_hh_z_k\" : \"19\",       \"p_marokko\" : \"9\",       \"p_n_w_al\" : \"35\",       \"p_ongehuwd\" : \"61\",       \"p_over_nw\" : \"10\",       \"p_surinam\" : \"9\",       \"p_turkije\" : \"5\",       \"p_verweduw\" : \"4\",       \"p_west_al\" : \"15\"     },   \"type\" : \"Feature\" }    Knockout.js: Setup the model  The first thing we’ll look at is the model we’ll use in Knockout.js. With Knockout.js you can bind any javascript object to a specific HTML/DOM element. So let’s look at the model we’ll use for this.   var country = function(cities, feature) {     this.cities = cities;     this.geojson = feature; }  // city object containing the name, code and properties // of a specific city. Also contains the geojson for // this specific city. var city = function(name, code, properties, feature)  {     this.name = name;     this.code = code;     this.properties = properties;     this.geojson = feature;      this.getPropertyValue = function(propertyName) {         for (var i = 0 ; i &lt; properties.length ; i++) {             if (properties[i].key == propertyName) return properties[i].value;         }     } };   Very simple javascript objects. Can be optimized for performance, but that’s not something for now.  The final model object we need is the viemodel. This model is the central access point for knockout.js bindings and access from other javascript libraries and scripts.   var viewModelContainer = function() {     self = this;      // contains all the cities     this.country = ko.observable();      // the metrics that are available     this.metrics = ko.observableArray([]);      // the selected metric     this.selectedMetric = ko.observable();      // the selected city when we start is an empty variable     this.selectedCity = ko.observable();      // computed to change when either the metric     // or the map changes     this.metricChange = ko.computed(function() {         this.selectedMetric();         this.country();     }, this);      // function to retrieve a single city     this.getCity = function(cityCode) {         for(var i = 0 ; i &lt; this.country().cities().length ; i++ ) {             var city = this.country().cities()[i];             if (city.code == cityCode) return city;         }     } }   Here we see the first signs of knockout.js. We define a number of ko.observable() objects. What this means is that knockout.js will inform the html/dom objects that are bound to such a  ko.observable() whenever the value of the observable changes. When we look at the html part of this tutorial we’ll see how to use them. First though, we need data to fill the model. For this we use JQuery’s REST support:    Load JSON data using a REST call  We load the geo data and the various metrics using the following piece of code.  function loadDataCBS() {     // get all the fields     $.getJSON(appConstants.JSON_URL_CBS, function(citiesList) {         // get the cities to proces         var allCities = citiesList.features;          // temp value, before we push it to the observable array         var cities = [];          // keep track whether we analyzed the supplied metrics         var propertiesAnalyzed = false;         var metrics = [];          // walk through the array and each city to the observable array         for (i = 0 ; i &lt; allCities.length ; i++) {              // keys are a set of keypairs that are added to the object             var keys = [];             analyzeProperties(allCities[i].properties, keys, metrics, propertiesAnalyzed);              // add all the found cities to the viewmodel at once. Add lazy loading in a             // later stage             cities.push(new city(                 allCities[i].gm_naam,                 allCities[i].gm_code,                 keys,                 allCities[i]             ));              // after the first iteration, metrics have been analyzed             propertiesAnalyzed = true;         }          // create a country and setthe properties         var country = new Country(cities,citiesList.features);          // set the values in the viewmodel.         viewModel.country(country);         viewModel.metrics(metrics);     }) }  function analyzeProperties(properties, keys, metrics, propertiesAnalyzed) {     for(var propertyName in properties) {         var prop = new property(propertyName,properties[propertyName]);         keys.push(prop);          if (!propertiesAnalyzed) {             // simple analyze of the properties. If starts with p_ assume it is a percentage             // which we can plot on the country overview             if (prop.key.substring(0,2) == \"p_\") {                 metrics.push(prop.key);             } else {                 // if doesn't start with a p_ we can check whether the value is                 // a number                 if (!isNaN(prop.value)==true) {                     // we have number, so add this metric                     metrics.push(prop.key);                 }             }         }     } }   The comments inline should explain what happens. Basically what we do is we make a call using   $.getJSON(appConstants.JSON_URL_CBS, function(citiesList) {  and then we analyze the result and fill the viewModel, so that we have information to show in the browser.   Connect all the things  We need to connect everything together so knockout.js knows what to bind. For this we created a simple ‘application’ that creates our model, loads the json data and initializes knockout.js   function init() {     // load json data     loadDataCBS();      // setup the projections, used to render geojson     geo.setupGeo();      // finally bind everything to the model     ko.applyBindings(viewModel); }  // create the viewmodel, this model can be referenced // from all the different files var viewModel = new viewModelContainer();  // and start the application init();   In our html we include this javascript at the end, so once our html is loaded, the application is initialized.   Bind HTML elements to the model  In this first part we’re not going to create too complex bindings. Basically we need to bind the following elements:      Dropdown boxes with selection   Table showing the properties of the selected city   SVG Map to show the details of the selected metric   SVG Map to show the contours of the selected city   Bind dropdown boxes  We want to bind the information from the model (all the available cities and all the available metrics) to two drop down selections.   We do this directly through a knockout.js binding:                    &lt;div data-bind=\"with: country\"&gt;                     &lt;select data-bind=\"options: cities,                        optionsText: 'name',                        value: $parent.selectedCity,                        optionsCaption: 'Choose city'\"&gt;&lt;/select&gt;                 &lt;/div&gt;                  &lt;!--                     Bind to list of available metrics. Only do this, when                     no city has been selected.                 --&gt;                 &lt;div data-bind=\"ifnot: selectedCity\"&gt;                     &lt;select data-bind=\"options: metrics,                        value: selectedMetric,                        optionsCaption: 'Choose metric'\"&gt;&lt;/select&gt;                 &lt;/div&gt;    In this example you can see a couple of interesting knockout.js features. Lets start by looking at the second binding. Here we use a data-bind=”ifnot: selectedCity” to determine whether to render this specific div. If no city is selected this div is rendered and the select element binding is processed. In this binding we bind the “options” of the select element to the metrics property of our viemodel, and once a value is selected it is stored in the “selectedMetric” property of the viewmodel. For the first dropdown we do something similar, but we’ve also wrapped the div inside another div with the data-bind=”with: country” binding. This means that all the bindings inside this div are processed in the context of the “country” element. The binding of the dropdown points to “cities”, since we are in the context of the country element, all elements from the country.cities array are shown in this dropdown. The label we show points to a property of the city object: name, and when a city is selected it is stored in “$parent.selectedCity”. $parent is a property you can use to access the parent context. In this case, since we use the “with” binding, we can access the root context of the viewmodel to set the “selectedCity” property.   Show a table when a city is selected  When a user selects a city from the dropdown menu, we want to show a table with all the properties of this city. To accomplish this we use the following piece of HTML                    &lt;div data-bind=\"if: selectedCity\" class=\"span9\"&gt;                     &lt;div class=\"well\"&gt;                         &lt;h2&gt;Properties&lt;/h2&gt;                         You have chosen a city with the following properties:                         &lt;table class=\"table\"&gt;                             &lt;thead&gt;                             &lt;tr&gt;                                 &lt;th&gt;Property&lt;/th&gt;                                 &lt;th&gt;Value&lt;/th&gt;                             &lt;/tr&gt;                             &lt;/thead&gt;                             &lt;!-- iterate over each property, and add a row --&gt;                             &lt;tbody data-bind=\"foreach: selectedCity().properties\"&gt;                             &lt;tr&gt;                                 &lt;td data-bind=\"text: $data.key\"&gt;&lt;/td&gt;                                 &lt;td data-bind=\"text: $data.value\"&gt;&lt;/td&gt;                             &lt;/tr&gt;                             &lt;/tbody&gt;                         &lt;/table&gt;                     &lt;/div&gt;                 &lt;/div&gt;    We once again use a binding to see if we should render this div, in the same way we’ve shown earlier. If a city is selected we start rendering a table, and bind the “tbody” element to the list of properties with a “foreach” binding. As the name implies, this binding iterates over the properties to render the “tr” elements. We then just need to do a simple text binding to “$data.key” to render the content of the cells. Here $data is a special keyword which allows you to access the current property from the list that is being processed.    SVG Map to show the contours of the selected city   So far we’ve seen how to connect the dropdown boxes and the table to the knockout view model. If the city we selected changes, our viewmodel is updated, which also triggers an update of the table. The next component we want to show when a city is selected are the contours of the city. When we retrieved the main JSON object we also received the geo coordinates (in WGS84 format) for each city, which we stored in our model:    var city = function(name, code, properties, feature)  {     this.name = name;     this.code = code;     this.properties = properties;     this.geojson = feature;  // the contours of the city      this.getPropertyValue = function(propertyName) {         for (var i = 0 ; i &lt; properties.length ; i++) {             if (properties[i].key == propertyName) return properties[i].value;         }     } };    To render this as a map we use the excellend d3.js library, I won’t dive into too much detail (see this article for more info) on d3.js, I’ll just show how you can create a custom binding that returns a rendered map. The goal we’re aiming for is this:      For this we create a custom binding that we’ll use like this:                    &lt;div data-bind=\"if: selectedCity\" class=\"span9\"&gt;                     &lt;div class=\"well\"&gt;                         &lt;h2&gt;View of the city &lt;span                                 data-bind=\"text: selectedCity() ? selectedCity().name : 'unknown'\"&gt;&lt;/span&gt;&lt;/h2&gt;                         &lt;!--                             in this binding, we bind directly to the group element,                             whenever the selectedCity changes, we update this element using                             a custom binding.                         --&gt;                         &lt;svg id=\"localMapContainer\" xmlns=\"http://www.w3.org/2000/svg\"&gt;                             &lt;g id=\"localMap\"                                data-bind=\"d3svgSingle: selectedCity\"/&gt;                         &lt;/svg&gt;                     &lt;/div&gt;    As you can see we first use a normal knockout.js binding to bind the name to our header. And next we bind an svg group element to the custom d3svgSingle binding. This means that this binding will be invoked each time our selectedCity in the model changes. This custom binding can be added using this piece of javascript:   ko.bindingHandlers.d3svgSingle = {     init:function (element, valueAccessor, allBindingsAccessor, viewModel) {         // we don't do anything on initialize     },     update:function (element, valueAccessor, allBindingsAccessor, viewModel) {         // make sure the model is correctly initialized. If not don't render         // anything         if (typeof viewModel.selectedCity() != \"undefined\" &amp;&amp; element.id != null) {             // first remove all the elements currently set on the object             // for this we use the d3 helper function. Remember for the single             // city we render directly to the group.             d3.select(element).selectAll(\"path\").remove();              // now convert the geojson string to a path object             // and add it to the supplied binding             var svgPath = geo.path(viewModel.selectedCity().geojson);             d3.select(element)                 .append(\"path\")                 .attr(\"d\", svgPath);              // resize to fill the complete div             geons.resizeSVG(element, appConstants.CITY_VIEW_X, appConstants.CITY_VIEW_Y);         }     } }   The inline comments should explain nicely what we do here. We use d3.js geo.path function (initialization of geo object isn’t shown here) to convert our geojson information to an SVG path. We then use d3.js to append a “path” element to the group we bound the property to, and on that new path element we set the d attribute with the value of the svg path. The last step we do here is resizing the svg so it nicely fits the space we allocated for it (code not shown).   SVG Map to show the details of the selected metric   The final step we need to do, is show a map that represents the values of a selected metric:      This is also called a cchoropleth map. For this we once again create a custom binding, that we bind like this:                    &lt;!--If no city is selected, view the complete map--&gt;                 &lt;div data-bind=\"ifnot: selectedCity\" class=\"span9\"&gt;                     &lt;div class=\"well\"&gt;                               &lt;h2&gt;Statistics&lt;/h2&gt;                          &lt;!-- here we bind the svg element itself to a binding. If the                              country value changes, this element is updated --&gt;                         &lt;svg id=\"countryMapContainer\" xmlns=\"http://www.w3.org/2000/svg\"                              data-bind=\"d3svg: metricChange()\"&gt;                         &lt;/svg&gt;                     &lt;/div&gt;                 &lt;/div&gt;             &lt;/div&gt;    This div is shown when no city is selected. The content of the svg is updated whenever one of the metrics changes. Before we look at the code of the binding, lets look at how this is configured in our viewmodel.       // computed to change when either the metric     // or the map changes     this.metricChange = ko.computed(function() {         this.selectedMetric();         this.country();     }, this);   As you can see, this metricChange value is not a normal knockout.js binding, but a computed one. In other words, our view will be updated whenever either the country element changes (e.g when the model is loaded) or when the metric (the dropdown) changes. This way we can easily let a view respons to changes in more than one element of our view. The binding code itself, uses d3.js once again for rendering the map.   // custom handler to draw the complete map ko.bindingHandlers.d3svg = {     init:function (element, valueAccessor, allBindingsAccessor, viewModel) {         // we don't do anything on initialize     },     update:function (element, valueAccessor, allBindingsAccessor, viewModel) {         // if we've got a filled json object with geo information         // we use it to drawn the complete country         if (typeof viewModel.country() != \"undefined\" &amp;&amp;             typeof viewModel.selectedCity() == \"undefined\") {              // remove everything, for the total map, we render directly             // to a svg element.             d3.select(element).selectAll(\"g\").remove();              // based on whether we've got a selected metric, we run a quantize function             // on the values. To do this we first need to calculate the max and min             // values. We also store the metric values directly in a map for easy reference             var qMin, qMax, metrics = {};             if (viewModel.selectedMetric() != null) {                 console.log(viewModel.selectedMetric());                 for (i = 0; i &lt; viewModel.country().cities.length; i++) {                     var metricValue = viewModel.country().cities[i].getPropertyValue(viewModel.selectedMetric());                     var parsed = parseInt(metricValue);                     if (i == 0) {                         qMin = parsed; qMax = parsed;                     } else {                         if (parsed &lt; qMin &amp;&amp; parsed &gt;= 0) qMin = parsed;                         if (parsed &gt; qMax) qMax = parsed;                     }                     metrics[viewModel.country().cities[i].code] = parsed;                 }             }              // draw the map, and color the fields based on the metrics             drawMap(element, metrics, qMax, qMin);               // if we've got a metric selected, render a legend             if (typeof viewModel.selectedMetric() != \"undefined\" || viewModel.election().selectedParty() != null) {                 // draw the legend                 drawLegend(element, qMin, qMax)             }         }     } };   In this part of the binding we check whether the values are as we expected, and if so we determine the mininum and maximum value of our metric. We do this to determine how to draw the legend, and how to distribute the values along these colors. The actual drawing of the map and the legend is done in the drawMap and the drawLegend functions.   function drawMap(element, metrics, qMax, qMin) {      d3.select(element)         .append(\"g\")         .attr(\"id\", \"countryMap\")         .attr(\"class\", appConstants.COLORBASE)         .selectAll(\"path\")         .data(viewModel.country().geojson)         .enter().append(\"path\")         .attr(\"d\", geo.path)         .attr(\"class\", function (d) {              if (viewModel.selectedMetric() != null) {                  var metricValue = metrics[d.gm_code];                 // we have a range of qMin to qMax, which should be divided                 // into 8 equal steps.                 // (value - qMin + qMax) / 8                 var range = qMax - qMin;                 var calcValue = metricValue - qMin;                  return \"q\" + Math.min(8, ~~(calcValue / (range / 8))) + \"-9\";             }         }) }   This is d3.js code that renders a map based on geojson. The class element of the svg path we add, the contours of a single city, is determined by the function we provide. This function distributes the value of the selected metric into 8 pieces. So depending on the value a contour gets assigned a class in the form of q2-9 or q3-9 etc. The names of values of the required classes are provided by a colorbrewer css file. This file contains ranges of colors you can use to colorize maps (or other graphs):    .PuBu .q0-8{fill:rgb(255,247,251)} .PuBu .q1-8{fill:rgb(236,231,242)} .PuBu .q2-8{fill:rgb(208,209,230)} .PuBu .q3-8{fill:rgb(166,189,219)} .PuBu .q4-8{fill:rgb(116,169,207)} .PuBu .q5-8{fill:rgb(54,144,192)} .PuBu .q6-8{fill:rgb(5,112,176)} .PuBu .q7-8{fill:rgb(3,78,123)}    The drawLegend isn’t a very interesting method. This just uses basic svg constructs to draw a simple legend:    // draw a legend function drawLegend(element, qMin, qMax) {     // create a new group with the specific base color and add the lower value     d3.select(element)         .append(\"g\")             .attr(\"id\", \"legenda\").attr(\"class\", appConstants.COLORBASE)         .append(\"text\")             .attr(\"x\", \"20\").attr(\"y\", \"40\").text(\"Min: \" + Math.round(qMin*100)/100);      // add the various blocks of the legenda     d3.select(element).select(\"#legenda\").selectAll(\"rect\")         .data(d3.range(0, 8))         .enter()         .append(\"rect\")             .attr(\"width\", \"20\").attr(\"height\", \"20\").attr(\"y\", \"0\")             .attr(\"class\", function (d, i) {                 return \"q\" + i + \"-9\";             })             .attr(\"x\", function (d, i) {                 return (i + 1) * 20;              });      // add a text element     d3.select(element).select(\"#legenda\").append(\"text\")         .attr(\"x\", \"140\").attr(\"y\", \"40\").text(\"Max: \" + Math.round(qMax*100)/100) }   With this last legend we’ve discussed all the various parts of our first application. We’ve shown the part knockout.js plays in this and we’ve used d3.js to render svg elements. The last thing we do in this tutorial is setting up a basic bootstrap layout. Bootstrap is a project from Twitter that allows you to easily create grid and flexible CSS based layouts. For this example I’ve used the following layout (full index.html without bindings):    &lt;!DOCTYPE html&gt; &lt;html lang=\"en\"&gt; &lt;head&gt;     &lt;title&gt;Dutch Elections: Step 1&lt;/title&gt;     &lt;!-- load externally used libraries --&gt;     &lt;script type='text/javascript' src='libraries/jquery/jquery-1.7.2.js'&gt;&lt;/script&gt;     &lt;script type='text/javascript' src='libraries/d3/d3.js'&gt;&lt;/script&gt;     &lt;script type='text/javascript' src='libraries/d3/d3.geo.js'&gt;&lt;/script&gt;     &lt;script type='text/javascript' src='libraries/knockout/knockout-2.0.0.js'&gt;&lt;/script&gt;      &lt;!-- load boot strap styles --&gt;     &lt;link href=\"libraries/bootstrap/assets/css/bootstrap.css\" rel=\"stylesheet\"&gt;     &lt;link href=\"libraries/bootstrap/assets/css/bootstrap-responsive.css\" rel=\"stylesheet\"&gt;      &lt;!-- load own styles --&gt;     &lt;link href=\"css/style.css\" rel=\"stylesheet\"&gt;      &lt;!-- load colorbrewer styles --&gt;     &lt;link href=\"css/colorbrewer.css\" rel=\"stylesheet\"&gt;      &lt;!-- From bootstrap, support for IE6-8 support of HTML5 elements --&gt;     &lt;!--[if lt IE 9]&gt;     &lt;script src=\"http://html5shim.googlecode.com/svn/trunk/html5.js\"&gt;&lt;/script&gt;     &lt;![endif]--&gt; &lt;/head&gt;  &lt;body&gt;  &lt;!-- use the bootstap fluid container --&gt; &lt;div class=\"container-fluid\"&gt;     &lt;div class=\"well-small\"&gt;         &lt;h1&gt;Election using Knockout.js, Bootstrap and d3.js&lt;/h1&gt;     &lt;/div&gt;      &lt;div class=\"row-fluid\"&gt;         &lt;!-- left column, contains the select options --&gt;         &lt;div class=\"span3\"&gt;             &lt;div class=\"well\"&gt;                  &lt;!--                     Bind to cities view model. Show the name of the city element. Bind                     the selected element to selectedCity. Has default element with value                     'Choose...'                 --&gt;                 &lt;div data-bind=\"with: country\"&gt;                     ...                 &lt;/div&gt;                  &lt;!--                     Bind to list of available metrics. Only do this, when                     no city has been selected.                 --&gt;                 &lt;div data-bind=\"ifnot: selectedCity\"&gt;                  ...                 &lt;/div&gt;             &lt;/div&gt;         &lt;/div&gt;           &lt;!-- right column shows the information --&gt;         &lt;div class=\"span9\"&gt;             &lt;div class=\"row\"&gt;                 &lt;!--If a city is selected, view the city--&gt;                 &lt;div data-bind=\"if: selectedCity\" class=\"span9\"&gt;                     &lt;div class=\"well\"&gt;                         ...                     &lt;/div&gt;                 &lt;/div&gt;                  &lt;div data-bind=\"ifnot: selectedCity\" class=\"span9\"&gt;                     &lt;div class=\"well\"&gt;                    ...                     &lt;/div&gt;                 &lt;/div&gt;             &lt;/div&gt;              &lt;div class=\"row\"&gt;                 &lt;div data-bind=\"if: selectedCity\" class=\"span9\"&gt;                     &lt;div class=\"well\"&gt;                     ...                     &lt;/div&gt;                 &lt;/div&gt;             &lt;/div&gt;              &lt;div class=\"row\"&gt;                 &lt;!--                     this element is processed if a city is selected. This shows all                     the properties of the selected city in a table                 --&gt;                 &lt;div data-bind=\"if: selectedCity\" class=\"span9\"&gt;                     &lt;div class=\"well\"&gt;                      ...                     &lt;/div&gt;                 &lt;/div&gt;             &lt;/div&gt;         &lt;/div&gt;     &lt;/div&gt; &lt;/div&gt;   &lt;!-- after the html is loaded, load the application and bind      the variables. --&gt; &lt;script type='text/javascript' src='js/app/election-constants.js'&gt;&lt;/script&gt; &lt;script type='text/javascript' src='js/models/city.js'&gt;&lt;/script&gt; &lt;script type='text/javascript' src='js/app/util-geo.js'&gt;&lt;/script&gt; &lt;script type='text/javascript' src='js/app/util-json.js'&gt;&lt;/script&gt; &lt;script type='text/javascript' src='js/app/election-bindings.js'&gt;&lt;/script&gt; &lt;script type='text/javascript' src='js/app/election.js'&gt;&lt;/script&gt;  &lt;!-- finally load all the bootstrap assets --&gt; &lt;script src=\"libraries/bootstrap/assets/js/jquery.js\"&gt;&lt;/script&gt; &lt;script src=\"libraries/bootstrap/assets/js/bootstrap-transition.js\"&gt;&lt;/script&gt; &lt;script src=\"libraries/bootstrap/assets/js/bootstrap-alert.js\"&gt;&lt;/script&gt; &lt;script src=\"libraries/bootstrap/assets/js/bootstrap-modal.js\"&gt;&lt;/script&gt; &lt;script src=\"libraries/bootstrap/assets/js/bootstrap-dropdown.js\"&gt;&lt;/script&gt; &lt;script src=\"libraries/bootstrap/assets/js/bootstrap-scrollspy.js\"&gt;&lt;/script&gt; &lt;script src=\"libraries/bootstrap/assets/js/bootstrap-tab.js\"&gt;&lt;/script&gt; &lt;script src=\"libraries/bootstrap/assets/js/bootstrap-tooltip.js\"&gt;&lt;/script&gt; &lt;script src=\"libraries/bootstrap/assets/js/bootstrap-popover.js\"&gt;&lt;/script&gt; &lt;script src=\"libraries/bootstrap/assets/js/bootstrap-button.js\"&gt;&lt;/script&gt; &lt;script src=\"libraries/bootstrap/assets/js/bootstrap-collapse.js\"&gt;&lt;/script&gt; &lt;script src=\"libraries/bootstrap/assets/js/bootstrap-carousel.js\"&gt;&lt;/script&gt; &lt;script src=\"libraries/bootstrap/assets/js/bootstrap-typeahead.js\"&gt;&lt;/script&gt; &lt;/body&gt; &lt;/html&gt;    I won’t go into too much detail on bootstrap yet. For this example we’ve only used a small part of what bootstrap offers. The only thing we used are:      well-small class: nicely render the dropdown boxes   row-fluid class: For a fluid two column layout. Left column, contains the drop down boxes. Right column the information   span# class: to determine the size of a specific column and row   well class: to nicely render the content in a grey area   row class: renders a row inside a span# div   In the next articles we’ll spent more time on the features bootstrap offers us. If you want more information on the elements used, look at the layout section of bootstrap: http://twitter.github.com/bootstrap/scaffolding.html#gridSystem   That’s it for this tutorial. In the next tutorial we’ll introduce actual election results and use bootstrap to define the layout we’ll be using. We’ll also extend knockout.js with Sammy.js to handle url mapping for back/forward button support and show you more how you can use d3.js to render graphs and show information in interesting ways. If you want to look at all the source code, open up http://www.smartjava.org/examples/election/part1.html in your browser and look at the source code.  ","categories": ["posts","bootstrap","knockout.js","javascript","rest","d3.js"],
        "tags": [],
        "url": "http://www.smartjava.org/content/election-site-part-1-basics-knockoutjs-bootstrap-and-d3js/",
        "teaser":null},{
        "title": "Connect to RabbitMQ (AMQP) using Scala, Play and Akka",
        "excerpt":"In this article we’ll look at how you can connect from Scala to RabbitMQ so you can support the AMQP protocol from your applications. In this example I’ll use the Play Framework 2.0 as container (for more info on this see my other article on this subject) to run the application in, since Play makes developing with Scala a lot easier. This article will also use Akka actors to send and receive the messages from RabbitMQ.   Update and shameless plug for someone else's book:  If you want some more information on RabbitMQ, Manning, who also is the publisher for my two books, has just released RabbitMQ in action. So if you’re interested in learning more about RabbitMQ, you can buy the book directly from the Manning site: http://www.manning.com/videla/. If you want to buy it with a 37% discount, use rabbit37 as promotional code (and no, I don’t get anything from this ;-).   What is AMQP  First, a quick introduction into AMQP. AMQP stands for “Advanced Message Queueing Protocol” and is an open standard for messaging. The AMQP homepage states their vision as this: “To become the standard protocol for interoperability between all messaging middleware”. AMQP defines a transport level protocol for exchanging messages that can be used to integrate applications from a number of different platform, languages and technologies.   There are a number of tools implementing this protocol, but one that is getting more and more attention is RabbitMQ. RabbitMQ is an open source, erlang based message broker that uses AMQP. All application that can speak AMQP can connect to and make use of RabbitMQ. So in this article we’ll show how you can connect from your Play2/Scala/Akka based application to RabbitMQ.   In this article we’ll show you how to do implement the two most common scenarios:     Send / recieve: We'll configure one sender to send a message every couple of seconds, and use two listeners that will read the messages, in a round robin fashion, from the queue.   Publish / subscribe: For this example we'll create pretty much the same scenario, but this time, the listeners will both get the message at the same time.   I assume you’ve got an installation of RabbitMQ. If not follow the instructions from their site.   Setup basic Play 2 / Scala project   For this example I created a new Play 2 project. Doing this is very easy:    jos@Joss-MacBook-Pro.local:~/Dev/play-2.0-RC2$ ./play new Play2AndRabbitMQ        _            _   _ __ | | __ _ _  _| | | '_ \\| |/ _' | || |_| |  __/|_|\\____|\\__ (_) |_|            |__/                play! 2.0-RC2, http://www.playframework.org  The new application will be created in /Users/jos/Dev/play-2.0/PlayAndRabbitMQ  What is the application name?  &gt; PlayAndRabbitMQ  Which template do you want to use for this new application?     1 - Create a simple Scala application   2 - Create a simple Java application   3 - Create an empty project  &gt; 1  OK, application PlayAndRabbitMQ is created.  Have fun!    I am used to work from Eclipse with the scala-ide pluging, so I execute play eclipsify and import the project in Eclipse.   The next step we need to do is set up the correct dependencies. Play uses sbt for this and allows you to configure your dependencies from the build.scala file in your project directory. The only dependency we’ll add is the java client library from RabbitMQ. Even though Lift provides a scala based AMQP library, I find using the RabbitMQ one directly just as easy. After adding the dependency my build.scala looks like this:    import sbt._ import Keys._ import PlayProject._  object ApplicationBuild extends Build {      val appName         = \"PlayAndRabbitMQ\"     val appVersion      = \"1.0-SNAPSHOT\"      val appDependencies = Seq(       \"com.rabbitmq\" % \"amqp-client\" % \"2.8.1\"     )      val main = PlayProject(appName, appVersion, appDependencies, mainLang = SCALA).settings(     ) }    Add rabbitMQ configuration to the config file   For our examples we can configure a couple of things. The queue where to send the message to, the exchange to use, and the host where RabbitMQ is running. In a real world scenario we would have more configuration options to set, but for this case we’ll just have these three. Add the following to your application.conf so that we can reference it from our application.    #rabbit-mq configuration rabbitmq.host=localhost rabbitmq.queue=queue1 rabbitmq.exchange=exchange1    We can now access these configuration files using the ConfigFactory. To allow easy access create the following object:    object Config {   val RABBITMQ_HOST = ConfigFactory.load().getString(\"rabbitmq.host\");   val RABBITMQ_QUEUE = ConfigFactory.load().getString(\"rabbitmq.queue\");   val RABBITMQ_EXCHANGEE = ConfigFactory.load().getString(\"rabbitmq.exchange\"); }    Initialize the connection to RabbitMQ   We’ve got one more object to define before we’ll look at how we can use RabbitMQ to send and receive messages. to work with RabbitMQ we require a connection. We can get a connection to a server by using a ConnectionFactory. Look at the javadocs for more information on how to configure the connection.    object RabbitMQConnection {    private val connection: Connection = null;    /**    * Return a connection if one doesn't exist. Else create    * a new one    */   def getConnection(): Connection = {     connection match {       case null =&gt; {         val factory = new ConnectionFactory();         factory.setHost(Config.RABBITMQ_HOST);         factory.newConnection();       }       case _ =&gt; connection     }   } }    Start the listeners when the application starts   We need to do one more thing before we can look at the RabbitMQ code. We need to make sure our message listeners are registered on application startup and our senders start sending. Play 2 provides a  GlobalSettings object for this which you can extend to execute code when your application starts. For our example we’ll use the following object (remember, this needs to be stored in the default namespace:    import play.api.mvc._ import play.api._ import rabbitmq.Sender  object Global extends GlobalSettings {      override def onStart(app: Application) {     Sender.startSending   } }    We’ll look at this Sender.startSending operation, which initializes all the senders and receivers in the following sections.   Setup send and receive scenario   Let’s look at the Sender.startSending code that will setup a sender that sends a msg to a specific queue. For this we use the following piece of code:    object Sender {    def startSending = {     // create the connection     val connection = RabbitMQConnection.getConnection();     // create the channel we use to send     val sendingChannel = connection.createChannel();     // make sure the queue exists we want to send to     sendingChannel.queueDeclare(Config.RABBITMQ_QUEUE, false, false, false, null);        Akka.system.scheduler.schedule(2 seconds, 1 seconds           , Akka.system.actorOf(Props(                new SendingActor(channel = sendingChannel,                                            queue = Config.RABBITMQ_QUEUE)))           , \"MSG to Queue\");   } }  class SendingActor(channel: Channel, queue: String) extends Actor {    def receive = {     case some: String =&gt; {       val msg = (some + \" : \" + System.currentTimeMillis());       channel.basicPublish(\"\", queue, null, msg.getBytes());       Logger.info(msg);     }     case _ =&gt; {}   } }    In this code we take the following steps:     Use the factory to retrieve a connection to RabbitMQ   Create a channel on this connection to use in communicating with RabbitMQ   Use the channel to create the queue (if it doesn't exist yet)   Schedule Akka to send a message to an actor every second.   This all should be pretty straightforward. The only (somewhat) complex part is the scheduling part. What this schedule operation does is this. We tell Akka to schedule a message to be sent to an actor. We want a 2 seconds delay before it is fired, and we want to repeat this job every second. The actor that should be used for this is the SendingActor you can also see in this listing. This actor needs access to a channel to send a message and this actor also needs to know where to send the message it receives to. This is the queue.  So every second this Actor will receive a message, append a timestamp, and use the provided channel to send this message to the queue: channel.basicPublish(“”, queue, null, msg.getBytes());. Now that we send a message each second it would be nice to have listeners on this queue that can receive messages. For receiving messages we’ve also created an Actor that listens indefinitely on a specific queue.    class ListeningActor(channel: Channel, queue: String, f: (String) =&gt; Any) extends Actor {    // called on the initial run   def receive = {     case _ =&gt; startReceving   }    def startReceving = {          val consumer = new QueueingConsumer(channel);     channel.basicConsume(queue, true, consumer);      while (true) {       // wait for the message       val delivery = consumer.nextDelivery();       val msg = new String(delivery.getBody());              // send the message to the provided callback function       // and execute this in a subactor       context.actorOf(Props(new Actor {         def receive = {           case some: String =&gt; f(some);         }       })) ! msg     }   } }    This actor is a little bit more complex than the one we used for sending. When this actor receives a message (kind of message doesn’t matter) it starts listening on the queue it was created with. It does this by creating a consumer using the supplied channel and tells the consumers to start listening on the specified queue. The consumer.nextDelivery() method will block until a message is waiting in the configured queue. Once a message is received, a new Actor is created to which the message is sent. This new actor passes the message on to the supplied method, where you can put your business logic.   To use this listener we need to supply the following arguments:      Channel: Allows access to RabbitMQ   Queue: The queue to listen to for messages   f: The function that we'll execute when a message is received.   The final step for this first example is glueing everything together. We do this by adding a couple of method calls to the Sender.startSending method.      def startSending = {    ...     val callback1 = (x: String) =&gt; Logger.info(\"Recieved on queue callback 1: \" + x);          setupListener(connection.createChannel(),Config.RABBITMQ_QUEUE, callback1);      // create an actor that starts listening on the specified queue and passes the     // received message to the provided callback     val callback2 = (x: String) =&gt; Logger.info(\"Recieved on queue callback 2: \" + x);          // setup the listener that sends to a specific queue using the SendingActor     setupListener(connection.createChannel(),Config.RABBITMQ_QUEUE, callback2);    ...   }    private def setupListener(receivingChannel: Channel, queue: String, f: (String) =&gt; Any) {     Akka.system.scheduler.scheduleOnce(2 seconds,          Akka.system.actorOf(Props(new ListeningActor(receivingChannel, queue, f))), \"\");   }    In this code you can see that we define a callback function, and use this callback function, together with the queue and the channel to create the ListeningActor. We use the scheduleOnce method to start this listener in a separate thread. Now with this code in place we can run the application (play run) open up localhost:9000 to start the application and we should see something like the following output.    [info] play - Starting application default Akka system. [info] play - Application started (Dev) [info] application - MSG to Exchange : 1334324531424 [info] application - MSG to Queue : 1334324531424 [info] application - Recieved on queue callback 2: MSG to Queue : 1334324531424 [info] application - MSG to Exchange : 1334324532522 [info] application - MSG to Queue : 1334324532522 [info] application - Recieved on queue callback 1: MSG to Queue : 1334324532522 [info] application - MSG to Exchange : 1334324533622 [info] application - MSG to Queue : 1334324533622 [info] application - Recieved on queue callback 2: MSG to Queue : 1334324533622 [info] application - MSG to Exchange : 1334324534722 [info] application - MSG to Queue : 1334324534722 [info] application - Recieved on queue callback 1: MSG to Queue : 1334324534722 [info] application - MSG to Exchange : 1334324535822 [info] application - MSG to Queue : 1334324535822 [info] application - Recieved on queue callback 2: MSG to Queue : 1334324535822    Here you can clearly see the round-robin way messages are processed.   Setup publish and subscribe scenario   Once we’ve got the above code running, adding publish / subscribe functionality is very trivial. Instead of the SendingActor we now use a PublishingActor:    class PublishingActor(channel: Channel, exchange: String) extends Actor {    /**    * When we receive a message we sent it using the configured channel    */   def receive = {     case some: String =&gt; {       val msg = (some + \" : \" + System.currentTimeMillis());       channel.basicPublish(exchange, \"\", null, msg.getBytes());       Logger.info(msg);     }     case _ =&gt; {}   } }    An exchange is used by RabbitMQ to allow multiple recipients to receive the same message (and a whole lot of other advanced functionality). The only change in the code from the other actor is that this time we send the message to an exchange instead of to a queue. The listener code is exactly the same, the only thing we need to do is connect a queue to a specific exchange. So that listeners on that queue receive the messages sent to to the exchange. We do this, once again, from the setup method we used earlier.        ...     // create a new sending channel on which we declare the exchange     val sendingChannel2 = connection.createChannel();     sendingChannel2.exchangeDeclare(Config.RABBITMQ_EXCHANGEE, \"fanout\");          // define the two callbacks for our listeners     val callback3 = (x: String) =&gt; Logger.info(\"Recieved on exchange callback 3: \" + x);     val callback4 = (x: String) =&gt; Logger.info(\"Recieved on exchange callback 4: \" + x);          // create a channel for the listener and setup the first listener     val listenChannel1 = connection.createChannel();     setupListener(listenChannel1,listenChannel1.queueDeclare().getQueue(),                     Config.RABBITMQ_EXCHANGEE, callback3);          // create another channel for a listener and setup the second listener     val listenChannel2 = connection.createChannel();     setupListener(listenChannel2,listenChannel2.queueDeclare().getQueue(),                     Config.RABBITMQ_EXCHANGEE, callback4);          // create an actor that is invoked every two seconds after a delay of     // two seconds with the message \"msg\"     Akka.system.scheduler.schedule(2 seconds, 1 seconds, Akka.system.actorOf(Props(                new PublishingActor(channel = sendingChannel2                     , exchange = Config.RABBITMQ_EXCHANGEE))),           \"MSG to Exchange\");     ...    We also created an overloaded method for setupListener, which, as an extra parameter, also accepts the name of the exchange to use.      private def setupListener(channel: Channel, queueName : String, exchange: String, f: (String) =&gt; Any) {     channel.queueBind(queueName, exchange, \"\");          Akka.system.scheduler.scheduleOnce(2 seconds,          Akka.system.actorOf(Props(new ListeningActor(channel, queueName, f))), \"\");   }    In this small piece of code you can see that we bind the supplied queue (which is a random name in our example) to the specified exchange. After that we create a new listener as we’ve seen before.   Running this code now will result in the following output:    [info] play - Application started (Dev) [info] application - MSG to Exchange : 1334325448907 [info] application - MSG to Queue : 1334325448907 [info] application - Recieved on exchange callback 3: MSG to Exchange : 1334325448907 [info] application - Recieved on exchange callback 4: MSG to Exchange : 1334325448907 [info] application - MSG to Exchange : 1334325450006 [info] application - MSG to Queue : 1334325450006 [info] application - Recieved on exchange callback 4: MSG to Exchange : 1334325450006 [info] application - Recieved on exchange callback 3: MSG to Exchange : 1334325450006    As you can see, in this scenario both listeners receive the same message. That pretty much wraps it up for this article. As you’ve seen using the Java based client api for RabbitMQ is more than sufficient, and easy to use from Scala. Note though that this example is not production ready, you should take care to close connections, nicely shutdown listeners and actors. All this shutdown code isn’t shown here.  ","categories": ["posts","play","amqp","rabbitmq","scala"],
        "tags": [],
        "url": "http://www.smartjava.org/content/connect-rabbitmq-amqp-using-scala-play-and-akka/",
        "teaser":null},{
        "title": "Face detection using HTML5, javascript, webrtc, websockets, Jetty and OpenCV",
        "excerpt":"Through HTML5 and the corresponding standards, modern browsers get more standarized features with every release. Most people have heard of websockets that allows you to easily setup a two way communication channel with a server, but one of the specifications that hasn’t been getting much coverage is the webrtc specificiation. &lt;p&gt; &lt;/p&gt; With the webrtc specification it will become easier to create pure HTML/Javascript real-time video/audio related applications where you can access a user’s microphone or webcam and share this data with other peers on the internet. For instance you can create video conferencing software that doesn’t require a plugin, create a baby monitor using your mobile phone or more easily facilitate webcasts. All using cross-browser features without the use of a single plugin. &lt;p&gt; &lt;/p&gt;   Update: in the newest versions of the webrtc spec we can also access the microphone! See here for an explanation! Update:This article is translated to Serbo-Croatian language by Vera Djuraskovic from  Webhostinggeeks.com.   As with a lot of HTML5 related specification, the webrtc one isn’t quite finished yet, and support amongst browsers is minimal. However, you can still do very cool things with the support that is currently available in the development builds of Opera and the latest Chrome builds. In this article I’ll show you how to do use webrtc and a couple of other HTML5 standards to accomplish the following:      For this we need to take the following steps:      Access the user's webcam through the getUserMedia feature   Send the webcam data using websockets to a server   At the server, we analyze the received data, using JavaCV/OpenCV to detect and mark any face that is recognized   Use websockets to send the data back from the server to the client   Show the received information from the server to the client   In other words, we’re going to create a real-time face detection system, where the frontend is completely provided by ‘standard’ HTML5/Javascript functionality. As you’ll see in this article, we’ll have to use a couple of workarounds, because some features haven’t been implemented yet.   Which tools and technologies do we use  Lets start by looking at the tools and technologies that we’ll use to create our HTML5 face detection system. We’ll start with the frontend technologies.      Webrtc: The specification page says this. These APIs should enable building applications that can be run inside a browser, requiring no extra downloads or plugins, that allow communication between parties using audio, video and supplementary real-time communication, without having to use intervening servers (unless needed for firewall traversal, or for providing intermediary services).   Websockets: Again, from the spec. To enable Web applications to maintain bidirectional communications with server-side processes, this specification introduces the WebSocket interface.   Canvas: And also from the spec: Element provides scripts with a resolution-dependent bitmap canvas, which can be used for rendering graphs, game graphics, or other visual images on the fly.   What do we use at the backend:     Jetty: Provides us with a great websockets implementation    OpenCV: Library that has all kind of algorithms for image manipulation. We use their support for face recognition.   JavaCV: We want to use OpenCV directly from Jetty to detect images based on the data we receive. With JavaCV we can use the features of OpenCV through a Java wrapper.   Frontend step 1: Enable mediastream in Chrome and access the webcam   Let’s start with accessing the webcam. In my example I’ve used the latest version of Chrome (canary) that has support for this part of the webrtc specifcation. Before you can use it, you first have to enable it. You can do this by opening the “chrome://flags/” URL and enable the mediastream feature:      Once you’ve enabled it (and have restarted the browser), you can use some of the features of webrtc to access the webcam directly from the browser without having to use a plugin. All you need to do to access the webcam is use the following piece of html and javascript:    &lt;div&gt;     &lt;video id=\"live\" width=\"320\" height=\"240\" autoplay&gt;&lt;/video&gt; &lt;/div&gt;   And the following javascript:     video = document.getElementById(\"live\")     var ctx;     // use the chrome specific GetUserMedia function     navigator.webkitGetUserMedia(\"video\",             function(stream) {                 video.src = webkitURL.createObjectURL(stream);             },             function(err) {                 console.log(\"Unable to get video stream!\")             }     )    With this small piece of HTML and javascript we can access the user’s webcam and show the stream in the HTML5 video element. We do this by first requesting access to the webcam by using the getUserMedia function (prefixed with the chrome specific webkit prefix). In the callback we pass in, we get access to a stream object. This stream object is the stream from the user’s webcam. To show this stream we need to attach it to the video element. The src attribute of the video element allows us to specify an URL to play. With another new HTML5 feature we can convert the stream to an URL. This is done by using the URL.CreateObjectURL function (once again prefixed). The result of this function is an URL which we attach to the video element. And that’s all it takes to get access to the stream of a user’s webcam:      The next thing we want to do is send this stream, using websockets, to the jetty server.   Frontend step 2: Send stream to Jetty server over websockets   In this step we want to take the data from the stream, and send it as binary data over a websocket to the listening Jetty server. In theory this sounds simple. We’ve got a binary stream of video information, so we should be able to just access the bytes and instead of streaming the data to the video element, we instead stream it over a websocket to our remote server. In practice though, this doesn’t work. The stream object you receive from the getUserMedia function call, doesn’t have an option to access it data as a stream. Or better said, not yet. If you look at the specifications you should be able to call record() to get access to a recorder. This recorder can then be used to access the raw data. Unfortunately, this functionality isn’t supported yet in any browser. So we need to find an alternative. For this we basically just have one option:      Take a snapshot of the current video.   Paint this to the canvas element.   Grab the data from the canvas as an image.   Send the imagedata over websockets.   A bit of a workaround that causes a lot of extra processing on the client side and results in a much higher amount of data being sent to the server, but it works. Implementing this isn’t that hard:   &lt;div&gt;     &lt;video id=\"live\" width=\"320\" height=\"240\" autoplay style=\"display: inline;\"&gt;&lt;/video&gt;     &lt;canvas width=\"320\" id=\"canvas\" height=\"240\" style=\"display: inline;\"&gt;&lt;/canvas&gt; &lt;/div&gt;   &lt;script type=\"text/javascript\"&gt;     var video = $(\"#live\").get()[0];     var canvas = $(\"#canvas\");     var ctx = canvas.get()[0].getContext('2d');      navigator.webkitGetUserMedia(\"video\",             function(stream) {                 video.src = webkitURL.createObjectURL(stream);             },             function(err) {                 console.log(\"Unable to get video stream!\")             }     )      timer = setInterval(             function () {                 ctx.drawImage(video, 0, 0, 320, 240);             }, 250); &lt;/script&gt;   Not that much more complex then our previous piece of code. What we added was a timer and a canvas on which we can draw. This timer is run every 250ms and draws the current video image to the canvas (as you can see in the following screenshot):    As you can see the canvas has a bit of a delay. You can tune this by setting the interval lower, but this does require a lot more resources.   The next step is to grab the image from the canvas, convert it to binary, and send it over a websocket. Before we look at the websocket part, let’s first look at the data part. To get the data we extend the timer function with the following piece of code:        timer = setInterval(             function () {                 ctx.drawImage(video, 0, 0, 320, 240);                 var data = canvas.get()[0].toDataURL('image/jpeg', 1.0);                 newblob = dataURItoBlob(data);             }, 250);     }    The toDataURL function copies the content from the current canvas and stores it in a dataurl. A dataurl is a string containing base64 encoded binary data. For our example it looks a bit like this:    data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAEBAQEBAQEBAQEBAQEB AQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEB AQH/2wBDAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBAQEBA QEBAQEBAQEBAQEBAQEBAQEBAQEBAQH/wAARCADwAUADASIAAhEBAxEB/8QAHwAAAQU .. snip .. QxL7FDBd+0sxJYZ3Ma5WOSYxwMEBViFlRvmIUFmwUO7O75Q4OSByS5L57xcoJuVaSTTpyfJ RSjKfxayklZKpzXc1zVXVpxlGRKo1K8pPlje6bs22oxSau4R9289JNJuLirpqL4p44FcQMkYMjrs+z vhpNuzDBjlmJVADuwMLzsIy4OTMBvAxuDM+AQW2vsVzIoyQQwG1j8hxt6VELxd7L5caoT5q4kj uc4rku4QjOPI4tNXxkua01y8uijJtSTS80le0Z6WjJuz5pXaa//2Q==     We could send this over as a text message and let the serverside decode it, but since websockets also allows us to send binary data, we’ll convert this to binary. We need to do this in two steps, since canvas doesn’t allow us (or I don’t know how) direct access to the binary data. Luckily someone at stackoverflow created a nice helper method for this (dataURItoBlob) that does exactly what we need (for more info and the code see this post).At this point we’ve got a data array containing a screenshot of the current video that is taken at the specified interval. The next step, and for now final step, at the client is to send this using websockets.   Using websockets from Javascript is actually very easy. You just need to specify the websockets url and implement a couple of callback functions. The first thing we need to do is open the connection:        var ws = new WebSocket(\"ws://127.0.0.1:9999\");     ws.onopen = function () {               console.log(\"Openened connection to websocket\");     }    Assuming everything went ok, we now have a two-way websockets connection. Sending data over this connection is as easy as just calling ws.send:        timer = setInterval(             function () {                 ctx.drawImage(video, 0, 0, 320, 240);                 var data = canvas.get()[0].toDataURL('image/jpeg', 1.0);                 newblob = dataURItoBlob(data);                 ws.send(newblob);             }, 250);     }    That’s it for the client side. If we open this page, we’ll get/request access to the user webcam, show the stream from the webcam in a video element, capture the video at a specific interval, and send the data using websockets to the backend server for further processing.   Setup backend environment   The backend for this example was created using Jetty’s websocket support (in a later article I’ll see if I can also get it running using Play 2.0 websockets support). With Jetty it is really easy to launch a server with a websocket listener. I usually run Jetty embedded, and to get websockets up and running I use the following simple Jetty launcher.   public class WebsocketServer extends Server { \t \tprivate final static Logger LOG = Logger.getLogger(WebsocketServer.class); \t \tpublic WebsocketServer(int port) { \t\tSelectChannelConnector connector = new SelectChannelConnector(); \t\tconnector.setPort(port); \t\taddConnector(connector); \t\t \t\tWebSocketHandler wsHandler = new WebSocketHandler() { \t\t\tpublic WebSocket doWebSocketConnect(HttpServletRequest request,\tString protocol) { \t\t\t\treturn new FaceDetectWebSocket(); \t\t\t} \t\t}; \t\tsetHandler(wsHandler); \t}  \t/** \t * Simple innerclass that is used to handle websocket connections. \t *  \t * @author jos \t */ \tprivate static class FaceDetectWebSocket implements WebSocket, \t\t\tWebSocket.OnBinaryMessage, WebSocket.OnTextMessage {  \t\tprivate Connection connection; \t\tprivate FaceDetection faceDetection = new FaceDetection(); \t\t \t\tpublic FaceDetectWebSocket() { \t\t\tsuper(); \t\t}  \t\t/** \t\t * On open we set the connection locally, and enable \t\t * binary support \t\t */ \t\tpublic void onOpen(Connection connection) { \t\t\tthis.connection = connection; \t\t\tthis.connection.setMaxBinaryMessageSize(1024 * 512); \t\t} \t\t \t\t/** \t\t * Cleanup if needed. Not used for this example \t\t */ \t\tpublic void onClose(int code, String message) {}  \t\t/** \t\t * When we receive a binary message we assume it is an image. We then run this \t\t * image through our face detection algorithm and send back the response. \t\t */ \t\tpublic void onMessage(byte[] data, int offset, int length) { \t\t\t \t\t\tByteArrayOutputStream bOut = new ByteArrayOutputStream(); \t\t\tbOut.write(data, offset, length); \t\t\ttry { \t\t\t\tbyte[] result = faceDetection.convert(bOut.toByteArray());\t\t\t\t \t\t\t\tthis.connection.sendMessage(result, 0, result.length); \t\t\t} catch (IOException e) { \t\t\t\tLOG.error(\"Error in facedetection, ignoring message:\" + e.getMessage()); \t\t\t} \t\t} \t\t   \t} \t \t/** \t * Start the server on port 999 \t */ \tpublic static void main(String[] args) throws Exception { \t\tWebsocketServer server = new WebsocketServer(9999); \t\tserver.start(); \t\tserver.join(); \t}\t }   A big source file, but not so hard to understand. The import parts are creating a handler that supports the websocket protocol. In this listing we create a WebSocketHandler that always returns the same WebSocket. In a real world scenario you’d determine the type of WebSocket based on properties or URL, in this example we just always return this same one. The websocket itself isn’t that complex either, but we do need to configure a couple of things for everything to work correctly. In the onOpen method we do the following:   \t\tpublic void onOpen(Connection connection) { \t\t\tthis.connection = connection; \t\t\tthis.connection.setMaxBinaryMessageSize(1024 * 512); \t\t}   This enables support for binary message. Our WebSocket can now receive binary messages up to 512KB, since we don’t directly stream the data, but send a canvas rendered image the message size is rather large. 512KB however is more then enough for messages sized 640x480. Our face detection also works great with a resolution of just 320x240, so this should be enough. The processing of the received binary image is done in the onMessage method:  \t\tpublic void onMessage(byte[] data, int offset, int length) { \t\t\t \t\t\tByteArrayOutputStream bOut = new ByteArrayOutputStream(); \t\t\tbOut.write(data, offset, length); \t\t\ttry { \t\t\t\tbyte[] result = faceDetection.convert(bOut.toByteArray());\t\t\t\t \t\t\t\tthis.connection.sendMessage(result, 0, result.length); \t\t\t} catch (IOException e) { \t\t\t\tLOG.error(\"Error in facedetection, ignoring message:\" + e.getMessage()); \t\t\t} \t\t}   This isn’t really optimized code, but its intentions should be clear. We get the data sent from the client, write it to a bytearray with fixed size and pass it onto the faceDetection class. This faceDetection class does its magic and returns the processed image. This processed image is the same as the original one, but now with a yellow rectangle indicating the found face.      This processed image is sent back over the same websocket connection to be processed by the HTML client. Before we look at how we can show this data using javascript, we’ll have a quick look at the FaceDetection class.   The FaceDetection class uses a CvHaarClassifierCascade from JavaCV, java wrappers for OpenCV, to detect a face.  I won’t go into too much detail how face detection works, since that is a very extensive subject in it self.   public class FaceDetection {  \tprivate static final String CASCADE_FILE = \"resources/haarcascade_frontalface_alt.xml\"; \t \tprivate int minsize = 20; \tprivate int group = 0; \tprivate double scale = 1.1;  \t/** \t * Based on FaceDetection example from JavaCV. \t */ \tpublic byte[] convert(byte[] imageData) throws IOException { \t\t// create image from supplied bytearray \t\tIplImage originalImage = cvDecodeImage(cvMat(1, imageData.length,CV_8UC1, new BytePointer(imageData)));  \t\t// Convert to grayscale for recognition \t\tIplImage grayImage = IplImage.create(originalImage.width(), originalImage.height(), IPL_DEPTH_8U, 1); \t\tcvCvtColor(originalImage, grayImage, CV_BGR2GRAY);  \t\t// storage is needed to store information during detection \t\tCvMemStorage storage = CvMemStorage.create();  \t\t// Configuration to use in analysis \t\tCvHaarClassifierCascade cascade = new CvHaarClassifierCascade(cvLoad(CASCADE_FILE));  \t\t// We detect the faces. \t\tCvSeq faces = cvHaarDetectObjects(grayImage, cascade, storage, scale, group, minsize);  \t\t// We iterate over the discovered faces and draw yellow rectangles around them. \t\tfor (int i = 0; i &lt; faces.total(); i++) { \t\t\tCvRect r = new CvRect(cvGetSeqElem(faces, i)); \t\t\tcvRectangle(originalImage, cvPoint(r.x(), r.y()), \t\t\t\t\tcvPoint(r.x() + r.width(), r.y() + r.height()), \t\t\t\t\tCvScalar.YELLOW, 1, CV_AA, 0); \t\t}  \t\t// convert the resulting image back to an array \t\tByteArrayOutputStream bout = new ByteArrayOutputStream(); \t\tBufferedImage imgb = originalImage.getBufferedImage(); \t\tImageIO.write(imgb, \"png\", bout); \t\treturn bout.toByteArray(); \t} }   The code should at least explain the steps. For more info on how this really works you should look at the OpenCV and JavaCV websites. By changing the cascade file, and playing around with the minsize, group and scale properties you can also use this to detect eyes, nose, ears, pupils etc. For instance eye detection looks something like this:      Frontend, display detected face   The final step is to receive the message send by Jetty in our webapplication, and render it to an img element. We do this by setting the onmessage function on our websocket. In the following code, we receive the binary message. Convert this data to an objectURL (see this as a local, temporary URL), and set this value as the source of the image. Once the image is loaded, we revoke the objectURL since it is no longer needed.        ws.onmessage = function (msg) {         var target = document.getElementById(\"target\");         url=window.webkitURL.createObjectURL(msg.data);          target.onload = function() {             window.webkitURL.revokeObjectURL(url);         };         target.src = url;     }    We now only need to update our html to the following:    &lt;div style=\"visibility: hidden;  width:0; height:0;\"&gt;      &lt;canvas width=\"320\" id=\"canvas\" height=\"240\"&gt;&lt;/canvas&gt; &lt;/div&gt;  &lt;div&gt;     &lt;video id=\"live\" width=\"320\" height=\"240\" autoplay style=\"display: inline;\"&gt;&lt;/video&gt;     &lt;img id=\"target\" style=\"display: inline;\"/&gt; &lt;/div&gt;    And we’ve got working face recognition:      As you’ve seen we can do much with just the new HTML5 APIs. It’s too bad not all are finished and support over browsers is in some cases a bit lacking. But it does offer us nice and powerful features. I’ve tested this example on the latest version of chrome and on Safari (for Safari remove the webkit prefixes). It should however also work on the “userMedia” enabled mobile safari browser. Make sure though that you’re on a high bandwith WIFI, since this code isn’t optimized at all for bandwidth. I’ll revisit this article in a couple of weeks, when I have time to make a Play2/Scala based version of the backend.  ","categories": ["posts","javascript","jetty","html5"],
        "tags": [],
        "url": "http://www.smartjava.org/content/face-detection-using-html5-javascript-webrtc-websockets-jetty-and-opencv/",
        "teaser":null},{
        "title": "Binary websockets with Play 2.0 and Scala (and a bit op JavaCV/OpenCV)",
        "excerpt":"In a recent article I showed how you can use webrtc, canvas and websockets together to create a face detection application whose frontend runs completely in the browser, without the need for plugins. In that article I used a Jetty based backend to handle the image analysis using OpenCV through the JavaCV wrapper.  When I almost finished the article, I noticed that websockets is also supported from Play 2.0. I really like developping in Play and in Scala so as an experiment I rewrote the backend part from a Jetty/Java/JavaCV stack to a Play2.0/Scala/JavaCV stack. If you want to do this for yourself, make sure you start with the frontend code from here. Since the frontend code hasn’t changed except the location where the websockets are listening.   Setting up the Play 2.0 environment  I’m not going to talk too much about how to start a Play 2.0/Scala project. you can find the details in some of my other posts should you need more information. What we do need to do, is setup the dependencies for JavaCV so that they can be used from Play 2. I’ve manually added them to my local ivy repository, so that I can reference them from the sbt configuration like any other dependency. For my example I created the following directory layout for the JavaCV libraries:    ./play-2.0-RC2/repository/cache/javacv ./play-2.0-RC2/repository/cache/javacv/javacpp ./play-2.0-RC2/repository/cache/javacv/javacpp/ivy-2.3.1.xml ./play-2.0-RC2/repository/cache/javacv/javacpp/ivydata-2.3.1.properties ./play-2.0-RC2/repository/cache/javacv/javacv ./play-2.0-RC2/repository/cache/javacv/javacv/ivy-2.3.1.xml ./play-2.0-RC2/repository/cache/javacv/javacv/ivydata-2.3.1.properties ./play-2.0-RC2/repository/cache/javacv/javacv-macosx-x86_64 ./play-2.0-RC2/repository/cache/javacv/javacv-macosx-x86_64/ivy-2.3.1.xml ./play-2.0-RC2/repository/cache/javacv/javacv-macosx-x86_64/ivydata-2.3.1.properties ./play-2.0-RC2/repository/local/javacv ./play-2.0-RC2/repository/local/javacv/javacpp ./play-2.0-RC2/repository/local/javacv/javacpp/2.3.1 ./play-2.0-RC2/repository/local/javacv/javacpp/2.3.1/ivys ./play-2.0-RC2/repository/local/javacv/javacpp/2.3.1/ivys/ivy.xml ./play-2.0-RC2/repository/local/javacv/javacpp/2.3.1/jars ./play-2.0-RC2/repository/local/javacv/javacpp/2.3.1/jars/javacpp.jar ./play-2.0-RC2/repository/local/javacv/javacv ./play-2.0-RC2/repository/local/javacv/javacv/2.3.1 ./play-2.0-RC2/repository/local/javacv/javacv/2.3.1/ivys ./play-2.0-RC2/repository/local/javacv/javacv/2.3.1/ivys/ivy.xml ./play-2.0-RC2/repository/local/javacv/javacv/2.3.1/jars ./play-2.0-RC2/repository/local/javacv/javacv/2.3.1/jars/javacv.jar ./play-2.0-RC2/repository/local/javacv/javacv-macosx-x86_64 ./play-2.0-RC2/repository/local/javacv/javacv-macosx-x86_64/2.3.1 ./play-2.0-RC2/repository/local/javacv/javacv-macosx-x86_64/2.3.1/ivys ./play-2.0-RC2/repository/local/javacv/javacv-macosx-x86_64/2.3.1/ivys/ivy.xml ./play-2.0-RC2/repository/local/javacv/javacv-macosx-x86_64/2.3.1/jars ./play-2.0-RC2/repository/local/javacv/javacv-macosx-x86_64/2.3.1/jars/javacv-macosx-x86_64.jar    As you can see from this listing, I just added the three javacv supplied jars to my local repository. I also added a minimal ivy.xml so that they can be used from ivy and sbt. This minimal ivy.xml looks like this:    &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;ivy-module version=\"2.0\" xmlns:m=\"http://ant.apache.org/ivy/maven\"&gt;     &lt;info         organisation=\"javacv\"         module=\"javacv-macosx-x86_64\"         revision=\"2.3.1\"         status=\"release\"&gt;         &lt;/info&gt;                  &lt;publications&gt;             &lt;artifact type=\"jar\"/&gt;         &lt;/publications&gt; &lt;/ivy-module&gt;    With these files added to my repository I can setup the dependencies for the Play 2.0 project in the Build.scala file.    import sbt._ import Keys._ import PlayProject._  object ApplicationBuild extends Build {      val appName         = \"PlayWebsocketJavaCV\"     val appVersion      = \"1.0-SNAPSHOT\"      val appDependencies = Seq(       \"javacv\" % \"javacv\" % \"2.3.1\",       \"javacv\" % \"javacpp\" % \"2.3.1\",       \"javacv\" % \"javacv-macosx-x86_64\" % \"2.3.1\"     )      val main = PlayProject(appName, appVersion, appDependencies, mainLang = SCALA).settings(       // Add your own project settings here           ) }    Now run “play update” and “play eclipsify” to update the dependencies and your Eclipse configuation (if you’re using Eclipse that is).   Configure websockets in Play   Using websockets in Play 2.0 is very easy. The first thing you need to do is add the URL to your routes configuration in the conf directory.    GET     /wsrequest                  controllers.Application.wsrequest    And, of course, you need to implement the action this route points to:    /**  * Simple websocket listener configured in play 2. This uses a synchronous model, where  * the same channel is used to send the response. For this usecase this is useful, if  * we want async processing we could have used Akka actors together with play 2.0 async  * support.  */ def wsrequest = WebSocket.using[Array[Byte]] { request =&gt;      // Create the outbound value that is called for each   val out = Enumerator.imperative[Array[Byte]]();   val in = Iteratee.foreach[Array[Byte]](content =&gt; { \t   out.push(FaceDetect.detect(content));   })    // tie the in and out values to each other   (in, out) }    In this code we configure  an input channel (in), and an output channel (out) and connect them to the socket. Whenever the HTML5 client sends a request over the websocket our “in” method is called, and when we want to send something to the client we can use the “out” channel. The “in” channel needs to be defined as an Iteratee (more info see these Play docs). What this does is, that for each input message we receive we run the specifici method. In this case we run the FaceDetect.detect operation (more on this later) and the result from this operation is pushed back to the client using the “out” channel. This “out” channel itself is defined as an Enumerator (see these play docs). We can attach different listeners if we want to this enumerator, but in this case we don’t do anything with the message, just pass it along to the client.   Using JavaCV from scala   The last step is the code of the FaceDetect.detect function. The java version, see earlier mentioned article, is very easily converted to a scala one.    package javacv  import com.googlecode.javacv.cpp.opencv_core._ import com.googlecode.javacv.cpp.opencv_imgproc._ import com.googlecode.javacv.cpp.opencv_highgui._ import com.googlecode.javacv.cpp.opencv_objdetect._ import com.googlecode.javacpp.BytePointer import java.nio.ByteBuffer import javax.imageio.ImageIO import java.io.ByteArrayOutputStream import scala.tools.nsc.io.VirtualFile  object FaceDetect {    \tvar CASCADE_FILE =PATH_TO_CASCADE_FILE;   \tvar minsize = 20; \tvar group = 0; \tvar scale = 1.1;      def detect(imageData:Array[Byte]) : Array[Byte] = {          // we need to wrap the input array, since BytePointer doesn't accept     // a bytearray as input. It accepts a byte varargs, but Array[Byte]     // doesn't convert automatically     var wrappedData = ByteBuffer.wrap(imageData);     var originalImage = cvDecodeImage(cvMat(1, imageData.length,CV_8UC1, new BytePointer(wrappedData)));          // convert to grayscale for easy detection     var grayImage = IplImage.create(originalImage.width(), originalImage.height(), IPL_DEPTH_8U, 1);     cvCvtColor(originalImage, grayImage, CV_BGR2GRAY);          // storage is needed to store information during detection \tvar storage = CvMemStorage.create(); \t \t// load and run the cascade \tvar cascade = new CvHaarClassifierCascade(cvLoad(CASCADE_FILE)); \tvar faces = cvHaarDetectObjects(grayImage, cascade, storage, scale, group, minsize); \t \t// draw a rectangle for the detected faces \tfor (i &lt;- 0 until faces.total) { \t  var r = new CvRect(cvGetSeqElem(faces, i)); \t  cvRectangle(originalImage, cvPoint(r.x, r.y), \t\t\t\t\tcvPoint(r.x + r.width(), r.y + r.height), \t\t\t\t\tCvScalar.YELLOW, 1, CV_AA, 0); \t} \t \t// convert to bytearray and return \tvar bout = new ByteArrayOutputStream(); \tvar imgb = originalImage.getBufferedImage(); \tImageIO.write(imgb, \"png\", bout); \t     bout.toByteArray()   } }    The only issue I ran into was with the BytePointer constructor. One of the signatures accepts a varargs of the type byte. In java this allows me to just supply this constructor with a byte[], in Scala however this doesn’t work. Luckily though, a BytePointer can also be created using a ByteBuffer. For the rest this is a one-to-one conversion of Java to Scala.   Running the code   And that’s almost it. By default play listens on port 9000, in the Jetty based example we had the server running on 9999 and listening to the root context. To work with our Play2/Scala based server we just need to point the browser to the correct websocket server url.     ws = new WebSocket(\"ws://127.0.0.1:9000/wsrequest\");    And now, when we run it, we use Play 2 as our server and run the JavaCV code using scal. And more importantly it still works:      ","categories": ["posts","html5","play","scala"],
        "tags": [],
        "url": "http://www.smartjava.org/content/binary-websockets-play-20-and-scala-and-bit-op-javacvopencv/",
        "teaser":null},{
        "title": "Using SPDY and HTTP transparently using Netty",
        "excerpt":"Most people have already heard about SPDY, the protocol, from google, proposed as a replacement for the aging HTTP protocol. Webservers are browsers are slowly implementing this protocol and support is growing. In a recent article I already wrote about how SPDY works and how you can enable SPDY support in Jetty. Since a couple of months Netty (originally from JBoss) also has support for SPDY. Since Netty is often used for high performant protocol servers, SPDY is a logical fit. In this article I’ll show you how you can create a basic Netty based server that does protocol negotiation between SPDY and HTTP. It used the example HTTPRequestHandler from the Netty snoop example to consume and produce some HTTP content.   To get everything working we’ll need to do the following things:      Enable NPN in Java to determine protocol to use.   Determine, based on the negotiated protocol, whether to use HTTP or SPDY.   Make sure the correct SPDY headers are sent back with HTTP.   SPDY uses an TLS extension to determine the protocol to use in communication. This is called NPN. I wrote a more complete explanation and shown the messages involved in the article on how to use SPDY on Jetty, so for more info look at that article. Basically what this extension does is that during the TLS exchange a server and client also exchange the transport level protocols they support. In the case of SPDY a server could support both the SPDY protocol and the HTTP protocol. A client implementation can then determine which protocol to use.  Since this isn’t something which is available in the standard Java implementation, we need to extend the Java TLS functionality with NPN.   Enable NPN support in Java   So far I found two options that can be used to add NPN support in Java. One is from https://github.com/benmmurphy/ssl_npn who also has a basic SPDY/Netty example in his repo where he uses his own implementation. The other option, and the one I’ll be using, is the NPN support provided by Jetty. Jetty provides an easy to use API that you can use to add NPN support to your Java SSL contexts. Once again, in the in the article on Jetty you can find more info on this. To set up NPN for Netty, we need to do the following:      Add NPN lib to bootpath  Connect the SSL context to the NPN Api   Add NPN lib to boothpath   First things first. Download the NPN boot jar from http://repo2.maven.org/maven2/org/mortbay/jetty/npn/npn-boot/8.1.2.v20120308/ and make sure that when you run the server you start it like this:    java -Xbootclasspath/p:&lt;path_to_npn_boot_jar&gt;     With this piece of code, Java SSL has support for NPN. We still, however, need access to the results from this negotiation. We need to know whether we’re using HTTP or SPDY, since that determines how we process the received data. For this Jetty provides an API. For this and for the required Netty libraries, we add the following dependencies, since I’m using maven, to the pom.    \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;io.netty&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;netty&lt;/artifactId&gt; \t\t\t&lt;version&gt;3.4.1.Final&lt;/version&gt; \t\t&lt;/dependency&gt; \t\t \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.eclipse.jetty.npn&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;npn-api&lt;/artifactId&gt; \t\t\t&lt;version&gt;8.1.2.v20120308&lt;/version&gt; \t\t&lt;/dependency&gt;    Connect the SSL context to the NPN API   Now that we’ve got NPN enabled and the correct API added to the project, we can configure the Netty SSL handler. Configuring handlers in Netty is done in a PipelineFactory. For our server I created the following PipelineFactory:   package smartjava.netty.spdy;  import static org.jboss.netty.channel.Channels.pipeline;  import java.io.FileInputStream; import java.security.KeyStore;  import javax.net.ssl.KeyManagerFactory; import javax.net.ssl.SSLContext; import javax.net.ssl.SSLEngine;  import org.eclipse.jetty.npn.NextProtoNego; import org.jboss.netty.channel.ChannelPipeline; import org.jboss.netty.channel.ChannelPipelineFactory; import org.jboss.netty.handler.ssl.SslHandler;  public class SPDYPipelineFactory implements ChannelPipelineFactory {  \tprivate SSLContext context;  \tpublic SPDYPipelineFactory() { \t\ttry { \t\t\tKeyStore keystore = KeyStore.getInstance(\"JKS\"); \t\t\tkeystore.load(new FileInputStream(\"src/main/resources/server.jks\"), \t\t\t\t\t\"secret\".toCharArray());  \t\t\tKeyManagerFactory kmf = KeyManagerFactory.getInstance(\"SunX509\"); \t\t\tkmf.init(keystore, \"secret\".toCharArray());  \t\t\tcontext = SSLContext.getInstance(\"TLS\"); \t\t\tcontext.init(kmf.getKeyManagers(), null, null);  \t\t} catch (Exception e) { \t\t\te.printStackTrace(); \t\t} \t}  \tpublic ChannelPipeline getPipeline() throws Exception { \t\t// Create a default pipeline implementation. \t\tChannelPipeline pipeline = pipeline();  \t\t// Uncomment the following line if you want HTTPS \t\tSSLEngine engine = context.createSSLEngine(); \t\tengine.setUseClientMode(false);  \t\tNextProtoNego.put(engine, new SimpleServerProvider()); \t\tNextProtoNego.debug = true;  \t\tpipeline.addLast(\"ssl\", new SslHandler(engine)); \t\tpipeline.addLast(\"pipeLineSelector\", new HttpOrSpdyHandler());  \t\treturn pipeline; \t} }   In the constructor from this class we setup a basic SSL context. The keystore and key we use I created using the java keytool, this is normal SSL configuration. When we receive a request, the getPipeline operation is called to determine how to handle the request. Here we use the NextProtoNego class, provided by Jetty-NPN-API, to connect our SSL connection to the NPN implementation. In this operation we pass a provider that is used as callback and configuration for our server. We also set NextProtoNego.debug to true. This prints out some debugging information that makes, well, debugging easier. The code for the SimpleServerProvider is very simple:   public class SimpleServerProvider implements ServerProvider { \t \tprivate String selectedProtocol = null;  \tpublic void unsupported() { \t\t//if unsupported, default to http/1.1 \t\tselectedProtocol = \"http/1.1\"; \t}  \tpublic List&lt;String&gt; protocols() { \t\t   return Arrays.asList(\"spdy/2\",\"http/1.1\"); \t}  \tpublic void protocolSelected(String protocol) { \t\tselectedProtocol = protocol; \t} \t \tpublic String getSelectedProtocol() {\t \t\treturn selectedProtocol; \t} }   This code is pretty much self-explanatory.     The unsupported operation is called when the client doesn't support NPN. In that case we default to HTTP.    The protocols() operation returns the protocols the server supports   The protocolSelected operation is called when a protocol has been negotiated by the server and the client   The getSelectedProtocol is a method we will use to get the selected protocol from a different handler in the Netty pipeline.   Determine, based on the negotiated protocol, whether to use HTTP or SPDY   Now we need to configure Netty in such a way that it runs a specific pipeline for HTTPS request and a pipeline for SPDY requests. For this let’s look back at a small part of the pipelinefactory.    \t\tpipeline.addLast(\"ssl\", new SslHandler(engine)); \t\tpipeline.addLast(\"pipeLineSelector\", new HttpOrSpdyHandler());   The first part of this pipeline is the SslHandler that is configured with NPN support. The next handler that will be called is the HttpOrSpdyHandler. This handler determines, based on the protocol, which pipeline to use. The code for this handler is listed next:   public class HttpOrSpdyHandler implements  ChannelUpstreamHandler { \t \tpublic void handleUpstream(ChannelHandlerContext ctx, ChannelEvent e) \t\t\tthrows Exception { \t \t\t// determine protocol type \t\tSslHandler handler = ctx.getPipeline().get(SslHandler.class); \t\tSimpleServerProvider provider = (SimpleServerProvider) NextProtoNego.get(handler.getEngine()); \t\t \t\tif (\"spdy/2\".equals(provider.getSelectedProtocol())) { \t\t\tChannelPipeline pipeline = ctx.getPipeline(); \t\t\t \t        pipeline.addLast(\"decoder\", new SpdyFrameDecoder()); \t        pipeline.addLast(\"spdy_encoder\", new SpdyFrameEncoder()); \t        pipeline.addLast(\"spdy_session_handler\", new SpdySessionHandler(true)); \t        pipeline.addLast(\"spdy_http_encoder\", new SpdyHttpEncoder());                 // Max size of SPDY messages set to 1MB \t        pipeline.addLast(\"spdy_http_decoder\", new SpdyHttpDecoder(1024*1024));  \t        pipeline.addLast(\"handler\", new HttpRequestHandler()); \t         \t        // remove this handler, and process the requests as spdy \t        pipeline.remove(this); \t\t\tctx.sendUpstream(e); \t\t}  else if (\"http/1.1\".equals(provider.getSelectedProtocol())) { \t\t\tChannelPipeline pipeline = ctx.getPipeline(); \t\t\tpipeline.addLast(\"decoder\", new HttpRequestDecoder()); \t\t\tpipeline.addLast(\"http_encoder\", new HttpResponseEncoder()); \t\t\tpipeline.addLast(\"handler\", new HttpRequestHandler()); \t\t\t \t\t\t// remove this handler, and process the requests as http \t\t\tpipeline.remove(this); \t\t\tctx.sendUpstream(e); \t\t} else { \t\t\t// we're still in protocol negotiation, no need for any handlers \t\t\t// at this point. \t\t} \t} }   Using the NPN API and our current SSL context, we retrieve the SimpleServerProvider we added earlier. We check whether the selectedProtocol has been set, and if so, we setup a chain for processing. We handle three options in this class:      There is no protocol: It's possible that no protocol has been negotiated yet. In that case we don't do anything special, and just process it normally.   There is a http protocol: We set up a handler chain to handle HTTP requests.   There is a spdy protocol: We set up a handler chain to handle SPDY requests.   With this chain all the messages we receive eventually by the HttpRequestHandler are HTTP Requests. We can process this HTTP request normally, and return a HTTP response. The various pipeline configurations will handle all this correctly.   Make sure the correct SPDY headers are sent back with HTTP   The final step we need to do, is this test. We’ll test this with the latest version of Chrome to test whether SPDY is working, and we’ll use wget to test the normal http requests. I mentioned that the HttpRequestHandler, the last handler in the chain, does our HTTP processing. I’ve used the http://netty.io/docs/stable/xref/org/jboss/netty/example/http/snoop/HttpSnoopServerHandler.html as the HTTPRequestHandler since that one nicely returns information about the HTTP request, without me having to do anything. If you run this without alteration, you do run into an issue. To correlate the HTTP response to the correct SPDY session, we need to copy a header from the incoming request to the response: the “X-SPDY-Stream-ID” header. I’ve added the following to the HttpSnoopServerHandler to make sure these headers are copied (should really have done this in a seperate handler).   \tprivate final static String SPDY_STREAM_ID = \"X-SPDY-Stream-ID\"; \tprivate final static String SPDY_STREAM_PRIO = \"X-SPDY-Stream-Priority\";          // in the writeResponse method add          if (request.containsHeader(SPDY_STREAM_ID)) {             response.addHeader(SPDY_STREAM_ID,request.getHeader(SPDY_STREAM_ID));             // optional header for prio             response.addHeader(SPDY_STREAM_PRIO,0);         }   Now all that is left is a server with a main to start everything, and we can test our SPDY implementation.   public class SPDYServer {  \tpublic static void main(String[] args) { \t\t// bootstrap is used to configure and setup the server \t\tServerBootstrap bootstrap = new ServerBootstrap( \t\t\t\tnew NioServerSocketChannelFactory( \t\t\t\t\t\tExecutors.newCachedThreadPool(), \t\t\t\t\t\tExecutors.newCachedThreadPool())); \t\t \t\tbootstrap.setPipelineFactory(new SPDYPipelineFactory()); \t\tbootstrap.bind(new InetSocketAddress(8443)); \t} }   Start up the server, fire up Chrome and let’s see whether everything is working. Open the https://localhost:8443/thisIsATest url and you should get a result that looks something like this:      In the output of the server, you can see some NPN debug logging:    [S] NPN received for 68ce4f39[SSLEngine[hostname=null port=-1] SSL_NULL_WITH_NULL_NULL] [S] NPN protocols [spdy/2, http/1.1] sent to client for 68ce4f39[SSLEngine[hostname=null port=-1] SSL_NULL_WITH_NULL_NULL] [S] NPN received for 4b24e48f[SSLEngine[hostname=null port=-1] SSL_NULL_WITH_NULL_NULL] [S] NPN protocols [spdy/2, http/1.1] sent to client for 4b24e48f[SSLEngine[hostname=null port=-1] SSL_NULL_WITH_NULL_NULL] [S] NPN selected 'spdy/2' for 4b24e48f[SSLEngine[hostname=null port=-1] SSL_NULL_WITH_NULL_NULL]    An extra check is looking at the open SPDY sessions in chrome browser by using the following url: chrome://net-internals/#spdy      Now lets check whether plain old HTTP is still working. From a command line do the following:    jos@Joss-MacBook-Pro.local:~$ wget --no-check-certificate https://localhost:8443/thisIsATest --2012-04-27 16:29:09--  https://localhost:8443/thisIsATest Resolving localhost... ::1, 127.0.0.1, fe80::1 Connecting to localhost|::1|:8443... connected. WARNING: cannot verify localhost's certificate, issued by `/C=NL/ST=NB/L=Waalwijk/O=smartjava/OU=smartjava/CN=localhost':   Self-signed certificate encountered. HTTP request sent, awaiting response... 200 OK Length: 285 [text/plain] Saving to: `thisIsATest'  100%[==================================================================================&gt;] 285         --.-K/s   in 0s        2012-04-27 16:29:09 (136 MB/s) - `thisIsATest' saved [285/285]  jos@Joss-MacBook-Pro.local:~$ cat thisIsATest  WELCOME TO THE WILD WILD WEB SERVER =================================== VERSION: HTTP/1.1 HOSTNAME: localhost:8443 REQUEST_URI: /thisIsATest  HEADER: User-Agent = Wget/1.13.4 (darwin11.2.0) HEADER: Accept = */* HEADER: Host = localhost:8443 HEADER: Connection = Keep-Alive  jos@Joss-MacBook-Pro.local:~$     And it works! Wget uses standard HTTPS, and we get a result, and chrome uses SPDY and presents the result from the same handler.  In the net couple of days, I’ll also post on article on how you can enable SPDY for the Play Framework 2.0, since their webserver is also based on Netty.  ","categories": ["posts","java","jetty","spdy"],
        "tags": [],
        "url": "http://www.smartjava.org/content/using-spdy-and-http-transparently-using-netty/",
        "teaser":null},{
        "title": "Access WSO2 Registry programatically",
        "excerpt":"When you have many services it is a good thing to register them somewhere so your consumers can easily find them, see what they are about, determine whether the service levels are good enough etc. When you look back at the heyday of SOA everyone was pusing registries (or repositories, but that terminology is a different subject). To fully do SOA you needed a UDDI registry to accompany your SOAP based services, defined by a WSDL and XML Schemas.   But, as with so many interesting ideas, UDDI never really took off. There are still a couple of UDDI repositories out there, but I’ve never seen this used in practice. SOAP and the whole set of WS-* standards however are still much in use in organizations. For public facing APIs on the other hand people most often choose a more RESTful approach using the basic HTTP verbs and JSON.   Open source registries   Regardless of whether your creating an internal SOAP/WS-* based service or a public RESTful API using self-describing resources, it’s very useful to have a central registry that you can use to store meta information on your services: lifecycle, SLA, documentation, policies etc. When you start looking for an open source registry, your options are rather slim. Basically there is, at the moment, in my opinion only one good open source registry: WSO2 Governance Registry.      (full disclosure: I use the WSO2 Governance Registry in my book, so I’m just a little bit biased, but if you know of any other mature open source registry please let me know)   This registry has a nice web frontend that you can use to regsiter resources, search for services and much more. Good to use for your end users or for operations during the runtime phase of your application and services. It would, however, also be nice to access the registry programatically. That way you could provision URLs, documentation, WSDLs and for instance schemas directly from a central location.   Access WSO2 programatically   Luckily the WSO2 guys provide us with a simple REST API and a java based client library you can use to access the repository directly. If you just need access to a resource you can use the REST interface using HTTP basic authentication, for more advanced functionality you should probably use the java based client.   Access using java client   If you want to get all the depencies using Maven for using the java client, you download pretty much the complete WSO2 Registry, and if your working with a fairly new version, you might also run into non-existing or corrupt dependencies. Luckily though, since Maven (unofficially) support wildcard exclusions (see my article on this here). I use the following pom.xml that explicitely defines the minimum set of dependencies to access the WSO2 Registry programatically.    &lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" \txmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \txsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; \t&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; \t&lt;groupId&gt;wsclient&lt;/groupId&gt; \t&lt;artifactId&gt;wsclient&lt;/artifactId&gt; \t&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; \t&lt;repositories&gt; \t\t&lt;repository&gt; \t\t\t&lt;id&gt;maven2&lt;/id&gt; \t\t\t&lt;url&gt;http://repo1.maven.org/maven2/&lt;/url&gt; \t\t&lt;/repository&gt; \t\t&lt;repository&gt; \t\t\t&lt;id&gt;wso2-maven2-repository&lt;/id&gt; \t\t\t&lt;url&gt;http://dist.wso2.org/maven2&lt;/url&gt; \t\t&lt;/repository&gt;  \t&lt;/repositories&gt; \t&lt;dependencies&gt;  \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.apache.ws.commons.axiom&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;axiom-impl&lt;/artifactId&gt; \t\t\t&lt;version&gt;1.2.12&lt;/version&gt; \t\t&lt;/dependency&gt; \t\t \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.apache.ws.commons.axiom&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;axiom-api&lt;/artifactId&gt; \t\t\t&lt;version&gt;1.2.12&lt;/version&gt; \t\t&lt;/dependency&gt;\t\t  \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.wso2.carbon&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;org.wso2.carbon.registry.ws.client&lt;/artifactId&gt; \t\t\t&lt;version&gt;3.2.0&lt;/version&gt; \t\t\t&lt;scope&gt;provided&lt;/scope&gt; \t\t\t&lt;exclusions&gt; \t\t\t\t&lt;exclusion&gt; \t\t\t\t\t&lt;!-- excluded since the version in the repo is incorrect --&gt; \t\t\t\t\t&lt;groupId&gt;*&lt;/groupId&gt; \t\t\t\t\t&lt;artifactId&gt;*&lt;/artifactId&gt; \t\t\t\t&lt;/exclusion&gt; \t\t\t&lt;/exclusions&gt; \t\t&lt;/dependency&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.wso2.carbon&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;org.wso2.carbon.core.common&lt;/artifactId&gt; \t\t\t&lt;version&gt;3.2.0&lt;/version&gt; \t\t\t&lt;scope&gt;provided&lt;/scope&gt; \t\t\t&lt;exclusions&gt; \t\t\t\t&lt;exclusion&gt; \t\t\t\t\t&lt;!-- excluded since the version in the repo is incorrect --&gt; \t\t\t\t\t&lt;groupId&gt;*&lt;/groupId&gt; \t\t\t\t\t&lt;artifactId&gt;*&lt;/artifactId&gt; \t\t\t\t&lt;/exclusion&gt; \t\t\t&lt;/exclusions&gt; \t\t&lt;/dependency&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.wso2.carbon&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;org.wso2.carbon.utils&lt;/artifactId&gt; \t\t\t&lt;version&gt;3.2.0&lt;/version&gt; \t\t\t&lt;scope&gt;provided&lt;/scope&gt; \t\t\t&lt;exclusions&gt; \t\t\t\t&lt;exclusion&gt; \t\t\t\t\t&lt;!-- excluded since the version in the repo is incorrect --&gt; \t\t\t\t\t&lt;groupId&gt;*&lt;/groupId&gt; \t\t\t\t\t&lt;artifactId&gt;*&lt;/artifactId&gt; \t\t\t\t&lt;/exclusion&gt; \t\t\t&lt;/exclusions&gt; \t\t&lt;/dependency&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.wso2.carbon&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;org.wso2.carbon.base&lt;/artifactId&gt; \t\t\t&lt;version&gt;3.2.0&lt;/version&gt; \t\t\t&lt;scope&gt;provided&lt;/scope&gt; \t\t\t&lt;exclusions&gt; \t\t\t\t&lt;exclusion&gt; \t\t\t\t\t&lt;!-- excluded since the version in the repo is incorrect --&gt; \t\t\t\t\t&lt;groupId&gt;*&lt;/groupId&gt; \t\t\t\t\t&lt;artifactId&gt;*&lt;/artifactId&gt; \t\t\t\t&lt;/exclusion&gt; \t\t\t&lt;/exclusions&gt; \t\t&lt;/dependency&gt;\t\t\t\t \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.wso2.carbon&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;org.wso2.carbon.authenticator.stub&lt;/artifactId&gt; \t\t\t&lt;version&gt;3.2.0&lt;/version&gt; \t\t\t&lt;scope&gt;provided&lt;/scope&gt; \t\t\t&lt;exclusions&gt; \t\t\t\t&lt;exclusion&gt; \t\t\t\t\t&lt;!-- excluded since the version in the repo is incorrect --&gt; \t\t\t\t\t&lt;groupId&gt;*&lt;/groupId&gt; \t\t\t\t\t&lt;artifactId&gt;*&lt;/artifactId&gt; \t\t\t\t&lt;/exclusion&gt; \t\t\t&lt;/exclusions&gt; \t\t&lt;/dependency&gt;\t\t\t\t\t \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.wso2.carbon&lt;/groupId&gt; \t\t\t&lt;artifactId&gt; org.wso2.carbon.registry.resource.ui&lt;/artifactId&gt; \t\t\t&lt;version&gt;3.2.0&lt;/version&gt; \t\t\t&lt;scope&gt;provided&lt;/scope&gt; \t\t\t&lt;exclusions&gt; \t\t\t\t&lt;exclusion&gt; \t\t\t\t\t&lt;!-- excluded since the version in the repo is incorrect --&gt; \t\t\t\t\t&lt;groupId&gt;*&lt;/groupId&gt; \t\t\t\t\t&lt;artifactId&gt;*&lt;/artifactId&gt; \t\t\t\t&lt;/exclusion&gt; \t\t\t&lt;/exclusions&gt; \t\t&lt;/dependency&gt;  \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.wso2.carbon&lt;/groupId&gt; \t\t\t&lt;version&gt;3.2.0&lt;/version&gt; \t\t\t&lt;artifactId&gt;org.wso2.carbon.registry.core&lt;/artifactId&gt; \t\t\t&lt;scope&gt;provided&lt;/scope&gt; \t\t\t&lt;exclusions&gt; \t\t\t\t&lt;exclusion&gt; \t\t\t\t\t&lt;!-- excluded since the version in the repo is incorrect --&gt; \t\t\t\t\t&lt;groupId&gt;*&lt;/groupId&gt; \t\t\t\t\t&lt;artifactId&gt;*&lt;/artifactId&gt; \t\t\t\t&lt;/exclusion&gt; \t\t\t&lt;/exclusions&gt; \t\t&lt;/dependency&gt;  \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.wso2.carbon&lt;/groupId&gt; \t\t\t&lt;version&gt;3.2.0&lt;/version&gt; \t\t\t&lt;artifactId&gt;org.wso2.carbon.registry.api&lt;/artifactId&gt; \t\t\t&lt;scope&gt;provided&lt;/scope&gt; \t\t\t&lt;exclusions&gt; \t\t\t\t&lt;exclusion&gt; \t\t\t\t\t&lt;!-- excluded since the version in the repo is incorrect --&gt; \t\t\t\t\t&lt;groupId&gt;*&lt;/groupId&gt; \t\t\t\t\t&lt;artifactId&gt;*&lt;/artifactId&gt; \t\t\t\t&lt;/exclusion&gt; \t\t\t&lt;/exclusions&gt; \t\t&lt;/dependency&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;commons-logging&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;commons-logging&lt;/artifactId&gt; \t\t\t&lt;version&gt;1.1.1&lt;/version&gt; \t\t\t&lt;type&gt;jar&lt;/type&gt; \t\t\t&lt;scope&gt;provided&lt;/scope&gt; \t\t&lt;/dependency&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.apache.axis2&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;axis2&lt;/artifactId&gt; \t\t\t&lt;version&gt;1.6.1&lt;/version&gt; \t\t\t&lt;type&gt;pom&lt;/type&gt; \t\t\t&lt;scope&gt;compile&lt;/scope&gt; \t\t&lt;/dependency&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.apache.axis2&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;axis2-kernel&lt;/artifactId&gt; \t\t\t&lt;version&gt;1.6.1&lt;/version&gt; \t\t\t&lt;type&gt;jar&lt;/type&gt; \t\t\t&lt;scope&gt;compile&lt;/scope&gt; \t\t&lt;/dependency&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.apache.axis2&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;axis2-transport-local&lt;/artifactId&gt; \t\t\t&lt;version&gt;1.6.1&lt;/version&gt; \t\t\t&lt;scope&gt;compile&lt;/scope&gt; \t\t&lt;/dependency&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.apache.axis2&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;axis2-transport-tcp&lt;/artifactId&gt; \t\t\t&lt;version&gt;1.0.0&lt;/version&gt; \t\t\t&lt;scope&gt;compile&lt;/scope&gt; \t\t&lt;/dependency&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.apache.axis2&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;axis2-transport-http&lt;/artifactId&gt; \t\t\t&lt;version&gt;1.6.1&lt;/version&gt; \t\t\t&lt;scope&gt;compile&lt;/scope&gt; \t\t&lt;/dependency&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.apache.axis2&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;axis2-json&lt;/artifactId&gt; \t\t\t&lt;version&gt;1.6.1&lt;/version&gt; \t\t\t&lt;type&gt;jar&lt;/type&gt; \t\t\t&lt;scope&gt;compile&lt;/scope&gt; \t\t&lt;/dependency&gt; \t\t&lt;dependency&gt; \t\t\t&lt;groupId&gt;org.apache.axis2&lt;/groupId&gt; \t\t\t&lt;artifactId&gt;axis2-adb&lt;/artifactId&gt; \t\t\t&lt;version&gt;1.6.1&lt;/version&gt; \t\t\t&lt;type&gt;jar&lt;/type&gt; \t\t\t&lt;scope&gt;compile&lt;/scope&gt; \t\t&lt;/dependency&gt; \t&lt;/dependencies&gt; &lt;/project&gt;    With this pom it is now very easy to access the repository. The default examples reference information in a locally installed WSO2 repository. This isn’t really necessary. You need to set up the correct keystores, and the correct Axis2 configuration, but that’s it. You don’t need a locally installed repository for the client. The following code shows how you can do this:   package soa.governance.chapter2.registry;  import java.util.ArrayList; import java.util.List; import java.util.Properties;  import org.apache.axis2.context.ConfigurationContext; import org.apache.axis2.context.ConfigurationContextFactory; import org.wso2.carbon.registry.core.Association; import org.wso2.carbon.registry.core.Collection; import org.wso2.carbon.registry.core.Registry; import org.wso2.carbon.registry.core.Resource; import org.wso2.carbon.registry.core.exceptions.RegistryException;  import org.wso2.carbon.registry.ws.client.registry.WSRegistryServiceClient;  /**  * Simple client that can be used to connect to the repository.  *   * @author jos  */ public class RegistryClient {  \t// url where the repository is running its services interface \tprivate static String backendURL = \"http://localhost:9763/services/\";     private static ConfigurationContext configContext = null; \t     // configuration locations used to bootstrap axis2     private static String axis2Repo = \"src/main/resources/client\";     private static String axis2Conf = \"src/main/resources/conf/axis2_client.xml\";     private static String serverURL = \"https://localhost:9443/services/\"; \t     /**      * Initialize the repository client      *       * @return an initialized and authenticated client      * @throws Exception      */     private static WSRegistryServiceClient initialize() throws Exception {     \t// set these properties, this is used for authentication over https to the registry     \t// if you have a newer version, you can update the keystore by copying it from     \t// the security directory of the repository         System.setProperty(\"javax.net.ssl.trustStore\", \"src/main/resources/security/wso2carbon.jks\");         System.setProperty(\"javax.net.ssl.trustStorePassword\", \"wso2carbon\");         System.setProperty(\"javax.net.ssl.trustStoreType\",\"JKS\");                  configContext = ConfigurationContextFactory.createConfigurationContextFromFileSystem(axis2Repo, axis2Conf);         return new WSRegistryServiceClient(serverURL, \"admin\", \"admin\", backendURL, configContext);     }          /**      * Simple main method that uses the repository to execute a certain operation on the repository      *       * @param args      * @throws Exception      */ \tpublic static void main(String[] args) throws Exception { \t\tRegistry registry = initialize(); \t\t \t    // get the governance folder \t    Resource governanceFolder = registry.get(\"/_system/governance\"); \t    System.out.println(\"Folder description: \" +                          \t                         governanceFolder.getDescription());             \t     \t    // get the WSDL folder resource (use the url we browsed to)    \t    String wsdlUrl = \"/_system/governance/trunk/wsdls/_0/service_1\" + \t    \t\t\"/account/wsdl/trafficavoidance/accountService.wsdl\"; \t    Resource wsdlResource = registry.get(wsdlUrl); \t     \t    // output the content of the wsdl \t    System.out.println(new String((byte[])wsdlResource           \t                                          .getContent())); \t     \t    List&lt;Resource&gt; paths = getServicePath(registry, \"/_system/governance/trunk/services\"); \t        \t    for (Resource service : paths) { \t\t\t// we've got all the services here \t    \t \t    \tProperties props = service.getProperties(); \t\t    for (Object prop : props.keySet()) { \t\t\t\tSystem.out.println(prop + \" - \" + props.get(prop)); \t\t\t} \t\t     \t\t    Association[] associations = registry.getAssociations(service.getPath(), \"Documentation\"); \t\t    for (Association association : associations) { \t\t\t\tSystem.out.println(association.getAssociationType()); \t\t\t} \t\t} \t} \t \tprivate static List&lt;Resource&gt; getServicePath(Registry registry, String servicesResource) throws RegistryException { \t\tList&lt;Resource&gt; result = new ArrayList&lt;Resource&gt;(); \t\tResource resource = registry.get(servicesResource); \t\t \t\tif (resource instanceof Collection) { \t\t\tObject content = resource.getContent(); \t\t\tfor (Object path : (Object[])content) { \t\t\t\tresult.addAll(getServicePath(registry,(String)path)); \t\t\t} \t\t} else if (resource instanceof Resource){ \t\t\tresult.add(resource); \t\t}  \t\treturn result; \t} }   As you can see we reference an axis2 configuration file here (axis2_client.xml). This is the standard axis2_client.xml provided by wso2. For completeness sake the content of this file is listed here:    &lt;!--   ~ Copyright 2005-2007 WSO2, Inc. (http://wso2.com)   ~   ~ Licensed under the Apache License, Version 2.0 (the \"License\");   ~ you may not use this file except in compliance with the License.   ~ You may obtain a copy of the License at   ~   ~ http://www.apache.org/licenses/LICENSE-2.0   ~   ~ Unless required by applicable law or agreed to in writing, software   ~ distributed under the License is distributed on an \"AS IS\" BASIS,   ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.   ~ See the License for the specific language governing permissions and   ~ limitations under the License.   --&gt;  &lt;axisconfig name=\"AxisJava2.0\"&gt;     &lt;!-- ================================================= --&gt;     &lt;!-- Parameters --&gt;     &lt;!-- ================================================= --&gt;     &lt;parameter name=\"hotdeployment\"&gt;true&lt;/parameter&gt;     &lt;parameter name=\"hotupdate\"&gt;false&lt;/parameter&gt;     &lt;parameter name=\"enableMTOM\"&gt;false&lt;/parameter&gt;      &lt;!-- commons-http-client defaultMaxConnPerHost --&gt;     &lt;parameter name=\"defaultMaxConnPerHost\"&gt;500&lt;/parameter&gt;     &lt;!-- commons-http-client maxTotalConnections --&gt;     &lt;parameter name=\"maxTotalConnections\"&gt;15000&lt;/parameter&gt;      &lt;!--If turned on with use the Accept header of the request to determine the contentType of the     response--&gt;     &lt;parameter name=\"httpContentNegotiation\"&gt;false&lt;/parameter&gt;      &lt;!--During a fault, stacktrace can be sent with the fault message. The following flag will control --&gt;     &lt;!--that behaviour.--&gt;     &lt;parameter name=\"sendStacktraceDetailsWithFaults\"&gt;true&lt;/parameter&gt;      &lt;!--If there aren't any information available to find out the fault reason, we set the message of the exception--&gt;     &lt;!--as the faultreason/Reason. But when a fault is thrown from a service or some where, it will be --&gt;     &lt;!--wrapped by different levels. Due to this the initial exception message can be lost. If this flag--&gt;     &lt;!--is set then, Axis2 tries to get the first exception and set its message as the faultreason/Reason.--&gt;     &lt;parameter name=\"DrillDownToRootCauseForFaultReason\"&gt;false&lt;/parameter&gt;      &lt;!--This is the user name and password of admin console--&gt;     &lt;parameter name=\"userName\"&gt;admin&lt;/parameter&gt;     &lt;parameter name=\"password\"&gt;axis2&lt;/parameter&gt;      &lt;!--To override repository/services you need to uncomment following parameter and value SHOULD be absolute file path.--&gt;     &lt;!--ServicesDirectory only works on the following cases--&gt;     &lt;!---File based configurator and in that case the value should be a file URL (http:// not allowed)--&gt;     &lt;!---When creating URL Based configurator with URL “file://”  --&gt;     &lt;!--- War based configurator with expanded case , --&gt;      &lt;!--All the other scenarios it will be ignored.--&gt;     &lt;!--&lt;parameter name=\"ServicesDirectory\"&gt;service&lt;/parameter&gt;--&gt;     &lt;!--To override repository/modules you need to uncomment following parameter and value SHOULD be absolute file path--&gt;     &lt;!--&lt;parameter name=\"ModulesDirectory\"&gt;modules&lt;/parameter&gt;--&gt;      &lt;!--Following params will set the proper context paths for invocations. All the endpoints will have a commons context--&gt;     &lt;!--root which can configured using the following contextRoot parameter--&gt;     &lt;!--&lt;parameter name=\"contextRoot\"&gt;axis2&lt;/parameter&gt;--&gt;      &lt;!--Our HTTP endpoints can handle both REST and SOAP. Following parameters can be used to distinguish those endpoints--&gt;     &lt;!--&lt;parameter name=\"servicePath\"&gt;services&lt;/parameter&gt;--&gt;     &lt;!--&lt;parameter name=\"restPath\"&gt;rest&lt;/parameter&gt;--&gt;      &lt;!-- Following parameter will completely disable REST handling in Axis2--&gt;     &lt;parameter name=\"disableREST\" locked=\"false\"&gt;false&lt;/parameter&gt;      &lt;!--POJO deployer , this will alow users to drop .class file and make that into a service--&gt;     &lt;deployer extension=\".class\" directory=\"pojo\" class=\"org.apache.axis2.deployment.POJODeployer\"/&gt;      &lt;!-- Following parameter will set the host name for the epr--&gt;     &lt;!--&lt;parameter name=\"hostname\" locked=\"true\"&gt;myhost.com&lt;/parameter&gt;--&gt;      &lt;!-- ================================================= --&gt;     &lt;!-- Message Receivers --&gt;     &lt;!-- ================================================= --&gt;     &lt;!--This is the Default Message Receiver for the system , if you want to have MessageReceivers for --&gt;     &lt;!--all the other MEP implement it and add the correct entry to here , so that you can refer from--&gt;     &lt;!--any operation --&gt;     &lt;!--Note : You can override this for particular service by adding the same element with your requirement--&gt;     &lt;messageReceivers&gt;         &lt;messageReceiver mep=\"http://www.w3.org/2004/08/wsdl/in-only\"                          class=\"org.apache.axis2.receivers.RawXMLINOnlyMessageReceiver\"/&gt;         &lt;messageReceiver mep=\"http://www.w3.org/2004/08/wsdl/in-out\"                          class=\"org.apache.axis2.receivers.RawXMLINOutMessageReceiver\"/&gt;         &lt;messageReceiver mep=\"http://www.w3.org/2006/01/wsdl/in-only\"                          class=\"org.apache.axis2.receivers.RawXMLINOnlyMessageReceiver\"/&gt;         &lt;messageReceiver mep=\"http://www.w3.org/2006/01/wsdl/in-out\"                          class=\"org.apache.axis2.receivers.RawXMLINOutMessageReceiver\"/&gt;     &lt;/messageReceivers&gt;      &lt;!-- ================================================= --&gt;     &lt;!-- Message Formatter --&gt;     &lt;!-- ================================================= --&gt;     &lt;!--Following content type to message formatter mapping can be used to implement support for different message --&gt;     &lt;!--format  serialization in Axis2. These message formats are expected to be resolved based on the content type. --&gt;     &lt;messageFormatters&gt;         &lt;messageFormatter contentType=\"application/x-www-form-urlencoded\"                           class=\"org.apache.axis2.transport.http.XFormURLEncodedFormatter\"/&gt;         &lt;messageFormatter contentType=\"multipart/form-data\"                           class=\"org.apache.axis2.transport.http.MultipartFormDataFormatter\"/&gt;         &lt;messageFormatter contentType=\"application/xml\"                           class=\"org.apache.axis2.transport.http.ApplicationXMLFormatter\"/&gt;         &lt;messageFormatter contentType=\"text/xml\"                           class=\"org.apache.axis2.transport.http.SOAPMessageFormatter\"/&gt;         &lt;messageFormatter contentType=\"application/soap+xml\"                           class=\"org.apache.axis2.transport.http.SOAPMessageFormatter\"/&gt; \t&lt;!--JSON Message Formatters--&gt;         &lt;messageFormatter contentType=\"application/json\"                           class=\"org.apache.axis2.json.JSONMessageFormatter\"/&gt;         &lt;messageFormatter contentType=\"application/json/badgerfish\"                           class=\"org.apache.axis2.json.JSONBadgerfishMessageFormatter\"/&gt;         &lt;messageFormatter contentType=\"text/javascript\"                           class=\"org.apache.axis2.json.JSONMessageFormatter\"/&gt;     &lt;/messageFormatters&gt;      &lt;!-- ================================================= --&gt;     &lt;!-- Message Builders --&gt;     &lt;!-- ================================================= --&gt;     &lt;!--Following content type to builder mapping can be used to implement support for different message --&gt;     &lt;!--formats in Axis2. These message formats are expected to be resolved based on the content type. --&gt;     &lt;messageBuilders&gt;         &lt;messageBuilder contentType=\"application/xml\"                         class=\"org.apache.axis2.builder.ApplicationXMLBuilder\"/&gt;         &lt;messageBuilder contentType=\"application/x-www-form-urlencoded\"                         class=\"org.apache.axis2.builder.XFormURLEncodedBuilder\"/&gt; \t&lt;!--JSON Message Builders--&gt;         &lt;messageBuilder contentType=\"application/json\"                         class=\"org.apache.axis2.json.JSONOMBuilder\"/&gt;         &lt;messageBuilder contentType=\"application/json/badgerfish\"                         class=\"org.apache.axis2.json.JSONBadgerfishOMBuilder\"/&gt;         &lt;messageBuilder contentType=\"text/javascript\"                         class=\"org.apache.axis2.json.JSONOMBuilder\"/&gt;         &lt;!--Left commented because it adds the depandancy of servlet-api to other modules.         Please uncomment to Receive messages in multipart/form-data format--&gt;         &lt;!--&lt;messageBuilder contentType=\"multipart/form-data\"--&gt;         &lt;!--class=\"org.apache.axis2.builder.MultipartFormDataBuilder\"/&gt;--&gt;     &lt;/messageBuilders&gt;      &lt;!-- ================================================= --&gt;     &lt;!-- Target Resolvers --&gt;     &lt;!-- ================================================= --&gt;     &lt;!-- Uncomment the following and specify the class name for your TargetResolver to add --&gt;     &lt;!-- a TargetResolver. TargetResolvers are used to process the To EPR for example to --&gt;     &lt;!-- choose a server in a cluster --&gt;     &lt;!--&lt;targetResolvers&gt;--&gt;     &lt;!--&lt;targetResolver class=\"\" /&gt;--&gt;     &lt;!--&lt;/targetResolvers&gt;--&gt;       &lt;!-- ================================================= --&gt;     &lt;!-- Transport Ins --&gt;     &lt;!-- ================================================= --&gt;     &lt;transportReceiver name=\"http\"                        class=\"org.apache.axis2.transport.http.SimpleHTTPServer\"&gt;         &lt;parameter name=\"port\"&gt;6071&lt;/parameter&gt;         &lt;!--If you want to give your own host address for EPR generation--&gt;         &lt;!--uncomment following parameter , and set as you required.--&gt;         &lt;!--&lt;parameter name=\"hostname\"&gt;http://myApp.com/ws&lt;/parameter&gt;--&gt;     &lt;/transportReceiver&gt;      &lt;!--Uncomment if you want to have TCP transport support--&gt;     &lt;!--&lt;transportReceiver name=\"tcp\"                        class=\"org.apache.axis2.transport.tcp.TCPServer\"&gt;         &lt;parameter name=\"port\"&gt;6061&lt;/parameter&gt;--&gt;     &lt;!--If you want to give your own host address for EPR generation--&gt;     &lt;!--uncomment following parameter , and set as you required.--&gt;     &lt;!--&lt;parameter name=\"hostname\"&gt;tcp://myApp.com/ws&lt;/parameter&gt;--&gt;     &lt;!--&lt;/transportReceiver&gt;--&gt;      &lt;!-- ================================================= --&gt;     &lt;!-- Transport Outs --&gt;     &lt;!-- ================================================= --&gt;      &lt;!--&lt;transportSender name=\"jms\"--&gt;     &lt;!--class=\"org.apache.axis2.transport.jms.JMSSender\"/&gt;--&gt;     &lt;transportSender name=\"tcp\"                      class=\"org.apache.axis2.transport.tcp.TCPTransportSender\"/&gt;     &lt;transportSender name=\"local\"                      class=\"org.apache.axis2.transport.local.LocalTransportSender\"/&gt;     &lt;transportSender name=\"http\"                      class=\"org.apache.axis2.transport.http.CommonsHTTPTransportSender\"&gt;         &lt;parameter name=\"PROTOCOL\"&gt;HTTP/1.1&lt;/parameter&gt;         &lt;parameter name=\"Transfer-Encoding\"&gt;chunked&lt;/parameter&gt;         &lt;parameter name=\"SO_TIMEOUT\"&gt;60000&lt;/parameter&gt;         &lt;parameter name=\"CONNECTION_TIMEOUT\"&gt;60000&lt;/parameter&gt;     &lt;/transportSender&gt;     &lt;transportSender name=\"https\"                      class=\"org.apache.axis2.transport.http.CommonsHTTPTransportSender\"&gt;         &lt;parameter name=\"PROTOCOL\"&gt;HTTP/1.1&lt;/parameter&gt;         &lt;parameter name=\"Transfer-Encoding\"&gt;chunked&lt;/parameter&gt;         &lt;parameter name=\"SO_TIMEOUT\"&gt;60000&lt;/parameter&gt;         &lt;parameter name=\"CONNECTION_TIMEOUT\"&gt;60000&lt;/parameter&gt;     &lt;/transportSender&gt;     &lt;!--&lt;transportSender name=\"java\"--&gt;     &lt;!--class=\"org.apache.axis2.transport.java.JavaTransportSender\"/&gt;--&gt;       &lt;!-- ================================================= --&gt;     &lt;!--  SOAP Role Configuration                          --&gt;     &lt;!-- ================================================= --&gt;     &lt;!-- Use the following pattern to configure this axis2          instance to act in particular roles. Note that in          the absence of any configuration, Axis2 will act           only in the ultimate receiver role --&gt;     &lt;!--     &lt;SOAPRoleConfiguration isUltimateReceiver=\"true\"&gt;     \t&lt;role&gt;http://my/custom/role&lt;/role&gt;     &lt;/SOAPRoleConfiguration&gt; \t--&gt;      &lt;!-- ================================================= --&gt;     &lt;!-- Phases  --&gt;     &lt;!-- ================================================= --&gt;     &lt;phaseOrder type=\"InFlow\"&gt;         &lt;!--  System pre-defined phases       --&gt;         &lt;phase name=\"Transport\"&gt;             &lt;handler name=\"RequestURIBasedDispatcher\"                      class=\"org.apache.axis2.dispatchers.RequestURIBasedDispatcher\"&gt;                 &lt;order phase=\"Transport\"/&gt;             &lt;/handler&gt;             &lt;handler name=\"SOAPActionBasedDispatcher\"                      class=\"org.apache.axis2.dispatchers.SOAPActionBasedDispatcher\"&gt;                 &lt;order phase=\"Transport\"/&gt;             &lt;/handler&gt;         &lt;/phase&gt;         &lt;phase name=\"Addressing\"&gt;             &lt;handler name=\"AddressingBasedDispatcher\"                      class=\"org.apache.axis2.dispatchers.AddressingBasedDispatcher\"&gt;                 &lt;order phase=\"Addressing\"/&gt;             &lt;/handler&gt;         &lt;/phase&gt;         &lt;phase name=\"Security\"/&gt;         &lt;phase name=\"PreDispatch\"/&gt;         &lt;phase name=\"Dispatch\" class=\"org.apache.axis2.engine.DispatchPhase\"&gt;             &lt;handler name=\"RequestURIBasedDispatcher\"                      class=\"org.apache.axis2.dispatchers.RequestURIBasedDispatcher\"/&gt;             &lt;handler name=\"SOAPActionBasedDispatcher\"                      class=\"org.apache.axis2.dispatchers.SOAPActionBasedDispatcher\"/&gt;             &lt;handler name=\"RequestURIOperationDispatcher\"                      class=\"org.apache.axis2.dispatchers.RequestURIOperationDispatcher\"/&gt;             &lt;handler name=\"SOAPMessageBodyBasedDispatcher\"                      class=\"org.apache.axis2.dispatchers.SOAPMessageBodyBasedDispatcher\"/&gt;              &lt;handler name=\"HTTPLocationBasedDispatcher\"                      class=\"org.apache.axis2.dispatchers.HTTPLocationBasedDispatcher\"/&gt;         &lt;/phase&gt;         &lt;phase name=\"RMPhase\"/&gt;         &lt;!--  System pre defined phases       --&gt;         &lt;!--   After Postdispatch phase module author or or service author can add any phase he want      --&gt;         &lt;phase name=\"OperationInPhase\"/&gt;     &lt;/phaseOrder&gt;     &lt;phaseOrder type=\"OutFlow\"&gt;         &lt;!--      user can add his own phases to this area  --&gt;         &lt;phase name=\"OperationOutPhase\"/&gt;         &lt;!--system predefined phase--&gt;         &lt;!--these phase will run irrespective of the service--&gt;         &lt;phase name=\"RMPhase\"/&gt;         &lt;phase name=\"PolicyDetermination\"/&gt;         &lt;phase name=\"MessageOut\"/&gt;         &lt;phase name=\"Security\"/&gt;     &lt;/phaseOrder&gt;     &lt;phaseOrder type=\"InFaultFlow\"&gt;         &lt;phase name=\"Addressing\"&gt;             &lt;handler name=\"AddressingBasedDispatcher\"                      class=\"org.apache.axis2.dispatchers.AddressingBasedDispatcher\"&gt;                 &lt;order phase=\"Addressing\"/&gt;             &lt;/handler&gt;         &lt;/phase&gt;         &lt;phase name=\"Security\"/&gt;         &lt;phase name=\"PreDispatch\"/&gt;         &lt;phase name=\"Dispatch\" class=\"org.apache.axis2.engine.DispatchPhase\"&gt;             &lt;handler name=\"RequestURIBasedDispatcher\"                      class=\"org.apache.axis2.dispatchers.RequestURIBasedDispatcher\"/&gt;             &lt;handler name=\"SOAPActionBasedDispatcher\"                      class=\"org.apache.axis2.dispatchers.SOAPActionBasedDispatcher\"/&gt;             &lt;handler name=\"RequestURIOperationDispatcher\"                      class=\"org.apache.axis2.dispatchers.RequestURIOperationDispatcher\"/&gt;             &lt;handler name=\"SOAPMessageBodyBasedDispatcher\"                      class=\"org.apache.axis2.dispatchers.SOAPMessageBodyBasedDispatcher\"/&gt;              &lt;handler name=\"HTTPLocationBasedDispatcher\"                      class=\"org.apache.axis2.dispatchers.HTTPLocationBasedDispatcher\"/&gt;         &lt;/phase&gt;         &lt;phase name=\"RMPhase\"/&gt;         &lt;!--      user can add his own phases to this area  --&gt;         &lt;phase name=\"OperationInFaultPhase\"/&gt;     &lt;/phaseOrder&gt;     &lt;phaseOrder type=\"OutFaultFlow\"&gt;         &lt;!--      user can add his own phases to this area  --&gt;         &lt;phase name=\"OperationOutFaultPhase\"/&gt;         &lt;phase name=\"RMPhase\"/&gt;         &lt;phase name=\"PolicyDetermination\"/&gt;         &lt;phase name=\"MessageOut\"/&gt;         &lt;phase name=\"Security\"/&gt;     &lt;/phaseOrder&gt; &lt;/axisconfig&gt;     So if you ever need a repository, I’ve currently haven’t found any better than the one from the guys at WSO2.  ","categories": ["posts","java","governance","wso2"],
        "tags": [],
        "url": "http://www.smartjava.org/content/access-wso2-registry-programatically/",
        "teaser":null},{
        "title": "HTML5: Combining physics engine (box2dWeb) with DeviceOrientation",
        "excerpt":"When the first smartphones came out, they all had the traditional labyrinth applications. Application where you had to move a ball around a certain path by tilting and moving your phone around. There is also an HTML5 specification that allows you to do this directly from your browser. This would allow us to create all kind of cool games that run directly from the browser. In this article I’ll show you my first experiments with this API in combination with a javascript based physics engine. The goal is to create a simple HTML5 application where you can move a number of balls around using the motion sensors in your device. What we’re aiming for is this:      You can test this for yourself here: “Box2dWeb and deviceorientation demo”.   Before I start with the explanation, a quick note on device compatibility. My initial idea was to write this for my tablet to play around a bit with the various sensors, HTML5 apis etc. that are available. The problem however is, that at the moment the box2dweb.js library is much too heavy for mobile development. I’ve alo tried other physics javascript libraries out there, but they all result in a framerate of about 2 to 3 frames per second. I’m currently looking into offloading the physics engine to a serverside backend (just as I did in the facedetection example). So I tested and changed this example to work with the sensors from my MacBook Pro.   Accessing device sensors   Accessing a device’s sensors is very easy. The specification defines that you have to register for a specific event, and you’ll receive a callback at a specific interval like this:         window.addEventListener(\"deviceorientation\", function(event) {           // process event.alpha, event.beta and event.gamma       }, true); &lt;/javscript&gt;  The event you receive contains the following information:    {alpha: 90,        // represents position on the z-axis    beta: 0,          // represents position on the x-axis    gamma: 0};    // represents position on the y-axis ```   For my macbook this results in the following set of events:   absolute: null alpha: null beta: 2.2814938370474303 bubbles: false cancelBubble: false cancelable: false clipboardData: undefined currentTarget: DOMWindow defaultPrevented: false eventPhase: 0 gamma: 1.3697507397371704 returnValue: true srcElement: DOMWindow target: DOMWindow timeStamp: 1336114836578 type: \"deviceorientation\"   As you can see, besides a lot of other info, we receive an alpha, beta and a gamma. For my macbook I never receive an alpha value. For this demo I want to roll all the balls, from the image, to the left when I tilt my laptop to the left and they should roll to the right when I tilt my laptop to the right. The same goes for when I tilt my laptop backwards, the balls should roll to the top and when I tilt my laptop downwards the balls should roll to the bottom.   I use the following listener for this:    // initialize the device orientation and set the callback function initOrientation() {     if (window.DeviceOrientationEvent) {         console.log(\"DeviceOrientation is supported\");         window.addEventListener('deviceorientation', function (eventData) {              var LR = eventData.gamma; // used as x gravity             var FB = eventData.beta;  // used as y gravity              var newXGravity = Math.round(LR / 2);             var newYGravity = Math.round(FB / 2);              if (newXGravity != world.m_gravity.x || newYGravity != world.m_gravity.y ) {                  // set new gravity                 world.m_gravity.x = newXGravity                 world.m_gravity.y = newYGravity                  // wakeup all the bodies when the gravity changes                 for (var body = world.m_bodyList; body; body = body.m_next) {                     body.SetAwake(true);                 }             }         }, false);     } else {         alert(\"Not supported\");     } }   In this listing I start listening for the event and retrieve the new X and the new Y gravity based on the tilt of the laptop. When the tilt has changed, I set the new gravity of the physics world and wake up any sleeping bodies (see below for more info on this). And that’s all you need to do.   Setup the physics engine  Next lets look at the physics engine. I’ve use box2dweb, which is an javascript implementation of the box2d javascript engine. I won’t go into too much detail here, but the comments in the code should explain what is happening.   function initWorld() {     var b2Vec2 = Box2D.Common.Math.b2Vec2         , b2AABB = Box2D.Collision.b2AABB         , b2BodyDef = Box2D.Dynamics.b2BodyDef         , b2Body = Box2D.Dynamics.b2Body         , b2FixtureDef = Box2D.Dynamics.b2FixtureDef         , b2Fixture = Box2D.Dynamics.b2Fixture         , b2World = Box2D.Dynamics.b2World         , b2MassData = Box2D.Collision.Shapes.b2MassData         , b2PolygonShape = Box2D.Collision.Shapes.b2PolygonShape         , b2CircleShape = Box2D.Collision.Shapes.b2CircleShape         , b2DebugDraw = Box2D.Dynamics.b2DebugDraw         , b2MouseJointDef = Box2D.Dynamics.Joints.b2MouseJointDef;      // setup the world     world = new b2World(         new b2Vec2(0, 10)    //gravity         , true               //allow sleep     );      // define the borders     var fixDef = new b2FixtureDef;     fixDef.density = 0.1;     fixDef.friction = 0.3;     fixDef.restitution = 0.2;      var bodyDef = new b2BodyDef;     //create enclosure     bodyDef.type = b2Body.b2_staticBody;     fixDef.shape = new b2PolygonShape;     fixDef.shape.SetAsBox(width / 10, 2);      // draw lower bound     bodyDef.position.Set(0, height / scale);     world.CreateBody(bodyDef).CreateFixture(fixDef);      // draw upper bound     bodyDef.position.Set(0, 0);     world.CreateBody(bodyDef).CreateFixture(fixDef);      // draw left bound     fixDef.shape.SetAsBox(2, height);     bodyDef.position.Set(0, 0);     world.CreateBody(bodyDef).CreateFixture(fixDef);      // draw right bound     bodyDef.position.Set(width / scale, 0);     world.CreateBody(bodyDef).CreateFixture(fixDef);      //create 100 objects     bodyDef.type = b2Body.b2_dynamicBody;     for (var i = 0; i &lt; 100; ++i) {         fixDef.shape = new b2CircleShape(1);         bodyDef.position.x = (Math.random() * width) / scale;         bodyDef.position.y = (Math.random() * height) / scale;         world.CreateBody(bodyDef).CreateFixture(fixDef);     }      //setup debug draw     var debugDraw = new b2DebugDraw();     debugDraw.SetSprite(document.getElementById(\"canvas\").getContext(\"2d\"));     debugDraw.SetDrawScale(10.0);     debugDraw.SetFillAlpha(1);     debugDraw.SetLineThickness(1.0);     debugDraw.SetFlags(b2DebugDraw.e_shapeBit | b2DebugDraw.e_jointBit);     world.SetDebugDraw(debugDraw);      // start drawing     window.setInterval(update, 1000 / 100); }  function update() {     world.Step(1 / 60, 8, 3);     world.DrawDebugData();     world.ClearForces(); }   Wrapping up   Accessing the device orientation API is actually pretty easy. Using the information in a useful way is a whole other story. It really is too bad though that there isn’t a performant javascript physics engine yet for mobile devices. On the other hand, with the speed the power of smartphones and tablets is increasing time might solve this.   ","categories": ["posts","box2d","javascript","html5"],
        "tags": [],
        "url": "http://www.smartjava.org/content/html5-combining-physics-engine-box2dweb-deviceorientation/",
        "teaser":null},{
        "title": "Partial image manipulation with canvas and webworkers",
        "excerpt":"In my previous article I showed how to use box2d-web and deviceorientation to rol around a set of circles using standard HTML5 APIs. My initial goal of that article was to load in an image, translate it to a series of seperate circles, and let you play around with that. But that’s for later. For now I’ll show you how you can use canvas together with web workers to offload heavy computing functions to a background thread.   For this example we’ll take an input image and manipulate the image in blocks of 10x10 pixels. For each block we’ll calculate the dominating color and render a rectangle on our target in that color. Since an example says more then a thousand words, we’re going to create this:      For those wondering, that’s my daugther thinking very hard about something. You can see this demo in action here.   Getting started   To implement this example we don’t need to do that much. We just have to take the following steps:      Wait until the image is loaded.   Split the image into seperate parts ready for processing.   Configure a web worker to start processing when it receives a message   Calculate the dominating color from our image sample.   Render a rectangle on our target canvas   Let’s begin simple, and look at the image loading code.   Wait for image to be loaded  Before we start processing the image we have to make sure it is completely loaded. For this we use the following piece of code:       // start processing when the document is loaded.     $(document).ready(function () {          // handles rendering the elements         setupWorker();          // wait for the image to be loaded, before we start processing it.         $(\"#source\").load(function () {              // determine size of image             var imgwidth = $(this).width();             var imgheight = $(this).height();              // create a canvas and make context available             var targetCanvas = createTargetCanvas(imgwidth, imgheight);             targetContext = targetCanvas.getContext(\"2d\");              // render elements             renderElements(imgwidth, imgheight, $(this).get()[0]);         });     });   As you can see here, we register the jquery ready function first. This will trigger when the complete document is loaded. This however doesn’t have to mean that the images have also already been loaded. To make sure the image is ready to be processed, we add the jquery load function to our source image (has id of #source). When the image is loaded we determine the required size of our target canvas, on which we render the result, and fire of the rendering using the renderElements function. The renderElements function splits the image and fires of the webworkers.   Split the image into seperate parts ready for processing   The goal of this example is to create a kind of low pixel effect on our source image. We do this by selecting part of the image, calculate the dominating color, and render a square on the target canvas. The following code shows how you can use a temporary canvas to select part of the image.       // process the image by splitting it in parts and sending it to the worker     function renderElements(imgwidth, imgheight, image) {         // determine image grid size         var nrX = Math.round(imgwidth / bulletSize);         var nrY = Math.round(imgheight / bulletSize);          // iterate through all the parts of the image         for (var x = 0; x &lt; nrX; x++) {             for (var y = 0; y &lt; nrX; y++) {                 // create a canvas element we use for temporary rendering                 var canvas2 = document.createElement('canvas');                 canvas2.width = bulletSize;                 canvas2.height = bulletSize;                 var context2 = canvas2.getContext('2d');                 // render part of the image for which we want to determine the dominant color                 context2.drawImage(image, x * bulletSize, y * bulletSize, bulletSize, bulletSize, 0, 0, bulletSize, bulletSize);                  // get the data from the image                 var data = context2.getImageData(0, 0, bulletSize, bulletSize).data                 // convert data, which is a canvas pixel array, to a normal array                 // since we can't send the canvas array to a webworker                 var dataAsArray = [];                 for (var i = 0; i &lt; data.length; i++) {                     dataAsArray.push(data[i]);                 }                  // create a workpackage                 var wp = new workPackage();                 wp.colors = 5;                 wp.data = dataAsArray;                 wp.pixelCount = bulletSize * bulletSize;                 wp.x = x;                 wp.y = y;                  // send to our worker.                 worker.postMessage(wp);             }         }     }   In this function we first determine in how many rows and columns we’re going to split up the image. We iterate over each of these elements and render that specific part of the image on a temporary canvas. From that canvas we get the data using the getImageData function. At this point we’ve got all the information we need for our worker to calculate the dominating color (this is an expensive operation). We store the info in a ‘workpackage’:        function workPackage() {         this.data = [];         this.pixelCount = 0;         this.colors = 0;         this.x = 0;         this.y = 0;          this.result = [0, 0, 0];     }    This is a convience class that serves as the message to and from our webworker. Note that we need to convert the result from the getImageData call to a normal array. Information to a webworker is copied, and chrome at least isn’t able to copy the resulting array from the getImageData operation.  So far so good. We now have nice workpackages for each part of our screen, which we pass to a webworker using the worker.postMessage operation. But what does this worker look like, and how do we configure it?   Configure a web worker to start processing when it receives a message  We create the worker in the setupWorker operation that is called when our document is loaded.       function setupWorker() {         worker = new Worker('extractMainColor.js');         worker.addEventListener('message', function (event) {              // the workpackage contains the results             var wp = event.data;              // get the colors             var colors = wp.result;              drawRectangle(targetContext, wp.x, wp.y, bulletSize, colors[0]);             //drawCircle(targetContext, wp.x, wp.y, bulletSize, colors[0]);          }, false);     }   Creating a worker, as you can see, is very simple. Just point the worker to the javascript he needs to execute. Note that there are all kind of restrictions with regards to the resources and objects a worker has access to. A good introduction to what can and what can’t be accessed can be found in this article.  Once we defined the worker, we add an eventListener. This listener is called when the worker uses the postMessage operation. In our example this is used to pass the result back in the same workpackage. Based on this result we draw a rectangle (or some other figure) on our target canvas. The worker itself is very basic:   importScripts('quantize.js' , 'color-thief.js');  self.onmessage = function(event) {      var wp = event.data;     var foundColor = createPaletteFromCanvas(wp.data,wp.pixelCount, wp.colors);     wp.result = foundColor;     self.postMessage(wp);  };   This worker uses two external scripts to calculate and return the dominating color. It does this by getting the required information from the workpackage, calculate the dominating color, and return the result in the workpackage using the postMessage. Calculating the dominating color itself isn’t that easy. I ran across a great library named color-thief, that does this for you. Apparently you need to take more into account than just the RGB values, if you do that then you just get a set of brown colors.   Calculate the dominating color from our image sample.   I mentioned that I used the color-thief library to calculate the dominating color. I do this using this code:   createPaletteFromCanvas(wp.data,wp.pixelCount, wp.colors);   This, however, isn’t directly provided by color-thief. Color-thief assumes you want to use it directly on an image element on your page. I had to extend the color-thief library with the following simple operation so that it can work directly with binary data.   function createPaletteFromCanvas(pixels, pixelCount, colorCount) {      // Store the RGB values in an array format suitable for quantize function     var pixelArray = [];     for (var i = 0, offset, r, g, b, a; i &lt; pixelCount; i++) {         offset = i * 4;         r = pixels[offset + 0];         g = pixels[offset + 1];         b = pixels[offset + 2];         a = pixels[offset + 3];         // If pixel is mostly opaque and not white         if (a &gt;= 125) {             if (!(r &gt; 250 &amp;&amp; g &gt; 250 &amp;&amp; b &gt; 250)) {                 pixelArray.push([r, g, b]);             }         }     }      // Send array to quantize function which clusters values     // using median cut algorithm      var cmap = MMCQ.quantize(pixelArray, colorCount);     var palette = cmap.palette();      return palette; }   This returns an array of most dominating colors (just as the normal color-thief functions do) but can work directly on the data from our worker.   Render a rectangle on our target canvas   And that’s pretty much it. At this point we’ve split our image into an array of subimages. Each part is sent to a webworker for processing. The webworker processes the image and passes the result back to our eventhandler. In the eventhandler we take the most dominating color and we can use that to draw on the canvas. In the figure at the beginning of this article I used rectangles:      Using this javascript (and with a bulletsize of 15):       // draw a rectangle on the supplied context     function drawRectangle(targetContext, x, y, bulletSize, colors) {         targetContext.beginPath();         targetContext.rect(x * bulletSize, y * bulletSize, bulletSize, bulletSize);         targetContext.fillStyle = \"rgba(\" + colors + \",1)\";         targetContext.fill();     }   But we could just as easily render circles:      Using this:       // draw a circle on the supplied context     function drawCircle(targetContext, x, y, bulletSize, colors) {         var centerX = x * bulletSize + bulletSize / 2;         var centerY = y * bulletSize + bulletSize / 2;         var radius = bulletSize / 2;          targetContext.beginPath();         targetContext.arc(centerX, centerY, radius, 0, 2 * Math.PI, false);         targetContext.fillStyle = \"rgba(\" + colors + \",1)\";         targetContext.fill();     }   As you can see web workers are really easy to use, and canvas allows us much options to work with imagedata. In this example I only used a single web worker, more interesting would be to add a queue on which multiple workers would listen to really process elements in parallel. The demo and complete code for this article can be found here.  ","categories": ["posts","javascript","html5"],
        "tags": [],
        "url": "http://www.smartjava.org/content/partial-image-manipulation-canvas-and-webworkers/",
        "teaser":null},{
        "title": "Presentation: From REST to HATEOAS",
        "excerpt":"Update 25th of May 2012: I changed this presentation a bit for the Goto Con Amsterdam 2012. These are the latest slides.   Yesterday I gave a presentation at JPoint’s meetingpoint on how to get your API or service from using the basic REST principles such as verbs and resources to a complete RESTful service that fully supports “Hypermedia as the engine of application state” (HATEOAS).   I’ve just put the slides on slideshare if you want to see what I talked about:    REST: From GET to HATEOAS    If you want to hear me talk about this again, I'll be giving pretty much the same presentation in a couple of weeks at gotocon:  Presentation: \"REST: from GET to HATEOAS\" Track: Browser As A Platform Time: Thursday 15:50 - 16:40 Location: Keurzaal   ","categories": ["posts","hmac","hateoas","rest"],
        "tags": [],
        "url": "http://www.smartjava.org/content/presentation-rest-hateoas/",
        "teaser":null},{
        "title": "HTML5: Easily parallelize jobs using web workers and a threadpool",
        "excerpt":"I’ve been experimenting with web workers and the various browser implementations. Most of the articles I’ve seen show an example where a single worker thread is started in the background to execute some heavy task. This frees up the main thread to render the rest of the webpage and respons to user input. In a previous article I showed how you can off-load CPU heavy tasks to a seperate web worker thread. In that example we used a couple of libraries to get the following effect:      Sinc almost everyone nowadays has multiple cores it’s a waste not to use them. In this article I’ll show how we can use a simple threadpool to parallelize this even further and increase the rendering time by +/- 300%. You can run this example from the following location: http://www.smartjava.org/examples/webworkers2/   The threadpool code   To test multiple threads with web workers I wrote a simple (and very naive) threadpool / taskqueue. You can configure the maximum number of concurrent web workers when you create this pool, and any ‘task’ you submit will be executed using one of the available threads from the pool. Note that we aren’t really pooling threads, we’re just using this pool to control the number of concurrently executing web workers.   function Pool(size) {     var _this = this;      // set some defaults     this.taskQueue = [];     this.workerQueue = [];     this.poolSize = size;      this.addWorkerTask = function(workerTask) {         if (_this.workerQueue.length &gt; 0) {             // get the worker from the front of the queue             var workerThread = _this.workerQueue.shift();             workerThread.run(workerTask);         } else {             // no free workers,             _this.taskQueue.push(workerTask);         }     }      this.init = function() {         // create 'size' number of worker threads         for (var i = 0 ; i &lt; size ; i++) {             _this.workerQueue.push(new WorkerThread(_this));         }     }      this.freeWorkerThread = function(workerThread) {         if (_this.taskQueue.length &gt; 0) {             // don't put back in queue, but execute next task             var workerTask = _this.taskQueue.shift();             workerThread.run(workerTask);         } else {             _this.taskQueue.push(workerThread);         }     } }  // runner work tasks in the pool function WorkerThread(parentPool) {      var _this = this;      this.parentPool = parentPool;     this.workerTask = {};      this.run = function(workerTask) {         this.workerTask = workerTask;         // create a new web worker         if (this.workerTask.script!= null) {             var worker = new Worker(workerTask.script);             worker.addEventListener('message', dummyCallback, false);             worker.postMessage(workerTask.startMessage);         }     }      // for now assume we only get a single callback from a worker     // which also indicates the end of this worker.     function dummyCallback(event) {         // pass to original callback         _this.workerTask.callback(event);          // we should use a seperate thread to add the worker         _this.parentPool.freeWorkerThread(_this);     }  }  // task to run function WorkerTask(script, callback, msg) {      this.script = script;     this.callback = callback;     this.startMessage = msg; };   Using the threadpool   To use this threadpool we now just have to do this:       var pool = new Pool(6);     pool.init();   This will create a pool that will allow a maximum number of 8 threads running concurrently. If we want to create a task to be executed by this pool we just create a workerTask and submit it like this:         var workerTask = new WorkerTask('extractMainColor.js',callback,wp);       pool.addWorkerTask(workerTask);   This will create a web worker from ‘extractMainColor.js’ and register the supplied function as callback. Once the worker is ready to be run, the last argument will be used to send a message to the worker. A caveat on this implementation. I now assume that the when the web worker sends a message back it will close itself after sending this message. As you can see in the following example:   importScripts('quantize.js' , 'color-thief.js');  self.onmessage = function(event) {     var wp = event.data;     var foundColor = createPaletteFromCanvas(wp.data,wp.pixelCount, wp.colors);     wp.result = foundColor;     self.postMessage(wp);      // close this worker     self.close(); };   Results   I’ve tested this a couple of times with different settings for number of concurrent threads. The results are shown in the following table:   Chrome:     Number of threadsTotal rendering time   114213 29956 38778 47846 56924 66309 75912 85468 95201 105193 115133 125208   The result for firefox are less impressive, but you can still see a big gain:   Firefox:     Number of threadsTotal rendering time   117909 211273 310422 410154 510115 610052 710000 89997   As you can see, both for Firefox and Chrome it’s useful to not just use a single web worker, but further split the tasks. For firefox we can see a big gain if we use two web workers, and for chrome we keep on getting better results to 8 or 9 parallel web workers!  ","categories": ["posts","javascript","web workers","html5"],
        "tags": [],
        "url": "http://www.smartjava.org/content/html5-easily-parallelize-jobs-using-web-workers-and-threadpool/",
        "teaser":null},{
        "title": "HTML5: Remotely vibrate a phone with morse code using web sockets and the vibrate API.",
        "excerpt":"In my last couple of blog posts I showed you can use a couple of the new HTML5 related APIs (webrtc, web workers, device orientation) directly from your browser. A couple of days ago I ran into the W3C Vibration API, which reached “Candidate Recommendation” status last week (8th of May). This API allows you, as the name might suggest (unless you have a very dirty mind), access to the vibrate functionality of your device. Or as the specification says it:   “This specification defines an API that provides access to the vibration mechanism of the hosting device. Vibration is a form of tactile feedback.”   This API in itself is very simple:    interface Vibration {     void vibrate (unsigned long time);     void vibrate (unsigned long[] pattern); };    So basically to vibrate your device for 10 seconds do this:     navigator.vibrate(10000);   If you want to vibrate in a specific pattern, you can supply the vibrate method with an array of values. Where each even entry (starts with 0) vibrates the device and each odd entry signals a pauze:      navigator.vibrate([1000, 500, 2000]);   Very easy to use as you can see. Now lets see if we can create something interesting with this. For this tutorial we’ll create a simple HTML5 application (the receiver) that can receive morse code encoded messages through a websocket and ‘play’ these messages using a device’s vibration function. We’ll also create a simple webpage (the sender) where you can enter the messages to be sent to the receiver.  Note that I’ve tested this with mozilla latest mobile browser, I couldn’t find whether any of the other browser already support this API.   The following video shows the application we’ll be building (turn up the volume, since my phone doesn’t make much noise…) :     In this video you can see that we use one browser (my tablet) to send messages. These messages are played back on the second device (my mobile phone) which runs the site in a browser.   Vibrate when a morse code message is received  The first thing we need to do is vibrate the device whenever a morse code message is received. Almost everybody knows what morse code is about. With morse code you can send words encoded using a set of short (dot) and longer (dash) signals. The best known one is SOS, which encodes to “··· — ···”. The following overview from wikipedia shows how this works in more detail:      So what we need to do is define the type of message we receive and based on that message we vibrate the device. Lets first look at the very simple message format we’ve defined. This message reads “HTML5 RULES”:   var morsecode=\".... - -- .-.. .....|.-. ..- .-.. . ...\"                  We seperate each letter using an empty space (“ “) character and seperate each word with the pipe (“       ”) character. Looking back at the figure from wikipedia we can now convert this to a series of vibrations for our device.                   CodeDuration            .Vibrate for 1T, after each . we also pause for 1T between parts of letters            -Vibrate for 3T, after each - we also pause for 1T between parts of letters            \"&nbsp;\"Signifies a pause of 3T            |Signifies a pause of 7T      By playing around with the value of T, we can get a good interval that can be easily detected by the person receiving the morse message. I’ve noticed that working with a value of 250ms to 500ms  works ok. Lets convert this to javascript.   $(document).ready(function() {     addListeners(); });  // time per tick is 200 ms var T = 200;  function addListeners() {     $(\"#vibrate\").click(function() {         var morsecode=\".... - -- .-.. .....|.-. ..- .-.. . ...\";         var toPlay = playMorseCode(morsecode);         navigator.mozVibrate(toPlay);     }); }  function playMorseCode(code) {      var arrayToPlay = [];      for(i=0; i&lt;code.length ; i++) {         var char = code.charAt(i);         // we first check if the code we received is a \".\"         if (char == '.') {             // add vibrate of 1T             arrayToPlay.push(T);         } else if (char == '-') {             // add vibrate of 3T             arrayToPlay.push(3*T);         } else if (char == ' ') {             // add pause of 3T             arrayToPlay.push(3*T);         } else if (char == '|') {             arrayToPlay.push(7*T);         }          // we might need to add a spacer if the next character is either a \".\" or a \"-\"         // and the current char is either a \".\" or a \"-\"         if (((i + 1) &lt; code.length)             &amp;&amp; (code.charAt(i) == \".\" || code.charAt(i) == \"-\")             &amp;&amp; (code.charAt(i+1) == \".\" || code.charAt(i+1) == \"-\")) {             arrayToPlay.push(T);         }     }      return arrayToPlay; }   The code above converts the string in the format we specified into an array we can pass to the vibrate function. In this example I’ve bound the function to a simple button to make it easier to test the vibrate functionality. I’ve used the following HTML for testing:    &lt;html&gt; &lt;head&gt;     &lt;script type=\"text/javascript\" src=\"./lib/jquery-1.7.1.3.js\"&gt;&lt;/script&gt;     &lt;script type=\"text/javascript\" src=\"./js/vibration.js\"&gt;&lt;/script&gt;     &lt;title&gt;Morse receiver&lt;/title&gt; &lt;/head&gt; &lt;body&gt;     &lt;button id=\"vibrate\"&gt;Test Vibrate&lt;/button&gt; &lt;/body&gt; &lt;/html&gt;    Can’t get any easier than that. When you click the button on a device that support vibration in a browser that support this API (I’ve tested with the Firefox Mobile Beta browser on Android).   That handles the morse code to vibrations part. Further down in the article we’ll change this code a bit to work with websockets, but for a quick demo you can use this code. Next we’ll look at how we can connect this webapp to another one using websockets.   Use Scala and Play's websocket support for communication   We need to setup a channel between the morse code receiver and the morse code sender. For this we’ll create two websockets that can be used from HTML5 to connect to. For this example I’ve used the Play Framework 2.0 together with scala. First define the routes:    GET     /sender              controllers.Application.morsesender GET     /receiver            controllers.Application.morsereceiver    And next define the actions that handle the websocket requests:      // keeps track of the last receiver   var receiverOut : PushEnumerator[String] = null;      /**    * Receiver doesn't do much. Just register the receiverOut channel    * so that it can be used by the sender.    */   def morsereceiver = WebSocket.using[String] { request =&gt;    receiverOut = Enumerator.imperative[String]();    val in = Iteratee.foreach[String](content =&gt; {})    (in, receiverOut)   }       /**    * The sender just pushes the received content directly to    * the output.    */   def morsesender = WebSocket.using[String] { request =&gt;      val out = Enumerator.imperative[String]();     val in = Iteratee.foreach[String](content =&gt; {       if (receiverOut != null) {          receiverOut.push(content);         }     })      (in, out)   }    All very trivial (more info on websockets and Play can be found in one of my previous articles here). With these routes and these actions all requests from the morse sender are immediately sent to the morse receiver. Now that we’ve got websockets that we can use to communicate between the sender and the receiver, lets look at the sender.   Morse code sender   The sender should just show a simple text area where we can type in plain text. After we click on a button this text is converted to the morse code format we defined, and is sent to a websocket. The server will pass this on to the receiver which will vibrate the phone. The sender we create looks like this:      The complete javascript for this is shown here:   // variable that holds ws connection var ws = null;  // define how morse code is created morjs.defineMode('smartjava', [     '.',     '-',     '',     ' ',     '|' ]);  $(document).ready(function() {     setupMorseSender();     addListeners(); });   function setupMorseSender() {     ws = new WebSocket(\"ws://10.0.0.157:9000/sender\");     ws.onopen = function () {         console.log(\"Openened connection to websocket\");     } }  function addListeners() {     $(\"#send\").click(function() {         var toSend = $(\"#morse\").val();         var encoded = morjs.encode({message: toSend , mode: 'smartjava'});         ws.send(encoded);     });      $(\"#clear\").click(function() {         $(\"#morse\").val(\"\");     }); }   As you can see in the code we convert the input text to morse code from javascript. For this I use the mor.js library. This is a javascript libary that allows you to convert text to morse code and also allows you to define the way this morse code is represented. Since I want to represent morse code in this format: “…. - – .-.. …..|.-. ..- .-.. . …” we need to supply the mor.js library with a formatter. This is done in the previous code by the call to the morjs.defineMode() function. The rest of the javascript is very straighforward. We use jquery to attach functionality to the two buttons. Once the ‘send’ button is clicked we create the morse code from the entered text and send it as a text message using a websocket that is connected to the simple play/scala server running at “ws://10.0.0.157:9000/sender”. That wraps up the sender configuration. The final step we need to take is change our example vibrate code to listen to a websocket and once we receive a message output that message through the device’s vibrate function.   Morse code receiver   We’ve already seen most of the code for the receiver. In this part I’ll only show how to connect the receiver to the websocket and invoke the vibrate function when a message arrives. The code that handles websockets is shown here:   function setupMorseReceiver() {     var ws = new WebSocket(\"ws://10.0.0.157:9000/receiver\");     ws.onopen = function () {         ws.send(\"message sent\");     }     ws.onmessage = function (msg) {         ws.send(\"msg received:\" + msg);         navigator.mozVibrate(playMorseCode(msg.data));     } }   Couldn’t be much easier! We connect to the websocket running at “ws://10.0.0.157:9000/sender”, and when we receive a message we directly pass it to the playMorseCode() function and our devices stats vibrating the supplied morse code pattern. And that’s all. As you can see in the video at the beginning of this article we can now send morse encoded messages from one web application to another where they are played back using the vibrate function of the device.  ","categories": ["posts","websockets","javascript","html5"],
        "tags": [],
        "url": "http://www.smartjava.org/content/html5-remotely-vibrate-phone-morse-code-using-web-sockets-and-vibrate-api/",
        "teaser":null},{
        "title": "Stop labeling everything as an impedance mismatch!",
        "excerpt":"I recently ran across an article that was talking (again) about the Object-Relational mismatch. And just like in many  articles this mismatch is called the Object-Relational Impedance mismatch. This “impedance mismatch” label isn’t just added when talking about object and relational databases, but pretty much in any situation where we have two concepts that don’t match nicely:      the XML/OO impedance mismatch   noSQL/OO impedance mismatch   document/OO impedance mismatch   JSON Strongly typed language impedance mismatch   Developer/Non developer impedance mismatch   etc.   But what really is impedance mismatch? Impedance matching is something that comes from the world of electronics (or RF or audio):   “A technique of electric circuit design in which one component provides power to another, and the output circuit of the first component has the same impedance as the input circuit of the second component. Maximum power transfer is achieved when the impedances in both circuits are exactly the same. Impedance matching is important wherever power needs to be transmitted efficiently, as in the design of power lines, transformers, and signal-processing devices such as audio and computer circuits.”   It comes down to the fact that power sources have their own internal resistance. Since we can’t reduce the internal resistance to zero (unless you want to start playing with super conductors at -273 degree celcius). Because of this internal resistance some amount of power is wasted in the power source, instead of it all being transferred to the connected circuit (the load). If you look at the internal resistance and the resistance of the load in relation to the power that is lost in the generator we see the following:      In this figure you can see that we get the maximum amount of power transferred when the load resistance matches the resistance of the power source. More generalized this means matching the source impedance with the load impedance. We use the term impedance here. Impedance really isn’t anything more than a more general term for resistance that also includes reactance. Impedance can be used for both AC and DC current, resistance doesn´t cover everything with regards to AC (but that’s a different discussion).   But is this all good? If you look at the graph, we still loose half the power in the generator. Even if we get maximum transfer of power we’re not really efficient. In practice what you see is that in power station the load resistance is set higher than that of the source to waste as little power as possible. The same goes when we’re connecting speakers to an HIFI amp. In that scenario we usually have a very low resistance at the source and a higher resistance at the speaker (8 ohm). This will result in minimal power loss and still cause most of the power being transferred to the speaker.   There are however cases where impedance matching is a good idea. I won’t go into the technical details but when you’re dealing with high frequency signals, is that if you don’t match up the impedance between source and load not all the energy is transferred to the load, but is reflected back along the cable to the source. Which can cause all kinds of problems.   So is it a badly chosen name, since in many cases having an impedance mismatch is actually a good thing?   Yes it is a very badly chosen name!    At least I think it is. In the way we use it impedance mismatch sounds like a bad thing. In electrical engineering it is just a property of an electronic circuit. In some circuits you might need to have impedance matching, in others you don’t.   Saying we have an object relation impedance mismatch doesn’t mean anything. Yes we have a problem between the OO world and the relation world, no discussion about that. Same goes for the other examples I gave in the beginning of this article. But labelling it with the “impedance mismatch” doesn’t tell us anything about the kind of problem we have. We have a “concept mismatch”, a “model mismatch”, or a “technology mismatch”.   I propose we start promoting the term “developer-manager impedance mismatch”, where developers with minimal resistance from management can be the most efficient and get the most done!  ","categories": ["posts","impedance mismatch"],
        "tags": [],
        "url": "http://www.smartjava.org/content/stop-labeling-everything-impedance-mismatch/",
        "teaser":null},{
        "title": "Adjust colors of your page based on the lighting of the room with HTML5, webrtc and a webcam",
        "excerpt":"Most modern laptops automatically dim the screen when the light conditions in the room change. If there is a lot of light, the screen normally increases in brightness to make sure you can still see everything clearly. If the light conditions are turned down, the screen will also descrease in brightness. This isn’t only pleasant on the eyes, but will also probably extend your battery life. But why not take this a step further and not only adjust the brightness of the screen, but also increase the readability of the web site in the user’s browser?   In this article I’ll show you how you can use input from the webcam, in a fully HTML5 compatible manner, to detect the luminance of the room. If you’re in a brightly lit location, the browser will dynamically increase the contrast between the background and the fonts. If the lighting conditions change, you’ll see this also automatically reflected in the background. If you want to directly look at the result look here.   With a bright room we see this:  With a dark room we see this:    For those of you that don’t have a webcam to test with, I recorded a small movie that shows the effect we’re aiming at:     What do you need to do for this:      Allow access to the webcam   At an interval get a screenshot from the webcam   Calculate the luminance of the room   Adjust the CSS for the background   The first couple of steps are easy, and you can find a more detailled explanation in one of my previous posts.   Allow access to the webcam   I tested the code in this article with the latest development build of chromium. To enable webcam access (through the webrtc APIs) you need to open chrome to the following page:  And there enable getUserMedia API. Restart the browser to be sure, and you have access to the getUserMedia operation that you can use to access the video stream (and in a later version also an audio stream).   At an interval get a screenshot from the webcam   To calculate the luminance of the room you need to take a snapshot from the video stream so that you can determine how bright it is in the room. This is also something I discussed in the same article, so I won’t go into too much detail. You need to do the following:      Output the videostream to a video element.   Copy a snapshot from the video element to a canvas element.   Get the content of the canvas as a set of bytes for further processing   It isn´t useful to show the video, so create a div with a video element, and use css to hide the div.    &lt;!-- use an hidden video element --&gt; &lt;div style=\"visibility: hidden; height: 0; width: 0\"&gt;     &lt;video id=\"live\" width=\"320\" height=\"240\" autoplay&gt;&lt;/video&gt; &lt;/div&gt;    To connect to the webcam and stream the data to the webcam all you need to do is this:       video = document.getElementById(\"live\")      navigator.webkitGetUserMedia(\"video\",             function (stream) {                 console.log(stream);                 video.src = webkitURL.createObjectURL(stream);             },             function (err) {                 console.log(\"Unable to get video stream!\")             }     )   Now you need to capture a screenshot every couple of seconds. For this you just use the setInterval javascript function, and from there you can use the following to capture the current screen from the video:     // create a dummy context    var ctx = $('&lt;canvas /&gt;', {width:'320', height:'240'})[0].getContext('2d');    ctx.drawImage(video, 0, 0, 320, 240);    var imgd = ctx.getImageData(0, 0, 320, 240);     var pix = imgd.data;   The pix variable will now contain the bytes that make up the image. With this bytearray you can calculate the luminance of the picture.   Calculate the luminance of the room   Calculating the luminance of the picture isn’t that hard. You check the brightness of each pixel and add them. The result is the total luminance of our picture. This function, combined with the capture part now looks like this:        function calculateLuminance(w, h) {          // draw the current image         ctx.drawImage(video, 0, 0, w, h);         var imgd = ctx.getImageData(0, 0, w, h);         var pix = imgd.data;          var totalL = 0;         for (var i = 0, n = pix.length; i &lt; n; i += 4) {             // Red, Green and Blue have different influence on the total luminance             totalL += pix[i  ] * .3 + pix[i + 1] * .59 + pix[i + 2] * .11;         }          return totalL;     }   That leaves us just with the last part. Change the background based on the current luminance.   Adjust the CSS for the background   To increase or decrease the luminance I use the following function (courtesy of stackoverflow):       // http://stackoverflow.com/questions/5560248/programmatically-lighten-or-darken-a-hex-color             function LightenDarkenColor(col, amt) {                 var usePound = false;                 if (col[0] == \"#\") {                     col = col.slice(1);                     usePound = true;                 }                  var num = parseInt(col, 16);                  var r = (num &gt;&gt; 16) + amt;                  if (r &gt; 255) r = 255;                 else if (r &lt; 0) r = 0;                  var b = ((num &gt;&gt; 8) &amp; 0x00FF) + amt;                  if (b &gt; 255) b = 255;                 else if (b &lt; 0) b = 0;                  var g = (num &amp; 0x0000FF) + amt;                  if (g &gt; 255) g = 255;                 else if (g &lt; 0) g = 0;                  return (usePound ? \"#\" : \"\") + (g | (b &lt;&lt; 8) | (r &lt;&lt; 16)).toString(16);             }   This function should be called from the setInterval javascript function which is executed every couple of seconds:       // luminance for dark is around 300.000     var lower = 300000;     // luminance for bright is around 6.000.000, could be different per webcam     var higher = 6400000;      // base color that we change     var baseColor = \"666666\";      timer = setInterval(             function () {                 var luminance = calculateLuminance(320, 240);                 // based on the luminance we need to set the background                 // color to a specific value. We do this by calculating                 // the required target offset                 var offsetFromCenter = ((luminance - lower) / (higher - lower)) * 100;                  // now we can increase the luminance of the background                 var targetColor = LightenDarkenColor(baseColor, offsetFromCenter);                  console.log(luminance);                  $(\"body\").animate({                     backgroundColor:\"#\" + targetColor                 }, 2000);              }, 2000);   And that’s it. Now every two seconds a screenshot is taken, the luminance of that screenshot is calculated, and using jquery animations the background color is adjusted.  ","categories": ["posts","webrtc","html5"],
        "tags": [],
        "url": "http://www.smartjava.org/content/adjust-colors-your-page-based-lighting-room-html5-webrtc-and-webcam/",
        "teaser":null},{
        "title": "Three.js tutorial: example with webgl, canvas and webworkers",
        "excerpt":"In this tutorial we’ll look at how you can use three.js to render a 3D map of an image using webgl on a canvas element. In this example we’ll rasterize an image (make it like an old 8-bit image), and use this rasterized image as input for our 3D model. Each raster element is rendered as a cube using three.js. The height of the cube is defined by the brightness of the raster element. Since an image usually better explains what we’re aiming for, lets look at what we’re going to create:      This article uses a couple of examples from previous articles:      It uses the web worker threadpool shown in this article.   And it rasterizes the image based on info from here.   And brightness is calculated as explained here.   You don’t really need to dive into those articles to learn about three.js, but if you like some background information, those articles are the places to look at. Now, what are we going to show in this article.      Create HTML layout: We'll create a very simple gallery, where you can select the image you want to render. We also need to setup a hidden canvas, we can use for rasterizing.   Initialize the three.js scene: We create a simple three.js scene, with a rotating camera. To this scene we'll add a couple of hundred cubes. One for each part of our rasterized image.   Add the cubes to the scene: When an image is selected we, rasterize the image (in a number of background web worker threads) and based on the brightness add a cube at a specific position to the three.js scene.   Create HTML layout  The HTML is very simple. We just got a couple of divs, include some javascript libraries and style some of the elements. The complete html is shown here:    &lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\"         \"http://www.w3.org/TR/html4/loose.dtd\"&gt; &lt;html&gt; &lt;head&gt;     &lt;title&gt;&lt;/title&gt;     &lt;script type=\"text/javascript\" src=\"libs/jquery-1.7.2.js\"&gt;&lt;/script&gt;     &lt;script type=\"text/javascript\" src=\"libs/three.js\"&gt;&lt;/script&gt;     &lt;script type=\"text/javascript\" src=\"libs/thread-pool.js\"&gt;&lt;/script&gt;     &lt;script type=\"text/javascript\" src=\"js/rasterize.js\"&gt;&lt;/script&gt;     &lt;script type=\"text/javascript\" src=\"js/voxel-image.js\"&gt;&lt;/script&gt;      &lt;style type=\"text/css\"&gt;         .thumb {             /*float: left;*/             margin-top: 10px;             height: 100px;             width: 100px;         }          #webglcontainer {             width: 600px;             height: 480px;             border: solid 1px #CCC;             -moz-box-shadow: 1px 1px 5px #ffffff;             -webkit-box-shadow: 1px 1px 5px #ffffff;             box-shadow: 1px 1px 5px #ffffff;             background-color: #eeeeee;         }          body {             background-color: #000000;         }     &lt;/style&gt; &lt;/head&gt; &lt;body&gt;  &lt;div id=\"target\" style=\"display: inline; visibility: hidden; height:0px; width: 0px;\"&gt;&lt;/div&gt;  &lt;div style=\"margin-left: 130px\"&gt; &lt;img id=\"f1\" src=\"assets/sophie.jpg\" class=\"thumb\" border=\"0\"/&gt; &lt;img id=\"f2\" src=\"assets/mp1.jpg\" class=\"thumb\" border=\"0\"/&gt; &lt;img id=\"f3\" src=\"assets/mp2.jpg\" class=\"thumb\" border=\"0\"/&gt; &lt;/div&gt;  &lt;div id=\"webglcontainer\"&gt;&lt;/div&gt;   &lt;script type=\"text/javascript\"&gt;      $(document).ready(function () {         $(\"#f3\").load(function () {             init();             renderImage($(this));              $(\".thumb\").click(function () {                 renderImage($(this));             });         });     }); &lt;/script&gt; &lt;/body&gt;  &lt;/html&gt;    As you can see from this HTML, everything we do here is rather straightforward. We define a hidden div that we use for rasterizing, a div that contains a couple of images for our gallery and finally a div that is going to be used to render the result in. We also use a simple JQuery $(document) and $(“#f3”) to make sure the document is ready and the image is loaded before we start rendering. Once the first image is loaded, we pass that to renderImage function. This function will rasterize the image and show the output in the webglcontainer div.   Initialize the three.js scene  Before the image can be rendered we first need to correctly setup the scene for three.js. We do this in thie init method.    // some global variables var camera, scene, renderer; var elements = [];  // some default values var bulletSize = 10; var offset = 300; var defPos = 800;  // Initialize the scene and threadpool function init() {      // create a queuepool with 6 queues     queuepool = new Pool(3);     queuepool.init();      // create a scene and a camera     scene = new THREE.Scene();      //Three.PerspectiveCamera()     camera = new THREE.PerspectiveCamera( 55, 1,  0.1, 10000, -2000, 10000 );     // position the camera     camera.position.y = defPos+200;     camera.position.z = defPos;     camera.position.x = defPos;      // and add to the scene     scene.add(camera);      // setup the renderer and attach to canvas     renderer = new THREE.WebGLRenderer();     renderer.setSize( 600, 600 );      $(\"#webglcontainer\").append(renderer.domElement);      animate(); }    In this method we first create a queuepool, this queuepool is used to run jobs. For this example we use jobs to calculate the most dominant color of a specific part of an image (more info can be found in this article).  Next we create the scene, the camera and add the camera to the scene. Finally, in this code fragment, we create the renderer for this scene and append it to the canvas. Now that the scene is created we can render it. This is done in the animate function:    // the animation loop. This rotates the camera around the central point. function animate() {     var timer = Date.now() * 0.0008;     camera.position.x = (Math.cos( timer ) * defPos);     camera.position.z = (Math.sin( timer ) * defPos) ;     camera.lookAt( scene.position );      renderer.render( scene, camera );     requestAnimationFrame( animate ); }    This animate function uses the requestAnimationFrame functionality to get a callback when the animation needs to be updated. We supply the animate method itself as it’s callback, so the animation will keep on running. In this animate function we also rotate the camera around the scene. For this we alter the X and the Z position of the camera, while keeping it focussed on our scene. Without going into the math behind this, if you alternate the X-pos using a Math.cos(t) and the Z-pos simultaniously using Math.sin(t) your camera will smoothly rotate around the scene.  Now we can call the render operation on the renderer to render the scene.   Add the cubes to the scene   What is left is adding the cubes to the scene. We do this in the following function called addElement    // add a cube to the grid. The cube is positioned base on the x,y values. The color // is used to define the material, and the luminance is used for the height of the element. function addElement(x,y, color, lumin) {     var voxelPosition2 = voxelPosition = new THREE.Vector3();     voxelPosition2.x = bulletSize*x -offset ;     voxelPosition2.z = bulletSize*y -offset ;     voxelPosition2.y = 200 + ((lumin/(255))*200)/2;      var geometry = new THREE.CubeGeometry( bulletSize, (lumin/(255))*200, bulletSize );     var mat = new THREE.MeshBasicMaterial( { color: color, shading: THREE.NoShading, wireframe: false, transparent: false })      var cube = new THREE.Mesh(geometry,mat);     cube.position=voxelPosition2;      // add to elements list and to scene     elements.push(cube);     scene.add(cube); }    This function takes as parameters the position of the element, the color in which we need to render the element and the luminance of the element. Based on this information we determine the position where we need to render the cube, we create a cube whose height is based on the luminance, and make a material for this cube based on the most dominant color. With all these parts we can add the cube at the correct position to the scene. And since we already started the animate function, the scene will be updated continuously.   One thing missing we haven’t talked about is what the renderImage operation looks like that we call from the HTML page whenever you click on an image. This function is shown here:    function callback(event) {      var wp = event.data;      // get the colors     var colors = wp.result;      var color = \"0x\" +         (\"0\" + parseInt(colors[0][0],10).toString(16)).slice(-2) +         (\"0\" + parseInt(colors[0][1],10).toString(16)).slice(-2) +         (\"0\" + parseInt(colors[0][2],10).toString(16)).slice(-2);        var lumin = colors[0][0] * .3 + colors[0][1] * .59 + colors[0][2] * .11;       addElement(wp.x,wp.y, color, lumin); }    What we do here, is that we clear the queue and stop any running tasks (this isn’t perfect at the moment, so you might see some cubes from the previous image). Next we remove all the current cubes from the scene and finally we rasterize the selected image. In this rasterize function we split the image in parts. I won’t show the details for rasterizing here, but I’ll show the callback that is called after determining the dominant color (see here for more info on the rasterizing part).       The event we receive here contains information about what the dominant color of a specific part of the image is. We convert this color to the format used by three.js and calculate the luminance of this color. all this information is passed to the addElement function we saw earlier and it is added to the scene.   That’s it. If you run this, you’ll slowly see the scene being filled by colored cubes with different heights like this:      This example was tested using the latest chrome build and the latest firefox beta. I noticed that chrome, even though it was quicker, sometimes crashed, but both browsers should be able to render this.  ","categories": ["posts","html5","web workers","three.js"],
        "tags": [],
        "url": "http://www.smartjava.org/content/threejs-tutorial-example-webgl-canvas-and-webworkers/",
        "teaser":null},{
        "title": "HTML5: Access Battery status through javascript",
        "excerpt":"Update 19-01-2014: Well the adoption rate is very small. Firefox still supports this, but support has still not landed in Chrome (see here: https://code.google.com/p/chromium/issues/detail?id=122593). I updated the article so the example works again for firefox users.   The HTML5 specification is maturing and slowly APIs surface that allow you to access more information from the device the browser is running on. One of the latest submissions is the “Battery Status API”. As the name implies the API allows you to access the battery status through javascript. So you could use this API to disable heavy animations on your web page, warn the user to sync its data, or proactively store the data in local storage. If you want to test this for yourself you can find a working example here. Note that the “Battery time left” property, at least for me, isn’t available when I open the page. It updates after a couple of minutes.   I tested this API with the latest betas of mozilla. And it doesn’t work yet on Mac (does work on iOS, Android and windows). I also didn’t get a good result for the charging time, but discharging and level worked nicely. For this small example we’ll just display information from the API in a simple webpage, like this (screenshot from my tablet):      We’ll look at the following in this article:      How to use the Battery API   Create a couple of text fields to show information from the API   Create a battery image that shows how much power we have left   Use event listeners to update the information   Lets start with a quick look at the API   How to use the Battery API   Using the API is very simple, it has only a couple of properties you can access (from the spec):    [NoInterfaceObject] interface BatteryManager : EventTarget {     readonly attribute boolean   charging;     readonly attribute double    chargingTime;     readonly attribute double    dischargingTime;     readonly attribute double    level; };    The charging property indicates whether we’re connected to a charger, chargingTime returns the amount of time it will take to completely charge the device, the dischargingTime returns the time it will take to approximately run out of power, and the level shows a percentage of how much power you’ve got left. Very straightforward.   Besides these properties the API also defines a couple of callbacks.        [TreatNonCallableAsNull]              attribute Function? onchargingchange;     [TreatNonCallableAsNull]              attribute Function? onchargingtimechange;     [TreatNonCallableAsNull]              attribute Function? ondischargingtimechange;     [TreatNonCallableAsNull]              attribute Function? onlevelchange;    You can register functions for these callbacks that will be called whenever one of the properties changes.   Create a couple of text fields to show information from the API   Lets beging with a couple of simple text fields that show information on the battery. We use the following HTML:    &lt;div id=\"box\"&gt;     &lt;div id=\"battery\"&gt;&lt;/div&gt;     &lt;div id=\"text\"&gt;         &lt;span style=\"display: block;margin-bottom:15px;font-size: xx-large;\"&gt;&lt;strong&gt;Battery             specifications&lt;/strong&gt;&lt;/span&gt;         &lt;span style=\"display: block\" id=\"level\"&gt;Battery level: unknown&lt;/span&gt;         &lt;span style=\"display: block\" id=\"status\"&gt;Charging status: unknown&lt;/span&gt;         &lt;span style=\"display: block\" id=\"charged\"&gt;Battery charged: unknown&lt;/span&gt;     &lt;/div&gt; &lt;/div&gt;    And to make sure they have the correct battery value, we fill these with the following javascript:        // get the battery information     var battery = navigator.battery || navigator.webkitBattery || navigator.mozBattery;      // get the battery information to be displayed     $('#level').text(\"Battery level: \" + Math.round(battery.level * 100) + \"%\");     $('#status').text(\"Charging status: \" + ((battery.charging) ? \"true\" : \"false\"));     if (battery.charging) {         $('#charged').text(\"Battery time to charge: \" + battery.chargingTime);     } else {         $('#charged').text(\"Battery time left: \" + (Math.round(battery.dischargingTime / 60)) + \" minutes\");     }    As you can see in the code, couldn’t be much simpler. We also show an image that reflects the level.   Create a battery image that shows how much power we have left   I won’t go into the details since that isn’t that interesting. If you want to see the details look at the  source code from the example . For this example I created a simple battery object (based on example from nokia) and with an updateBattery call I can set how full the battery is. To initialize this, use the following:      var b = new Battery(\"assets/bat_empty.png\", \"assets/bat_full.png\", 96, 168);     $(\"#battery\").append(b.domElement);     b.updateBattery(battery.level * 100);    Use event listeners to update the information   And finally we add a couple of event listeners that are called whenever one of the properties of the battery changes:       // when the loader is connected     battery.addEventListener(\"chargingchange\", function (e) {         $('#status').text(\"Charging status: \" + ((battery.charging) ? \"true\" : \"false\"));     }, false);      // when charging time changes update the time to charge / time left     battery.addEventListener(\"chargingtimechange\", function (e) {         if (battery.charging) {             $('#charged').text(\"Battery time to charge: \" + battery.chargingTime);         } else {             $('#charged').text(\"Battery time left: \" + (Math.round(battery.dischargingTime / 60)) + \" minutes\");         }      }, false);      // when dischargingtime changes update the time to charge / time left     battery.addEventListener(\"dischargingtimechange\", function (e) {         if (battery.charging) {             $('#charged').text(\"Battery time to charge: \" + (Math.round(battery.dischargingTime / 60)) + \" minutes\");         } else {             $('#charged').text(\"Battery time left: \" + (Math.round(battery.dischargingTime / 60)) + \" minutes\");         }     }, false);      // listener that is notified when the level changes     battery.addEventListener(\"levelchange\", function (e) {         $('#level').text(\"Battery level: \" + Math.round(battery.level * 100) + \"%\");         b.updateBattery(100 * battery.level)     }, false);    Easy isn’t it? The really good thing about this, is that this works across devices. On my mobile phone it looks like this:      And on windows (running in a VM) it shows me this:     ","categories": ["posts","javascript","html5"],
        "tags": [],
        "url": "http://www.smartjava.org/content/html5-access-battery-status-through-javascript/",
        "teaser":null},{
        "title": "Compress PDF in Mac using command line (for free)",
        "excerpt":"Compressing a PDF in Mac is very easy and doesn’t require any of the commercial tools out there. This isn’t an area I normally blog about, but when I needed this, I couldn’t find any useful information on how to do this on a Mac. All you have to do is install Macports  and from there install ghostscript:    jos@Joss-MacBook-Pro.local:~$ sudo port install ghostscript ... ---&gt;  Activating ghostscript @9.05_1 ---&gt;  Cleaning ghostscript    You’ll see a whole lot of text scrolling by, but at the end you’ll have an installed version of ghostscript. Now you can easily compress PDFs by just using the following command:    jos@Joss-MacBook-Pro.local:~$ gs -sDEVICE=pdfwrite -dCompatibilityLevel=1.4                      -dPDFSETTINGS=/ebook -dNOPAUSE                      -dQUIET -dBATCH                      -sOutputFile=/Users/jos/Desktop/scan-1-small.pdf /Users/jos/Desktop/scan-1.pdf    This will convert the file “scan-1.pdf” to “scan-1-small.pdf”. In this example this reduces the file size from 8.6MB to 600KB. You can define various quality settings by using a different value for the -dPDFSETTINGS option. From the ghostscript documentation you have the following options:    /screen selects low-resolution output similar to the Acrobat Distiller \"Screen Optimized\" setting. /ebook selects medium-resolution output similar to the Acrobat Distiller \"eBook\" setting. /printer selects output similar to the Acrobat Distiller \"Print Optimized\" setting. /prepress selects output similar to Acrobat Distiller \"Prepress Optimized\" setting. /default selects output intended to be useful across a wide variety of uses,              possibly at the expense of a larger output file.    So you see, you don’t need expensive commercial tools, just to resize a PDF, just use open source.  ","categories": ["posts","open source","mac"],
        "tags": [],
        "url": "http://www.smartjava.org/content/compress-pdf-mac-using-command-line-free/",
        "teaser":null},{
        "title": "HTML5: Web intents to share information between web apps",
        "excerpt":"If you’ve read any of my other articles you’re probably aware that I really like this whole “browser as the platform” concept. We can easily communicate with backend services using web sockets, parallelize javascript with web workers and access a lot of device capabilities such as web camera, battery and most of the acceleration and location sensors. One of the things, though, that was missing, was a way to easily share information between web apps.   Most web applications such as twitter, dropbox, facebook, google+ have their own custom APIs, and if you want to share information with these web applications, or make use of the services they provide (e.g. store a document in dropbox) you have to use those APIs. The share button used on this site for instance makes use of all the services specific APIs to share information. If one of the interfaces should change, a new version must be created. On android there is something called intents, which is a standard way of apps to share information that avoids all these problems.      Instead of requiring tight integration the application that wants to share something just asks the system which apps can fulfill the request. The system returns with a list of apps the user can select and that’s it. The data is sent exchanged between the apps, without requiring custom integration.   Wouldn’t it be nice if we could have something like that for web apps? No more need to support 30+ different bookmarking sites, 20+ social networks and 15+ picture websites. Just one simple way to exchange information in a standard manner. Well, we can. Or better said, we’ll be able to do this soon across browsers. Web intents is a new W3C specification proposed by google, to bring the goodness of Android intents to web applications.      In this tutorial I’ll show you an example how to do this. I’ve also put the demo online here if you want to test for yourself. Before this will work however, you need to add a chrome extension that registers for this intent (more information in the rest of the article). If you download, extract and install this archive in chrome the example will work. How to do this is shown at the end of this article.   SInce this is still a very early draft there isn’t much support yet amongst the various browsers. The spec was drafted by Google, so as you can expect Google chrome at least has support for this specification (I’ve tested this with version 21.0.1163.0 dev). In this example we’ll take the following steps:      Make a (very simple) website that shows a set of links you can share (Reddit, Digg or Dzone like)   Create a link sharing service that forwards this link to twitter   Add this service to chrome, so that it can be shown in the chrome \"intents\" user interface   But first we look at the website that wants to share information with other websites.   Make a (very simple) website that shows a set of links you can share (Reddit, Digg or Dzone like)   This is just a very simple static site, that lists a set of links. At the end of each link is a share button that the users can use to share this link using any of the configured web applications. This (very ugly) site looks like this:      Nothing to complex, just a simple list of links with a share button.  This share button is where it gets interesting. Lets look at this button, and the associated javascript.    &lt;li&gt;  &lt;p&gt;   &lt;a href=\"http://www.smartjava.org/content/html5-access-battery-status-through-javascript\"&gt;HTML5: Access Battery status through javascript&lt;/a&gt;    &lt;img class=\"share\" src=\"assets/share-this-icon.png\" alt=\"share\" height=\"20\"&gt;  &lt;/p&gt; &lt;/li&gt;  ...  &lt;script type=\"text/javascript\"&gt;      // whenever the share button is clicked     $(\".share\").click(function() {          // get the information we want to share, the url         var linkToShare = $(this).prev().attr('href');          // next we can define the webintent,         var params = {             'action': 'http://webintents.org/share',             'type': 'text/uri-list',             'data': linkToShare         };          // create the intent         var intent = new WebKitIntent(params);          // start the intent, and pass in the callback         // that is called on succes.         window.navigator.webkitStartActivity(intent, function(data) {             $(\"#callback\").text(\"Received from invoked intent: \" + data);         });     }); &lt;/script&gt;    As you can see, the button is just an image. We use jQuery to add an onClick listener. This onClick function is where we create the intent that we want to start. In this case it’s the share intent, and as the data we provide the url of the link. There are many more intents available, and you’re free to use your own. It’s however a good idea to use those from webintents.org, since those are standard and used by many other applications. In this small code fragment we also add a callback when the intent as successful. In this example we just show the result in the grey box at the bottom. This is all you have to do at the client side. Just specify the type of intent you want, and let the platform handle the rest.   Create a link sharing service that forwards this link to twitter   The other side of web intents are services that can handle these intents. In this example we create a simple service that forwards the URL to twitter to be shared.      We’ll only look at the javascript, since the rest isn’t really interesting.        var intent = window.webkitIntent;      var urlToShare = intent.data;     var twitterURL = \"http://twitter.com/intent/tweet\"                      + \"?via=josdirksen\"                      + \"&amp;text=Look at this! - \" + urlToShare + \" - Shared using web intents demo from http://www.smartjava.org\";      // set the correct link on the url     $(\"#twitterlink\").attr(\"href\", twitterURL);      // register twitter callback     $.getScript(\"http://platform.twitter.com/widgets.js\", function () {         function handleTweetEvent(event) {             // this event is called when the tweet has been sent.             // this means we can send the result back to the client.             intent.postResult(\"Link successfully sent to twitter!\");         }          twttr.events.bind('tweet', handleTweetEvent);     });      $(\"#faketwitter\").click(function() {         intent.postResult(\"Link successfully faked to twitter!\");     });    Here you can see that we create an url based on the data from the intent. From our client we sent an url, which we use to create an URL that we can use to directly post to twitter. After we’ve created the twitter url, we register the callback from twitter. So as soon as the tweet is sent, the callback is called, and we can pass the result back to our client.      I also added a fake twitter link, for those of you who want to test the scenario, but don’t want to actually post anything on twitter :)   So far we created our client, and a service. But how does chrome now of the existence of our service? What chrome does now is that it checks whether there are services registered in the chrome store that can handle the intent from our client, and it checks whether extensions are installed that can handle the intent. Since we don’t want to submit apps to the store just to test web intents, we’ll create a simple extension that we install locally.   Add this service to chrome, so that it can be shown in the chrome \"intents\" user interface   To create a minimal chrome extension we need an icon (that is shown in the application tab) and we need a manifest.json that describes what the extension does, and what intents it can service. The one I used is shown here:    {     \"name\":\"Smartjava Twitter share\",     \"version\":\"4\",     \"icons\":{         \"128\":\"share-this-icon.png\"     },     \"app\":{         \"urls\":[             \"http://www.smartjava.org\"         ],         \"launch\":{             \"web_url\":\"http://www.smartjava.org\"         }     },     \"manifest_version\":3,     \"intents\":{         \"http://webintents.org/share\":[             {                 \"type\":[\"text/uri-list\"],                 \"href\":\"http://www.smartjava.org/examples/webintents/provider\",                 \"disposition\":\"window\",                 \"title\":\"Share this with twitter\"             }         ]     } }    I won’t go into the details of the manifest, since there are many articles that explain how to write chrome extensions. What is interesting here is that we can now add an “intents” element. This element specifies the type of intent we’re interested in. So with this configuration we tell the platform that we can process “http://webintents.org/share” intents and know how to handle text/uri-list types. When we receive such an intent we open the supplied href, which points to the web application we discussed earlier.   Now all we need to do is register this in your browser. You can simply do this by using the “load unpacked extension…” functionality from chrome (in developers mode):      And that’s it. We can now use the chrome web intents functionality to share information just like we do on android devices.     ","categories": ["posts","chrome","web intents","html5"],
        "tags": [],
        "url": "http://www.smartjava.org/content/html5-web-intents-share-information-between-web-apps/",
        "teaser":null},{
        "title": "HTML5 geolocation API to measure speed and heading of your car",
        "excerpt":"In this article we’ll show you how you can use the W3C geolocation API to measure the speed and the heading of your car while driving. This article further uses SVG to render the speed gauge and heading compass.  What we’ll create in this article is the following:      Here you can see two gauges. One will show the heading you’re driving to, and the other shows the speed in kilometers. You can test this out yourself by using the following link: Open this in GPS enable device. Once opened the browser will probably ask you to allow access to your location. If you enable this and start moving, you’ll see the two gauges move appropriately.   Getting all this to work is actually very easy and consists of the following steps:      Alter the SVG images so we can rotate the needle and add to page.   Use the geolocation API to determine the current speed and heading   Update the needle based on the current and previous value   We’ll start with the SVG part.   Alter the SVG images so we can rotate the needle and add to page   For the images I decided to use SVG. SVG has the advantage that it can scale without losing detail, and you can easily manipulate and animate the various parts of a SVG image. Both the SVG images were copied from openclipart.org:      Compass rose   Speedometer   These are vector graphics, both created using illustrator. Before we can rotate the needles in these images we need to make a couple of small changes to the SVG code. With SVG you can apply matrix transformations to each SVG element, with this you can easily rotate, skew, scale or translate a component. Besides the matrix transformation you can also apply the rotation and translation directly using the translate and rotate keywords. In this example I’ve used the translaten and rotate functions directly.   When working with these functions you have to take into account that the rotate function doesn’t rotate around the center of the component, it rotates around point 0,0. So we need to make sure that for our needles the point we want to rotate around is set at 0,0. Without diving into too much details, I removed the two needles from the image, and added them as a seperate group to the svg image. I then made sure the needles we’re drawn relative to the 0,0 point I wanted to rotate around. For the speedometer the needle is now defined as this:    &lt;g transform=\"translate(171,157) rotate(45)\" id=\"speed\"&gt;    &lt;rect        y=\"0\"        x=\"-2.5\"        height=\"100\"        width=\"5\"        id=\"rect5532\"        style=\"fill:url(#linearGradient5547);fill-opacity:1;stroke:none\"        /&gt;    &lt;/g&gt;    And for the compass the needle is defined like this:    &lt;path          transform=\"translate(225,231) rotate(135)\"          sodipodi:nodetypes=\"ccccc\"          id=\"compass\"          d=\"M -4.21,88             L -4.21,-88             C -2.77,-72               -1.93,-35               4.21,-20              L -4.21,-21              L -4.21,88 z\"          style=\"fill:#000000;fill-rule:evenodd;stroke:#000000;stroke-width:2;stroke-linecap:round;stroke-linejoin:round;                      stroke-miterlimit:4;stroke-dasharray:none;stroke-opacity:1\" /&gt;    If you know how to read SVG, you can see that these figures are now drawn around their rotation point (the bottom center for the speedomoter and the center for the compass). As you can see we also added a specific id for both these elements. This way we can reference them directly from our javascript later on and update the transform property from a jquery animation.  Next we just need to add these to the page. For this I used d3.js, which has all kinds of helper functions for SVG and which you can use to load these elements like this:   function loadGraphics() {     d3.xml(\"assets/compass.svg\", \"image/svg+xml\", function(xml) {         document.body.appendChild(xml.documentElement);     });      d3.xml(\"assets/speed.svg\", \"image/svg+xml\", function(xml) {         document.body.appendChild(xml.documentElement);     });  }   And with this we’ve got our visualization components ready.   Use the geolocation API to determine the current speed and heading   The next step is using the geolocation API to access the speed and heading properties. You can get this information from the position object that is provided to you by this API:    interface Position {     readonly attribute Coordinates coords;     readonly attribute DOMTimeStamp timestamp;   };    This object has a Coordinate object that contains the information we’re looking for:    interface Coordinates {     readonly attribute double latitude;     readonly attribute double longitude;     readonly attribute double? altitude;     readonly attribute double accuracy;     readonly attribute double? altitudeAccuracy;     readonly attribute double? heading;     readonly attribute double? speed;   };    A lot of useful attributes, but we’re only interested in these last two. The heading (from 0 to 360) shows the direction we’re moving in, and the speed in meters per second is, as you’ve probably guessed, the speed we’re moving at.  There are two different options to get these values. We can poll ourselves for these values (e.g. setInterval) or we can wait wacth our position. In this second case you automatically recieve an update. In this example we use the second approach:   function initGeo() {     navigator.geolocation.watchPosition(         geosuccess,         geofailure,         {             enableHighAccuracy:true,             maximumAge:30000,             timeout:20000         }     );      //moveSpeed(30);     //moveCompassNeedle(56); }  var count = 0; function geosuccess(event) {      $(\"#debugoutput\").text(\"geosuccess: \" + count++ + \" : \" + event.coords.heading + \":\" + event.coords.speed);      var heading = event.coords.heading;     var speed = event.coords.speed;      if (heading != null &amp;&amp; speed !=null &amp;&amp; speed &gt; 0) {        moveCompassNeedle(heading);     }      if (speed != null) {         // update the speed         moveSpeed(speed);     } }   With this piece of code, we register a callback function on the watchPosition. We also add a couple of properties to the watchPosition function. With these properties we tell the API to use GPS (enableHighAccuracy) and set some timeout and caching values. Whenever we receive an update from the API the geosuccess function is called. This function recieves a position object (shown earlier) that we use to access the speed and the heading. Based on the value of the heading and the speed we update the compass and the speedomoter.   Update the needle based on the current and previous value   To update the needles we use jquery animations for the easing. Normally you use a jquery animation to animatie css properties of an object, but you can also use this to animate arbitrary properties. To animate the speedomoter we use the following:    var currentSpeed = {property: 0}; function moveSpeed(speed) {      // we use a svg transform to move to correct orientation and location     var translateValue  = \"translate(171,157)\";     // to is in the range of 45 to 315, which is 0 to 260 km     var to = {property: Math.round((speed*3.6/250) *270) + 45};      // stop the current animation and run to the new one     $(currentSpeed).stop().animate(to, {         duration: 2000,         step: function() {             $(\"#speed\").attr(\"transform\", translateValue                 + \" rotate(\" + this.property + \")\")         }     }); }   We create a custom object, currentSpeed, with a single property. This property is set to the rotate ratio that reflects the current speed. Next, this property is used in a jquery animation. Note that we stop any existing animations, should we get an update when the current animation is still running. In the step property of the animation we set the transfrom value of the SVG element. This will rotate the needle, in two seconds, from the old value to the new value.   And to animate the compass we do pretty much the same thing:   var currentCompassPosition =  {property: 0}; function moveCompassNeedle(heading) {      // we use a svg transform to move to correct orientation and location     var translateValue = \"translate(225,231)\";     var to = {property: heading};      // stop the current animation and run to the new one     $(currentCompassPosition).stop().animate(to, {         duration: 2000,         step: function() {             $(\"#compass\").attr(\"transform\", translateValue                 + \" rotate(\" + this.property + \")\")         }     }); }   There is a  smal bug I ran into with this setup. Sometimes my phone lost its GPS signal (running firefox mobile), and that stopped the dials moving. Refreshing the webpage was enough to get things started again however. I might change this to actively pull the information using the getCurrentLocation API call, to see whether that works better.   Another issue is that there is no way, at least that I found, for you to disable the phone entering sleep mode from the browser. So unless you configure your phone to not go to sleep, the screen will go black.  ","categories": ["posts","d3.js","svg","geolocation","html5"],
        "tags": [],
        "url": "http://www.smartjava.org/content/html5-geolocation-api-measure-speed-and-heading-your-car/",
        "teaser":null},{
        "title": "Create complex Word (.docx) documents programatically with docx4j",
        "excerpt":"A couple of months ago I needed to create a dynamic Word document with a number of tables and paragraphs. In the past I’ve used POI for this, but I’ve found this hard to use and it doesn’t work that well for me when creating more complex documents. So for this project, after some searching around, I decided to use docx4j. Docx4j, according to their site is a:   \"docx4j is a Java library for creating and manipulating Microsoft Open XML (Word docx, Powerpoint pptx, and Excel xlsx) files. It is similar to Microsoft's OpenXML SDK, but for Java. \"  In this article I’ll show you a couple of examples you can use to generate content for word documents. More specifically we’ll look at the following two examples:      Load in a template word document to add content to and save as new document   Add paragraphs to this template document   Add tables to this template document   The general approach here is to first create a Word document that contains the layout and main styles of your final document. In this document you’ll need to add placeholders (simple strings) that we’ll use to search for and replace with real content.   A very basic template for instance looks like this:    In this article we’ll show you how you can fill this so get this:    Load in a template word document to add content to and save as new document   First things first. Lets create a simple word document that we can use as a template. For this just open Word, create a new document and save it as template.docx. This is the word template we’ll use to add  content to. The first thing we need to do is load this document with docx4j. You can this with the following piece of java code:   \tprivate WordprocessingMLPackage getTemplate(String name) throws Docx4JException, FileNotFoundException { \t\tWordprocessingMLPackage template = WordprocessingMLPackage.load(new FileInputStream(new File(name))); \t\treturn template; \t}   This will return a java object representing the complete (at this moment) empty document. We can now use the Docx4J API to add, delete and modify content in this word document. Docx4J has a number of helper classes you can use to traverse through this document. I did write a couple of helpers myself though that make it really easy to find the specific placeholders and replace them with the real content. Lets look at one of them. This operation is a wrapper around a couple of JAXB operations that allows you to search through a specific element and all it’s children for a certain class. You can for instance use this to get all the tables in the document, all the rows within a table and more like that.   \tprivate static List&lt;Object&gt; getAllElementFromObject(Object obj, Class&lt;?&gt; toSearch) { \t\tList&lt;Object&gt; result = new ArrayList&lt;Object&gt;(); \t\tif (obj instanceof JAXBElement) obj = ((JAXBElement&lt;?&gt;) obj).getValue(); \t\t \t\tif (obj.getClass().equals(toSearch)) \t\t\tresult.add(obj); \t\telse if (obj instanceof ContentAccessor) { \t\t\tList&lt;?&gt; children = ((ContentAccessor) obj).getContent(); \t\t\tfor (Object child : children) { \t\t\t\tresult.addAll(getAllElementFromObject(child, toSearch)); \t\t\t}  \t\t} \t\treturn result; \t}   Nothing to complex, but really helpful. Lets see how we can use this operation. For this example we’ll just replace a simple text placeholder with a different value. This is for instance something you’d use to dynamically set the title of a document. First though, add a custom placeholder in the word template you created. I’ll use SJ_EX1 for this. We’ll replace this value with our name. The basic text elements in a docx4j are represented by the org.docx4j.wml.Text class. To replace this simple placeholder all we have to do is call this method:   \tprivate void replacePlaceholder(WordprocessingMLPackage template, String name, String placeholder ) { \t\tList&lt;Object&gt; texts = getAllElementFromObject(template.getMainDocumentPart(), Text.class);  \t\tfor (Object text : texts) { \t\t\tText textElement = (Text) text; \t\t\tif (textElement.getValue().equals(placeholder)) { \t\t\t\ttextElement.setValue(name); \t\t\t} \t\t} \t}   This will look for all the Text elements in the document, and those that match are replaced with the value we specify. Now all we need to do is write the document back to a file.   \tprivate void writeDocxToStream(WordprocessingMLPackage template, String target) throws IOException, Docx4JException { \t\tFile f = new File(target); \t\ttemplate.save(f); \t}   Not that hard as you can see.   With this setup we can also add more complex content to our word documents. The easiest way to determine how to add specific content is by looking at the XML source code of the word document. That’ll tell you which wrappers are needed and how Word marshalls the XML. For the next example we’ll look at how to add a complete paragraph.   Add paragraphs to this template document   You might wonder why we need to be able to add paragraphs? We can already add text, and isn’t a paragraph just a large piece of text? Well, yes and no. A paragraph indeed looks like a big piece of text, but what you need to take into account are the linebreaks. If you add a Text element, like we did earlier, and add linebreaks to the text, they won’t show up. When you want linebreaks, you’ll need to create a new paragraph. Luckily, though, this is also very easy to do with Docx4j. We’ll do this by taking the following steps:      Find the paragraph to replace from the template   Split the input text into seperate lines   For each line create a new paragraph based on the paragraph from the template   Remove the original paragraph   Shouldn’t be to hard with the helper methods we already have.   \tprivate void replaceParagraph(String placeholder, String textToAdd, WordprocessingMLPackage template, ContentAccessor addTo) { \t\t// 1. get the paragraph \t\tList&lt;Object&gt; paragraphs = getAllElementFromObject(template.getMainDocumentPart(), P.class);  \t\tP toReplace = null; \t\tfor (Object p : paragraphs) { \t\t\tList&lt;Object&gt; texts = getAllElementFromObject(p, Text.class); \t\t\tfor (Object t : texts) { \t\t\t\tText content = (Text) t; \t\t\t\tif (content.getValue().equals(placeholder)) { \t\t\t\t\ttoReplace = (P) p; \t\t\t\t\tbreak; \t\t\t\t} \t\t\t} \t\t} \t\t \t\t// we now have the paragraph that contains our placeholder: toReplace \t\t// 2. split into seperate lines \t\tString as[] = StringUtils.splitPreserveAllTokens(textToAdd, '\\n');  \t\tfor (int i = 0; i &lt; as.length; i++) { \t\t\tString ptext = as[i]; \t\t\t \t\t\t// 3. copy the found paragraph to keep styling correct \t\t\tP copy = (P) XmlUtils.deepCopy(toReplace); \t\t\t \t\t\t// replace the text elements from the copy \t\t\tList&lt;?&gt; texts = getAllElementFromObject(copy, Text.class); \t\t\tif (texts.size() &gt; 0) { \t\t\t\tText textToReplace = (Text) texts.get(0); \t\t\t\ttextToReplace.setValue(ptext); \t\t\t} \t\t\t \t\t\t// add the paragraph to the document \t\t\taddTo.getContent().add(copy); \t\t} \t\t \t\t// 4. remove the original one \t\t((ContentAccessor)toReplace.getParent()).getContent().remove(toReplace); \t\t \t}   In this method we replace the content of a paragraph with the supplied text and then new paragraphs to the argument specified with addTo.   \t\tString placeholder = \"SJ_EX1\"; \t\tString toAdd = \"jos\\ndirksen\"; \t\t \t\treplaceParagraph(placeholder, toAdd, template, template.getMainDocumentPart());   If you run this with more content in your word template you’ll notice that the paragraphs will appear at the bottom of your document. The reason is that the paragraphs are added back to the main document. If you want your paragraphs to be added at a specific place in your document (which is something you usually want) you can wrap them in a 1x1 borderless table. This table is than seen as the parent of the paragraph and new paragraphs can be added there.   Add tables to this template document   The final example I’d like to show is how to add tables to a word template. A better description actually would be, how you can fill predefined tables in your word template. Just as we did for simple text and paragraphs, we’ll replace placeholders. For this example add a simple table to your word document (which you can style as you like). To this table add 1 dummy row that serves as template for the content. In the code we’ll look for that row, copy it, and replace the content with new rows from java code like this:      find the table that contains one of our keywords   copy the row that serves as row template   for each row of data add a row to the table based on the row template   remove the original template row   The same approach as we’ve also shown for the paragraphs. First though lets look at how we’ll provide the replacement data. For this example I just supply a set of hashmaps that contain the name of the placeholder to replace and the value to replace it with. I also provide the replacement tokens that can be found in the table row.                  Map&lt;String,String&gt; repl1 = new HashMap&lt;String, String&gt;(); \t\trepl1.put(\"SJ_FUNCTION\", \"function1\"); \t\trepl1.put(\"SJ_DESC\", \"desc1\"); \t\trepl1.put(\"SJ_PERIOD\", \"period1\");  \t\tMap&lt;String,String&gt; repl2 = new HashMap&lt;String, String&gt;(); \t\trepl2.put(\"SJ_FUNCTION\", \"function2\"); \t\trepl2.put(\"SJ_DESC\", \"desc2\"); \t\trepl2.put(\"SJ_PERIOD\", \"period2\"); \t\t \t\tMap&lt;String,String&gt; repl3 = new HashMap&lt;String, String&gt;(); \t\trepl3.put(\"SJ_FUNCTION\", \"function3\"); \t\trepl3.put(\"SJ_DESC\", \"desc3\"); \t\trepl3.put(\"SJ_PERIOD\", \"period3\"); \t\t \t\treplaceTable(new String[]{\"SJ_FUNCTION\",\"SJ_DESC\",\"SJ_PERIOD\"}, Arrays.asList(repl1,repl2,repl3), template);   Now what does this replaceTable method look like.   \tprivate void replaceTable(String[] placeholders, List&lt;Map&lt;String, String&gt;&gt; textToAdd, \t\t\tWordprocessingMLPackage template) throws Docx4JException, JAXBException { \t\tList&lt;Object&gt; tables = getAllElementFromObject(template.getMainDocumentPart(), Tbl.class);  \t\t// 1. find the table \t\tTbl tempTable = getTemplateTable(tables, placeholders[0]); \t\tList&lt;Object&gt; rows = getAllElementFromObject(tempTable, Tr.class);  \t\t// first row is header, second row is content \t\tif (rows.size() == 2) { \t\t\t// this is our template row \t\t\tTr templateRow = (Tr) rows.get(1);  \t\t\tfor (Map&lt;String, String&gt; replacements : textToAdd) { \t\t\t\t// 2 and 3 are done in this method \t\t\t\taddRowToTable(tempTable, templateRow, replacements); \t\t\t}  \t\t\t// 4. remove the template row \t\t\ttempTable.getContent().remove(templateRow); \t\t} \t}   This method finds the table, gets the first row and for each supplied map it add a new row to the table. Before returning it removes the template row. This method uses two helpers: addRowToTable and getTemplateTable. We’ll first look at this last one:   \tprivate Tbl getTemplateTable(List&lt;Object&gt; tables, String templateKey) throws Docx4JException, JAXBException { \t\tfor (Iterator&lt;Object&gt; iterator = tables.iterator(); iterator.hasNext();) { \t\t\tObject tbl = iterator.next(); \t\t\tList&lt;?&gt; textElements = getAllElementFromObject(tbl, Text.class); \t\t\tfor (Object text : textElements) { \t\t\t\tText textElement = (Text) text; \t\t\t\tif (textElement.getValue() != null &amp;&amp; textElement.getValue().equals(templateKey)) \t\t\t\t\treturn (Tbl) tbl; \t\t\t} \t\t} \t\treturn null; \t}   This function just looks whether a table contains one of our placeholders. If so that table is returned. The addRowToTable operation is also very simple.   \tprivate static void addRowToTable(Tbl reviewtable, Tr templateRow, Map&lt;String, String&gt; replacements) { \t\tTr workingRow = (Tr) XmlUtils.deepCopy(templateRow); \t\tList&lt;?&gt; textElements = getAllElementFromObject(workingRow, Text.class); \t\tfor (Object object : textElements) { \t\t\tText text = (Text) object; \t\t\tString replacementValue = (String) replacements.get(text.getValue()); \t\t\tif (replacementValue != null) \t\t\t\ttext.setValue(replacementValue); \t\t}  \t\treviewtable.getContent().add(workingRow); \t}   This method copies our template and replaces the placeholders in this template row with the provided values. This copy is added to the table. And that’s it. With this piece of code we can fill arbitrairy tables in our word document, while preserving table layout and styling.   That’s it so far for this article. With paragraphs and tables you can create many different types of documents and this nicely matches the type of documents that are most often generated. This same approach though can also be used to add other type of content to word documents.  ","categories": ["posts","word","java"],
        "tags": [],
        "url": "http://www.smartjava.org/content/create-complex-word-docx-documents-programatically-docx4j/",
        "teaser":null},{
        "title": "Render geographic information in 3D with Three.js and D3.js",
        "excerpt":"The last couple of days I’ve been playing around with three.js and geo information. I wanted to be able to render map/geo data (e.g. in geojson format) inside the three.js scene. That way I have another dimension I could use to show a specific metric instead of just using the color in a 2D map. In this article I’ll show you how you can do this. The example we’ll create shows a 3D map of the Netherlands, rendered in Three.js, that uses a color to indicate the population density per municipality and the height of each municipality represents the actual number of residents.      Or if you can look at a working example.   This information is based on open data available from the Dutch government. If you look at the source from the example, you can see the json we use for this. For more information on geojson and how to parse it see the other articles I did on this subject:      Using d3.js to visualize GIS   Election site part 1: Basics with Knockout.js, Bootstrap and d3.js   To get this working we’ll take the following steps:      Load the input geo data   Setup a three.js scene   Convert the input data to a Three.js path using d3.js   Set the color and height of the Three.js object   Render everything   Just a reminder to see everything working, just look at the example.   Load the input geo data   D3.js has support to load json and directly transform it to an SVG path. Though this is a convenient way, I only needed the path data, not the complete SVG elements. So to load json I just used jquery’s json support.       // get the data     jQuery.getJSON('data/cities.json', function(data, textStatus, jqXHR) {     ..    });   This will load the data and pass it in the data object to the supplied function.   Setup a three.js scene   Before we do anything with the data lets first setup a basic Three.js scene.   // Set up the three.js scene. This is the most basic setup without         // any special stuff         function initScene() {             // set the scene size             var WIDTH = 600, HEIGHT = 600;              // set some camera attributes             var VIEW_ANGLE = 45, ASPECT = WIDTH / HEIGHT, NEAR = 0.1, FAR = 10000;              // create a WebGL renderer, camera, and a scene             renderer = new THREE.WebGLRenderer({antialias:true});             camera = new THREE.PerspectiveCamera(VIEW_ANGLE, ASPECT,                                                   NEAR, FAR);             scene = new THREE.Scene();              // add and position the camera at a fixed position             scene.add(camera);             camera.position.z = 550;             camera.position.x = 0;             camera.position.y = 550;             camera.lookAt( scene.position );              // start the renderer, and black background             renderer.setSize(WIDTH, HEIGHT);             renderer.setClearColor(0x000);              // add the render target to the page             $(\"#chart\").append(renderer.domElement);              // add a light at a specific position             var pointLight = new THREE.PointLight(0xFFFFFF);             scene.add(pointLight);             pointLight.position.x = 800;             pointLight.position.y = 800;             pointLight.position.z = 800;              // add a base plane on which we'll render our map             var planeGeo = new THREE.PlaneGeometry(10000, 10000, 10, 10);             var planeMat = new THREE.MeshLambertMaterial({color: 0x666699});             var plane = new THREE.Mesh(planeGeo, planeMat);              // rotate it to correct position             plane.rotation.x = -Math.PI/2;             scene.add(plane);         }   Nothing to special, the comments inline should nicely explain what we’re doing here. Next it gets more interesting.   Convert the input data to a Three.js path using d3.js   What we need to do next is convert our geojson input format to a THREE.Path that we can use in our scene. Three.js itself doesn’t support geojson or SVG for that matter. Luckily though someone already started work on integrating d3.js with three.js. This project is called “d3-threeD” (sources can be found on github here). With this extension you can automagically render SVG elements in 3D directly from D3.js. Cool stuff, but it didn’t allow me any control over how the elements were rendered. It does however contain a function we can use for our scenario. If you look through the source code of this project you’ll find a method called “transformSVGPath”. This method converts an SVG path string to a Three.Shape element. Unfortunately this method isn’t exposed, but that’s quickly solved by adding this to the d3-threeD.js file:   // at the top var transformSVGPathExposed; ... // within the d3threeD(exports) function transformSVGPathExposed = transformSVGPath;  &lt;/javscript&gt;   This way we can call this method separately. Now that we have a way to transform an SVG path to a Three.js shape, we only need to convert the geojson to an SVG string and pass it to this function. We can use the geo functionaly from D3.js for this:   geons.geoConfig = function() {     this.TRANSLATE_0 = appConstants.TRANSLATE_0;     this.TRANSLATE_1 = appConstants.TRANSLATE_1;     this.SCALE = appConstants.SCALE;   this.mercator = d3.geo.mercator(); this.path = d3.geo.path().projection(this.mercator);  this.setupGeo = function() {     var translate = this.mercator.translate();     translate[0] = this.TRANSLATE_0;     translate[1] = this.TRANSLATE_1;      this.mercator.translate(translate);     this.mercator.scale(this.SCALE); } } ```   The path variable from the previous piece of code can now be used like this:   var feature = geo.path(geoFeature);   To convert a geojson element to an SVG path. So how does this look combined?         // add the loaded gis object (in geojson format) to the map       function addGeoObject() {           // keep track of rendered objects           var meshes = [];           ...                     // convert to mesh and calculate values           for (var i = 0 ; i &lt; data.features.length ; i++) {               var geoFeature = data.features[i]               var feature = geo.path(geoFeature);               // we only need to convert it to a three.js path               var mesh = transformSVGPathExposed(feature);               // add to array               meshes.push(mesh);               ...       }   As you can see we iterate over the data.features list (this contains all the geojson representations of the municipalities). Each municipality is converted to an svg string, and each svg string is converted to a mesh. This mesh is a Three.js object that we can render on the scene.   Set the color and height of the Three.js object   Now we just need to set the height and the color of the Three.js shape and add it to the scene. The extended addGeoObject method now looks like this:        // add the loaded gis object (in geojson format) to the map       function addGeoObject() {           // keep track of rendered objects           var meshes = [];           var averageValues = [];           var totalValues = [];             // keep track of min and max, used to color the objects           var maxValueAverage = 0;           var minValueAverage = -1;            // keep track of max and min of total value           var maxValueTotal = 0;           var minValueTotal = -1;            // convert to mesh and calculate values           for (var i = 0 ; i &lt; data.features.length ; i++) {               var geoFeature = data.features[i]               var feature = geo.path(geoFeature);               // we only need to convert it to a three.js path               var mesh = transformSVGPathExposed(feature);               // add to array               meshes.push(mesh);                // we get a property from the json object and use it               // to determine the color later on               var value = parseInt(geoFeature.properties.bev_dichth);               if (value &gt; maxValueAverage) maxValueAverage = value;               if (value &lt; minValueAverage || minValueAverage == -1) minValueAverage = value;               averageValues.push(value);                // and we get the max values to determine height later on.               value = parseInt(geoFeature.properties.aant_inw);               if (value &gt; maxValueTotal) maxValueTotal = value;               if (value &lt; minValueTotal || minValueTotal == -1) minValueTotal = value;                totalValues.push(value);           }            // we've got our paths now extrude them to a height and add a color           for (var i = 0 ; i &lt; averageValues.length ; i++) {                // create material color based on average               var scale = ((averageValues[i] - minValueAverage) / (maxValueAverage - minValueAverage)) * 255;               var mathColor = gradient(Math.round(scale),255);               var material = new THREE.MeshLambertMaterial({                   color: mathColor               });                // create extrude based on total               var extrude = ((totalValues[i] - minValueTotal) / (maxValueTotal - minValueTotal)) * 100;               var shape3d = meshes[i].extrude({amount: Math.round(extrude), bevelEnabled: false});                // create a mesh based on material and extruded shape               var toAdd = new THREE.Mesh(shape3d, material);                // rotate and position the elements nicely in the center               toAdd.rotation.x = Math.PI/2;               toAdd.translateX(-490);               toAdd.translateZ(50);               toAdd.translateY(extrude/2);                // add to scene               scene.add(toAdd);           }       }          // simple gradient function         function gradient(length, maxLength) {              var i = (length * 255 / maxLength);             var r = i;             var g = 255-(i);             var b = 0;              var rgb = b | (g &lt;&lt; 8) | (r &lt;&lt; 16);             return rgb;         }   A big piece of code, but not that complex. What we do here is we keep track of two values for each municipality: the population density and the total population. These values are used to respectively calculate the color (using the gradient function) and the height. The height is used in the Three.js extrude function which converts our 2D Three.Js path to a 3D shape. The color is used to define a material. This shape and material is used to create the Mesh that we add to the scene.   Render everything   All that is left is to render everything. For this example we’re not interested in animations or anything so we can make a single call to the renderer:   renderer.render( scene, camera );   And the result is as you saw in the beginning. The following image shows a different example. This time we once again show the population density, but now the height represents the land area of the municipality.      I’m currently creating a new set of geojson data, but this time for the whole of Europe. So in the next couple of weeks expect some articles using maps of Europe.  ","categories": ["posts","gis","geo","d3.js","three.js"],
        "tags": [],
        "url": "http://www.smartjava.org/content/render-geographic-information-3d-threejs-and-d3js/",
        "teaser":null},{
        "title": "Three.js: render real world terrain from heightmap using open data",
        "excerpt":"Three.js is a great library for creating 3D objects and animations. In a couple of previous articles I explored this library a bit and in one of those examples I showed you how you can take GIS information (in geoJSON) format and use D3.js and three.js to convert it to a 3D mesh you can render in the browser using javascript. This is great for infographic, but it doesn’t really show a real map, a real terrain. Three.js, luckily also has helper classes to render a terrain as you can see in this demo: http://mrdoob.github.com/three.js/examples/webgl_terrain_dynamic.html   This demo uses a noise generator to generate a random terrain, and adds a whole lot of extra functionality, but we can use this concept to also render maps of real terrain. In this article I’ll show you how you can use freely available open geo data containing elevation info to render a simple 3D terrain using three.js. In this example we’ll use elevation data that visualizes the data for the island of Corsica.      Or look at the live site here: http://www.smartjava.org/examples/heightmap   Where do we get the data?   The open data that we’ll use comes from ASTER/GDEM. Which is:   \"The ASTER Global Digital Elevation Model (ASTER GDEM) is a joint product developed and made available to the public by the Ministry of Economy, Trade, and Industry (METI) of Japan and the United States National Aeronautics and Space Administration (NASA).  It is generated from data collected from the Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER), a spaceborne earth observing optical instrument.\"  This project offers us elevation data (geotagged) with a resolution of 30 meters. On this site you can download tiles for specific areas (after free registration):      The tiles you download from there are in GeoTIFF format (which I’ll dive into in a future article). This format contains values for specific geo coordinates. Three.js, however, can’t directly work with geotiff images (which are way to big anyways), so we need to convert them to either JPEG or PNG. For this we’ll use GDAL. GDAL is a library and a set of utilities that can be used to perform a wide range of GIS related functions. If you’re on a Mac you can install GDAL through macports:    sudo port install gdal  ---&gt;  Installing gdal @1.9.0_0+expat ---&gt;  Activating gdal @1.9.0_0+expat ---&gt;  Cleaning gdal    So download some tiles and make sure the form you download is a rectangle (see previous screenshot). Unzip the tiles and copy all the “*_dem.tif” files into a single directory. This isn’t really necessary but will make processing easier. With GDAL now you can convert these downloaded tiles to PNGs. For this I use the following very basic bash script.    #!/bin/bash for file in `ls -d *dem.tif` ; do    gdal_translate -b 1 -outsize 400 400 -scale -20 2200 -of PNG \"$file\" ~/output/$file.png done    This will convert all the “dem.tif” tiles to a set of PNGs. Each tile is 400x400 pixels and we also define the range of the input to -20 until 2200. In other words the elevation from the input is from -20 tot 2200 meters Since the gray scale in a PNG can only be in a range of 256, this will be downsampled. (We can use other input formats, or use an RGB scale, but that would require some custom code, which is a bit out of scope for this article). For most maps though, this should be enough for a general impression of the landscape.   One of the tiles for Corsica looks like this:      So now we’re stuck with a set of seperate PNGs. We need to combine those to a single PNG that we can use as a heightmap for Three.js. For this we’ll use another open source library: imagemagick (also available through macports). In the example for Corsica we have 8 files, which are combined into a single png using the following batch file:    process=\"ASTGTM2_N41E008_dem.tif.png ASTGTM2_N41E009_dem.tif.png  ASTGTM2_N40E008_dem.tif.png ASTGTM2_N40E009_dem.tif.png  ASTGTM2_N39E008_dem.tif.png ASTGTM2_N39E009_dem.tif.png ASTGTM2_N38E008_dem.tif.png ASTGTM2_N38E009_dem.tif.png\" montage -tile 2x4 -border 0 -frame 0 -geometry '1x1+0+0&lt;' $process combined.png    This results in a single PNG with the name “combined.png”, which looks like this (scaled down):      That’s it for the preparation part. We’ve know got a heightmap in grayscale that we can use in Three.js (and in many other programs for that matter).   Create a map using Three.js    Once we got a heightmap we can use it in Three.js. The following code shows how you to do this:   &lt;!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\"         \"http://www.w3.org/TR/html4/loose.dtd\"&gt; &lt;html&gt; &lt;head&gt;     &lt;title&gt;&lt;/title&gt;     &lt;script type=\"text/javascript\" src=\"js/Three.js\"&gt;&lt;/script&gt;     &lt;script type=\"text/javascript\" src=\"js/ShaderTerrain.js\"&gt;&lt;/script&gt;     &lt;script type=\"text/javascript\" src=\"js/jquery-1.7.2.js\"&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt;  &lt;div id=\"main_map\"&gt;  &lt;/div&gt;  &lt;script type=\"text/javascript\"&gt;      var CONS = {         // THREE.JS CONSTANTS         // set the scene size         WIDTH:904,         HEIGHT:604,          // set some camera attributes         VIEW_ANGLE:45,         NEAR:0.1,         FAR:10000,          CAMERA_X:1000,         CAMERA_Y:600,         CAMERA_Z:1300     }      var scene = {};     var renderer = {};     var camera = {};     var controls;       var n = 0;     initMap();      // Wait until everything is loaded before continuing     function loaded() {         n++;         console.log(\"loaded: \" + n);          if (n == 3) {             terrain.visible = true;             console.log('ff');             render();         }     }      function initMap() {          // setup default three.js stuff         renderer = new THREE.WebGLRenderer();         renderer.setSize(CONS.WIDTH, CONS.HEIGHT);         renderer.setClearColor(0x0000cc);         $(\"#main_map\").append(renderer.domElement);          camera = new THREE.PerspectiveCamera(CONS.VIEW_ANGLE, CONS.WIDTH / CONS.HEIGHT, CONS.NEAR, CONS.FAR);         scene = new THREE.Scene();         scene.add(camera);          camera.position.z = CONS.CAMERA_Z;         camera.position.x = CONS.CAMERA_X;         camera.position.y = CONS.CAMERA_Y;         camera.lookAt(scene.position);          // add a light         pointLight = new THREE.PointLight(0xFFFFFF);         scene.add(pointLight);         pointLight.position.x = 1000;         pointLight.position.y = 3000;         pointLight.position.z = -1000;         pointLight.intensity = 8.6;           // load the heightmap we created as a texture         var texture = THREE.ImageUtils.loadTexture('assets/combined.png', null, loaded);          // load two other textures we'll use to make the map look more real         var detailTexture = THREE.ImageUtils.loadTexture(\"assets/bg.jpg\", null, loaded);          // the following configuration defines how the terrain is rendered         var terrainShader = THREE.ShaderTerrain[ \"terrain\" ];         var uniformsTerrain = THREE.UniformsUtils.clone(terrainShader.uniforms);          // how to treat abd scale the normal texture         uniformsTerrain[ \"tNormal\" ].texture = detailTexture;         uniformsTerrain[ \"uNormalScale\" ].value = 1;          // the displacement determines the height of a vector, mapped to         // the heightmap         uniformsTerrain[ \"tDisplacement\" ].texture = texture;         uniformsTerrain[ \"uDisplacementScale\" ].value = 100;          // the following textures can be use to finetune how         // the map is shown. These are good defaults for simple         // rendering         uniformsTerrain[ \"tDiffuse1\" ].texture = detailTexture;         uniformsTerrain[ \"tDetail\" ].texture = detailTexture;         uniformsTerrain[ \"enableDiffuse1\" ].value = true;         uniformsTerrain[ \"enableDiffuse2\" ].value = true;         uniformsTerrain[ \"enableSpecular\" ].value = true;          // diffuse is based on the light reflection         uniformsTerrain[ \"uDiffuseColor\" ].value.setHex(0xcccccc);         uniformsTerrain[ \"uSpecularColor\" ].value.setHex(0xff0000);         // is the base color of the terrain         uniformsTerrain[ \"uAmbientColor\" ].value.setHex(0x0000cc);          // how shiny is the terrain         uniformsTerrain[ \"uShininess\" ].value = 3;          // handles light reflection         uniformsTerrain[ \"uRepeatOverlay\" ].value.set(3, 3);          // configure the material that reflects our terrain         var material = new THREE.ShaderMaterial({             uniforms:uniformsTerrain,             vertexShader:terrainShader.vertexShader,             fragmentShader:terrainShader.fragmentShader,             lights:true,             fog:false         });          // we use a plain to render as terrain         var geometryTerrain = new THREE.PlaneGeometry(2000, 4000, 256, 256);         geometryTerrain.applyMatrix(new THREE.Matrix4().makeRotationX(Math.PI / 2));         geometryTerrain.computeFaceNormals();         geometryTerrain.computeVertexNormals();         geometryTerrain.computeTangents();          // create a 3D object to add         terrain = new THREE.Mesh(geometryTerrain, material);         terrain.position.set(0, -125, 0);         terrain.rotation.x = -Math.PI / 2;          // add the terrain         scene.add(terrain);          // tell everything is ready         loaded();     }      // render the scene     function render() {         renderer.render(scene, camera);     } &lt;/script&gt; &lt;/body&gt; &lt;/html&gt;   And that’s all you need to do. Once you’ve got this basic setup working, you can easily add custom textures, fog or other extra objects. If you want to see this basic example in action look here: http://www.smartjava.org/examples/heightmap  ","categories": ["posts","javascript","three.js","webgl"],
        "tags": [],
        "url": "http://www.smartjava.org/content/threejs-render-real-world-terrain-heightmap-using-open-data/",
        "teaser":null},{
        "title": "Euro debt and financial crisis visualized with timeline using d3.js and Three.js",
        "excerpt":"In a couple of recent articles I’ve been using Three.js and d3.js to create 3D maps and visualize metrics. In the last week I combined a couple of these techniques to create a visualization of the financial/debt crisis in the Eurozone:        What does this timeline represent?  In this timeline I show how the interest rates for the long term bonds change over time for the Countries of the Eurozone. This is the interest rate countries pay to refinance their debts. Why is this interesting? This interest rate is a very good indication of the financial stability of a country. Is the rate high, then refinancing debt will cost more and more and interest payments will go up. A situation which, at certain percentages, will bankrupt a country.   In this visualization the following is shown:     Timeline at the bottom shows events that occur during the period 2009 to 2012.   For each event a 'bubble' is shown that explains a bit more about the event.   The map shows the countries. Green means low debt rate, red means high debt rate.  For better visualization the height of a country is also defined by this debt rate.   Techniques used   This timeline uses the following techniques:      d3.js: This library is used to create the timeline visualization at the bottom.   Three.js: To visualize the interest rate a 3D model is created in Three.j   In a couple of future articles I’ll go into more detail on how the various libraries are used.  ","categories": ["posts","html5","javascript","webgl","d3.js","three.js"],
        "tags": [],
        "url": "http://www.smartjava.org/content/euro-debt-and-financial-crisis-visualized-timeline-using-d3js-and-threejs/",
        "teaser":null},{
        "title": "Motion controlled space invaders using webrtc and canvas",
        "excerpt":"TL;DR: Play the game here (requires current Chrome, Opera of Firefox with webrtc enabled): Motion controlled space invaders   A couple of days ago I ran across an article on reactive super speed electromagnet dot display.  For the promotion of a new crime show they created an interactive board that responded to the audience in front. The follow image better explains what I’m getting at:      I really like way it looks, the old bit style graphics combined with motion detection looks really nice. So I tried to recreate this in HTML5 using canvas for rendering and webrtc to control the camera input (you need a recent version of Chrome, Opera of nightly firefox). Since just moving your arms around and seeing it on the screen is nice for the first couple of minutes, it quickly gets boring, so I added a game element using the 8-bit class space-invaders (play the game here):      The goal of this game is to try and hit the red “invader”, without hitting the white ones. You get a point for every red one you hit. Once you hit it, another one will light up for you to hit. The game is controlled by you moving in front of the webcam. If you want to get the maximum score you’ll have to move slowly or use your hands. You can also of course just headbang ferociously. Won’t get you much points however :)   In future articles I’ll dive a bit into the techniques used for this (very small) game:      Canvas   Webrtc   Motion detection (I based my motion detection on this article: http://www.adobe.com/devnet/html5/articles/javascript-motion-detection.html) &lt;/ul&gt;  You can play this game at: http://www.smartjava.org/examples/spaceinvaders/  ","categories": ["posts","canvas","webrtc","html5"],
        "tags": [],
        "url": "http://www.smartjava.org/content/motion-controlled-space-invaders-using-webrtc-and-canvas/",
        "teaser":null},{
        "title": "Remote chrome debugging on android",
        "excerpt":"With the latest versions of chrome debugging web apps running on smartphones and tablets has become a lot easier. This article shows you how to setup the required android tools, local chrome and remote chrome to debug a web app running in chrome on an android device. The first thing you need to do is make sure you’ve got the correct Android tools installed on your normal computer.   If you want to use a different mobile browser see these two articles:      Remote debugging using firefox   Remote debugging using opera.   Setup host computer   For this, if you haven’t done so already, download the android SDK from here: http://developer.android.com/sdk/index.html   Extract the archive and run the “android” command do install the “platform tools”.    &lt;EXTRACT_DIR&gt;/tools/android     This will show you the following screen:      Select the “Android SDK Platform-tools” and click the install button (you could uncheck the other boxes if you like). On the next screen accept all the licenses and click “install”. After the installation proces has run go back to the directory where you extracted the SDK and you’ll now see an additional directory named platform tools:    jos@Joss-MacBook-Pro.local:~/Downloads/android-sdk-macosx$ ls -1 SDK Readme.txt add-ons docs platform-tools platforms temp tools    In the platform-tools directory is the adb command, that you’ll need to setup remote debugging on your android device. Now that we’ve got everything ready on the server, we need to configure chrome on the android device.   Setup android device   This is very simple, go to the “settings”, open up the “advanced” tab and click on the “developer tools” option. Now check the box for “enable USB Web debugging”.      Make sure your android device is connected to your computer using USB, and that’s it for the android device side.   Connect the host to the android device   Now we can connect the chrome instance running on the tablet to a chrome instance running locally. Go to the directory where you installed the SDK and from there go to the platform-tools directory. In that directory you’ll find the ‘adb’ tool you need to setup the debug channel. Run the following command:    jos@Joss-MacBook-Pro.local:~/Downloads/android-sdk-macosx/platform-tools$ ./adb forward tcp:9222 localabstract:chrome_devtools_remote    This will startup the debug channel in the background.  Now you can open your local running chrome instance and point it to the following location: http://localhost:9222      This shows all the tabs open on your tablet that can be inspected with the remote chrome debugger. If you now click the page a debugger will open, just like your normal debugger.      Start debugging   Using this debugger in this way is really easy and works very intuitive. You can click on divs to see where and how they render on your android device, set breakpoints, see how resources are loaded etc. Pretty much everything you can do locally, can be done in the remote view. You can even execute arbitrary javascript directly from the debugger:   Execute javascript:    Directly see result on device:    On a side note, firefox mobile also offers this functionality. I’ll write about that in an upcoming article.   Update: Just added an article on how to do this in Firefox. You can find this article here: http://www.smartjava.org/content/remote-firefox-debugging-android  ","categories": ["posts","debugging","chrome"],
        "tags": [],
        "url": "http://www.smartjava.org/content/remote-chrome-debugging-android/",
        "teaser":null},{
        "title": "Remote firefox debugging on android",
        "excerpt":"In the previous article I wrote about remote debugging web applications using chrome on Android. If you don’t want to use chrome or you want to test and debug your application on a different mobile browser you can also use firefox mobile to debug (and use FF15+ on the desktop). The process is pretty much the same as for chrome.   If you want to use a different mobile browser see these two articles:      Remote debugging using chrome   Remote debugging using opera.      Connect your mobile device with USB to your desktop computer   Setup your mobile firefox for remote debugging   Configure your desktop browser for remote debugging   Connect desktop browser to mobile browser and start debugging   To start debugging you have to connect your mobile device using USB to your desktop. Once you’ve done this you can use the android ‘adb’ tool to setup a debug connection to your tablet or phone. If you need instructions on how to setup ‘adb’ see the previous article on chrome.   Setup your mobile firefox for remote debugging   Open your mobile firefox (make sure you’ve got the latest firefox mobile beta) and enter “about:config” in the url. This will open a screen where you can edit settings. In this screen set the “devtools.debugger.remote-enabled” to “true”:      Now restart firefox, and that’s it for this part.   Setup your desktop firefox for remote debugging   Next we need to configure firefox on your desktop for remote debugging (I’m using a nightly build). Once again go to the “about:config” url. This time set the “devtools.debugger.remote-enabled” property to true.      Restart your browser and now you’ll have a new entry in the “tools-&gt;web developer” menu called “Remote Debugger”   Connect desktop browser to mobile browser and start debugging   Now you’re pretty much ready to start debugging. We mentioned at the start that we’re going to use android’s adb tool to set up the debug channel. To do this, run the following command:    jos@Joss-MacBook-Pro.local:~/dev/android-sdk-macosx/platform-tools$ ./adb forward tcp:6000 tcp:6000 * daemon not running. starting it now on port 5037 * * daemon started successfully * jos@Joss-MacBook-Pro.local:~/dev/android-sdk-macosx/platform-tools$     Now point the browser on your mobile device to the page you want to debug and open your firefox on the desktop. On your desktop firefox select the “tools-&gt;web developer-&gt;Remote Debugger” option. Just click enter on this popup:      When you do, you’ll see another popup appear, but this time on your mobile device.      You’ve got a couple of seconds to click the Ok button, and once you do, a remote debugging session will be created. From your desktop you can now set breakpoints, view scripts etc.   View code:    Set breakpoints:    And that’s it! Very easy, and another great way to make debugging web/html applications on your mobile device easier. In the next couple of days, I’ll also show how you can do this with Opera.  ","categories": ["posts","debugging","firefox"],
        "tags": [],
        "url": "http://www.smartjava.org/content/remote-firefox-debugging-android/",
        "teaser":null},{
        "title": "Remote opera debugging on android",
        "excerpt":"The final article on remote debugging web applications on mobile devices shows how you can do this using Opera Mobile. With Opera Mobile you can easily debug web applications running on Android, without needing an USB connection (b.t.w you can also debug web application on firefox without USB). In this article I’ll quickly walk you through the steps required to setup the debug connection.   If you want to use a different mobile browser see these two articles:      Remote debugging using firefox   Remote debugging using chrome.   Using Opera for this, is just as simple as using firefox or chrome. All you need to do is the following:      Setup Opera on the desktop to listen for debug connections   Start a debug connection from Opera Mobile   Start debugging   Since we won’t be working with an USB cable, you’ll need to know the ip address of your Desktop. You can use the ipconfig/ifconfig command in a terminal to get this address.      Setup Opera on the desktop to listen for debug connections   Open Opera and start “Opera Dragonfly” from the menu (Tools-&gt;Advanced-&gt;Opera DragonFly). This will show the debugging console at the bottom of the current page. Click on the “Remote Debug Configuration” button and you’ll be shown the following screen:      Just click on “Apply” here, and Opera will wait for a connection request from a remote device.   Start a debug connection from Opera Mobile   Now open Opera Mobile on your phone or tablet and point your browser to “opera:debug”.      Enter the ip-address you looked up earlier and click on ‘connect’. This will setup a connection between opera on your mobile device and on your desktop. Now you can open the website you want to debug in a new tab on your device and use Opera on your desktop to debug the specific tab.      And that’s all you have to do.  ","categories": ["posts","android","debugging"],
        "tags": [],
        "url": "http://www.smartjava.org/content/remote-opera-debugging-android/",
        "teaser":null},{
        "title": "HTML5: Render open data on a 3D world globe with Three.js",
        "excerpt":"With Three.js it’s very easy to create 3D objects and render these using WebGL. In a couple of previous articles I already showed how you can create 3D maps and even use elevation data to create 3D representations of the real world. In this article we’ll continue a bit further on this path. In this article I’ll show you how you can render open data based onto a 3D globe. For this first article I’ll show you how you can create the following ‘infographic’:      This graphic, of which you can find a working example here, shows the population density around the world, plotted onto a 3D globe, rendered using Three.js. To create this we need to take the following steps:      Setup a Three.js scene   Create a 3D world globe and add it to the scene   Get density information and convert it to a format we can work with   Convert each data point to a coordinate on the sphere   Add all the information to the Three.js scene   Rotate the scene, so we can see the whole world   Many steps, but really not so difficult to do. We’ll start, as with every Three.js project, with the basics, and add the Three.js initialization code.   Setup a Three.js scene   The following code is the basic code you need to get started.      // couple of constants     var POS_X = 1800;     var POS_Y = 500;     var POS_Z = 1800;     var WIDTH = 1000;     var HEIGHT = 600;      var FOV = 45;     var NEAR = 1;     var FAR = 4000;      // some global variables and initialization code     // simple basic renderer     var renderer = new THREE.WebGLRenderer();     renderer.setSize(WIDTH,HEIGHT);     renderer.setClearColorHex(0x111111);      // add it to the target element     var mapDiv = document.getElementById(\"globe\");     mapDiv.appendChild(renderer.domElement);      // setup a camera that points to the center     var camera = new THREE.PerspectiveCamera(FOV,WIDTH/HEIGHT,NEAR,FAR);     camera.position.set(POS_X,POS_Y, POS_Z);     camera.lookAt(new THREE.Vector3(0,0,0));      // create a basic scene and add the camera     var scene = new THREE.Scene();     scene.add(camera);      // we wait until the document is loaded before loading the     // density data.     $(document).ready(function()  {         jQuery.get('data/density.csv', function(data) {             addDensity(CSVToArray(data));             addLights();             addEarth();             addClouds();             render();         });     });   In this small piece of code we create a Three.js scene, a camera and append it to a specific element in the html page. I use JQuery to determine when the document is ready. Once the HTML page is completely loaded I read in the data to plot and add the various elements of this graphic. We start of easy, by creating the 3D globe of the earth (the addEarth and addCloud functions).   Create a 3D world globe and add it to the scene       // add the earth     function addEarth() {         var spGeo = new THREE.SphereGeometry(600,50,50);         var planetTexture = THREE.ImageUtils.loadTexture( \"assets/world-big-2-grey.jpg\" );         var mat2 =  new THREE.MeshPhongMaterial( {             map: planetTexture,             shininess: 0.2 } );         sp = new THREE.Mesh(spGeo,mat2);         scene.add(sp);     }   We start of with a very basic earth. This earth is rendered as perfect sphere (which the earth in reality isn’t), where we add a texture that is a satellite map of the earth. I converted the map to grey scale to make it less prominent in the finalized scene. Good starting material for maps of the earth can be found from nasa here: http://visibleearth.nasa.gov/view_cat.php?categoryID=1484 .The basic earth map doesn’t contain clouds, we can easily add them by creating a somwhat bigger sphere with a cloud texture.       // add clouds     function addClouds() {         var spGeo = new THREE.SphereGeometry(600,50,50);         var cloudsTexture = THREE.ImageUtils.loadTexture( \"assets/earth_clouds_1024.png\" );         var materialClouds = new THREE.MeshPhongMaterial( { color: 0xffffff, map: cloudsTexture, transparent:true, opacity:0.3 } );          meshClouds = new THREE.Mesh( spGeo, materialClouds );         meshClouds.scale.set( 1.015, 1.015, 1.015 );         scene.add( meshClouds );     }   I’ve used the clouds texture from the Three.js examples, but you can also find different other textures online. Now all we need to do is add some lights and we’ve got our basic globe setup.       // add a simple light     function addLights() {         light = new THREE.DirectionalLight(0x3333ee, 3.5, 500 );         scene.add( light );         light.position.set(POS_X,POS_Y,POS_Z);     }   This adds a basic directional light (at the same position as our camera). I’ve used a blue-ish color here. If we render this scene we get the following: an earth rendered with a blue glow.      Get density information and convert it to a format we can work with   With the basic globe rendered we need to get some information that we can use to plot on this globe. For this example I used population density information from Socioeconomic Data and Applications Center - SEDAC. From there you can download density information in various formats. I used the 1 degree ascii format, which contains a data point for each lat/lon combination of the earth. This format looks something like this:    ncols         360 nrows         143 xllcorner     -180 yllcorner     -58 cellsize      1.0000000000008 NODATA_value  -9999 value1 value2 value3 value4 value5 (repeated 360 times) value1 value2 value3 value4 value5 (repeated 360 times)    So we got 143 rows and 360 columns representing data for the complete earth. In my first try I converted this to json data, but the resulting file was 1.5MB and took some time to parse. So in the next try I just stripped the header, and saved it as a simple cvs file, where each line is an x,y coordinate.    102,1,0.0003149387 103,1,0.0003149386 104,1,0.0003149387 105,1,0.0003149387 106,1,0.0003149387 107,1,0.0003149386 108,1,0.0003149387 109,1,0.0003149387 110,1,0.0003149387 133,1,0.008578668 etc..    This also allowed me to filter out the -9999 values and makes processing in javascript easier. To load this data I use jquery:   jQuery.get('data/density.csv', function(data) {             addDensity(CSVToArray(data));              ...         });   And use the CSVToArray function to convert the data to an array of arrays. The CSVToArray function was copied from this stackoverflow article: http://stackoverflow.com/questions/1293147/javascript-code-to-parse-csv-data   At this point we’ve got a set of x,y coordinates (in WGS84 style) that we can use to plot this information on a 2D map (as is done on the SEDAC) site. We need to convert this x,y to a point on our sphere.   Convert each data point to a coordinate on the sphere   Now how do we convert a point in a 2D space to a 3D sphere? Luckily there are a set of standard methods for this. This wikipedia article explains how to convert between the various coordinate systems. Without diving too much into the details, the javascript code to do this is the following:       // convert the positions from a lat, lon to a position on a sphere.     function latLongToVector3(lat, lon, radius, heigth) {         var phi = (lat)*Math.PI/180;         var theta = (lon-180)*Math.PI/180;          var x = -(radius+heigth) * Math.cos(phi) * Math.cos(theta);         var y = (radius+heigth) * Math.sin(phi);         var z = (radius+heigth) * Math.cos(phi) * Math.sin(theta);          return new THREE.Vector3(x,y,z);     }   This function converts a x,y coordinate to a point in a 3D space. The radius supplied here is the radius of our earth, and the height is used as an offset of how high above the surface we want to start drawing.   Add all the information to the Three.js scene   With all this in place we can render the density information on the scene. We do this in the following javascript function.       // simple function that converts the density data to the markers on screen     // the height of each marker is relative to the density.     function addDensity(data) {          // the geometry that will contain all our cubes         var geom = new THREE.Geometry();         // material to use for each of our elements. Could use a set of materials to         // add colors relative to the density. Not done here.         var cubeMat = new THREE.MeshLambertMaterial({color: 0x000000,opacity:0.6, emissive:0xffffff});         for (var i = 0 ; i &lt; data.length-1 ; i++) {              //get the data, and set the offset, we need to do this since the x,y coordinates             //from the data aren't in the correct format             var x = parseInt(data[i][0])+180;             var y = parseInt((data[i][1])-84)*-1;             var value = parseFloat(data[i][2]);              // calculate the position where we need to start the cube             var position = latLongToVector3(y, x, 600, 2);              // create the cube             var cube = new THREE.Mesh(new THREE.CubeGeometry(5,5,1+value/8,1,1,1,cubeMat));              // position the cube correctly             cube.position = position;             cube.lookAt( new THREE.Vector3(0,0,0) );              // merge with main model             THREE.GeometryUtils.merge(geom,cube);         }          // create a new mesh, containing all the other meshes.         var total = new THREE.Mesh(geom,new THREE.MeshFaceMaterial());          // and add the total mesh to the scene         scene.add(total);     }   In this code we do the following:   We first convert the x,y from the input format to the -90,90 - 180,-180 range.               var x = parseInt(data[i][0])+180;             var y = parseInt((data[i][1])-84)*-1;             var value = parseFloat(data[i][2]); &lt;/javscript&gt;   &lt;h4&gt;These coordinates are converted to a point on the sphere and used to draw a cube&lt;/h4&gt;  Using the function we described earlier, we convert the x,y to a position on the sphere. These values are then used to create a cube.           // calculate the position where we need to start the cube         var position = latLongToVector3(y, x, 600, 2);          // create the cube         var cube = new THREE.Mesh(new THREE.CubeGeometry(5,5,1+value/8,1,1,1,cubeMat)); ```   You can see that we use the value as the height op the sphere.   Rotate the cube so it nicely aligns with the globe    If we render the scene as this we get nice cubes but they all point ‘upwards’, they aren’t rendered aligned with the surface of the sphere. Aligning these objects according to the normal vector of the surface normally requires some interesting math. Luckily though, Three.js has a simpler option for us. we can use this:   cube.lookAt( new THREE.Vector3(0,0,0) );   To make the object ‘look’ at a specific point in space. If we make the object look at the center of the Sphere it will be aligned correctly.   Reduce number of objects to add   We made one optimization before adding the cubes to the scene.     function addDensity(data) {         var geom = new THREE.Geometry();         var cubeMat = new THREE.MeshLambertMaterial({color: 0x000000,opacity:0.6, emissive:0xffffff});         for (var i = 0 ; i &lt; data.length-1 ; i++) {             ...             var cube = new THREE.Mesh(new THREE.CubeGeometry(5,5,1+value/8,1,1,1,cubeMat));             ...             THREE.GeometryUtils.merge(geom,cube);         }         var total = new THREE.Mesh(geom,new THREE.MeshFaceMaterial());         scene.add(total);     }   The important method here is the merge method. What this method does is it copies all the faces and vertices from the cube we created to the geometry we created in the beginning of this function. The reason we do this, is that we now only have to add a single object to the scene, instead of 18000. This will massively increase rendering speed. We also reuse the material, which is another big rendering optimization.   However, this is still a fairly heavy 3D object, so rendering, especially on slower hardware can take some time. On the other hand, I was able to view this on my 2 year old smartphone.   Rotate the scene, so we can see the whole world   The final step we add is a simple rotating animation. This time I rotate the camera and the light.       function render() {         var timer = Date.now() * 0.0001;         camera.position.x = (Math.cos( timer ) *  1800);         camera.position.z = (Math.sin( timer ) *  1800) ;         camera.lookAt( scene.position );         light.position = camera.position;         light.lookAt(scene.position);         renderer.render( scene, camera );         requestAnimationFrame( render );     }   And that’s it. The complete example can be found here.  ","categories": ["posts","geo","three.js"],
        "tags": [],
        "url": "http://www.smartjava.org/content/html5-render-open-data-3d-world-globe-threejs/",
        "teaser":null},{
        "title": "HTML5: Render urban population growth on a 3D world globe with Three.js and canvas",
        "excerpt":"In this article I’ll once again look at data / geo visualization with Three.js. This time I’ll show you how you can plot the urban population growth over the years 1950 to 2050  on a 3D globe using Three.js. The resulting visualization animates the growth of the world’s largest cities on a rotating 3D world. The result we’re aiming for looks like this (for a working example look here.):      If you’re interested in an alternative way of rendering information look at my previous article where I used a somewhat different approach to render information on a 3D globe.   For this visualization I wanted to have the following features:     Draw circle on the surface of a sphere to visualize the size of a city   Show an overview of the five largest cities   Step through the years from 1950 to 2050 and update the animation   Control the rotation of the world by using some simple keyboard controls   All this isn’t too difficult to do with Three.js and some simple canvas manipulation. In this article I won’t dive into the details of setting up Three.js and creating the scene. If you want more information on that look at my previous article .   Draw circle on the surface of a sphere to visualize the size of a city   The first thing we need to be able to do is draw a circle for the size of each city. Before we do this let’s have a quick look at our input data. After some small fixes the data looks like this in simple csv format:    \"Sofia\";\"\";\"Bulgaria\";42.70;23.33;\"BGR\";520.00;620.00;710.00;810.00;890.00;980.00;10 70.00;1180.00;1190.00;1170.00;1130.00;1170.00;1180.00;1210.00;1230.00;1240.00;1236  \"Mandalay\";\"\";\"Myanmar\";21.97;96.08;\"MMR\";170.00;200.00;250.00;310.00;370.00;440.00; 500.00;560.00;640.00;720.00;810.00;920.00;960.00;1030.00;1170.00;1310.00;1446    Where the first record is the name of the city, the third and fourth are the location and from the seventh record we see the population size in a specific year. Starting from 1950 and increasing in steps of 5 years to 2025 with a final entry for 2050. So to draw circles we just need to read in the CSV file and for each line determine the point on the sphere and based on the year we want to draw, we create a circle with a certain radius.   Drawing a circle on a sphere sounds easy, but it’s rather difficult if you want to do it completely using Three.js. You need to create custom geometries that exactly match the curveture of the sphere and place them at the correct position. Luckily though, there is an easy alternative. With Three.js we can specify the content of a canvas as the texture for an object. We use this in the following manner:      Create a canvas with the same aspect ratio as the map we use for our globe.   Draw our circles directly on the canvas   Use the canvas as a texture for a transparent sphere, just a bit bigger then the globe   Tell Three.js when the texture needs to be updated.   I’ve defined a simple hidden canvas directly in the html for this.    &lt;canvas id=\"canvas\" width=\"1024\" height=\"512\" style=\"display: none;\"&gt;&lt;/canvas&gt;   And I use the following code to directly draw the circles to the canvas.       function addCities(data, year) {          var yearIndex = getYearIndex(year);          var ctx = $('#canvas')[0].getContext(\"2d\");         ctx.clearRect(0,0,1024,512);           var renderedValues = [];         for (var i = 0 ; i &lt; data.length-1 ; i++) {              //get the data, and set the offset, we need to do this since the x,y coordinates             //from the data aren't in the correct format             var x = parseInt(data[i][3]);             var y = parseInt(data[i][4]);              var x2 =   ((1024/360.0) * (180 + y));             var y2 =   ((512/180.0) * (90 - x));              //draw a circle              var yearValue = parseInt(data[i][yearIndex]);             var nextValue = parseInt(data[i][yearIndex+1]);              var yearSize = 5; \t    // we need to handle the period from 2025 to 2050 different             if (yearIndex == 21) {                 yearSize = 25;             }             var step = (nextValue-yearValue)/yearSize;             var valueToSet = yearValue + ((year%yearSize)*step);               // for each city draw it at the correct position and size             ctx.fillStyle = \"#cc3333\";             ctx.globalAlpha = 0.5;             ctx.beginPath();             ctx.arc(x2,y2,valueToSet/1000.0,0,2*Math.PI,false);             ctx.fill();          }     }   This function takes in a multidimensional array representing the CSV file (see the source or this article on how to read in CSV files). The year is the current year for which we need to render the data. Next we get the position and convert it to an x,y coordinate on our canvas. Now we need where to draw, now we need to know what to draw. For this I take the values of the two closest years. So for 1978, I use the values of 1975 and 1980 to determine the approximate city size in the specific year. Based on these calues I draw a simple semi transparent arc. This result in the following canvas (which normally you won’t see because it’s hidden).      Even though this already looks nice, especially when animated, we want to take it one step further and plot it on a sphere. For this we can use the following piece of javascript code:       var texture;     function addOverlay() {         var spGeo = new THREE.SphereGeometry(604,50,50);         texture = new THREE.Texture($('#canvas')[0]);          var material = new THREE.MeshBasicMaterial({             map : texture,             transparent : true,             opacity: 0.7,             blending: THREE.AdditiveAlphaBlending          });          var meshOverlay = new THREE.Mesh(spGeo,material);         scene.add(meshOverlay);     }   This creates a slightly larger sphere and defines a texture based on the canvas we’ve drawn our circles to. By using a transparent material and using this specific blending mode we create a new sphere that shows all our circles on the correct positions. To make sure Three.js updates the texture we need to add “texture.needsUpdate” to our render loop.         // render the scene     var timer = 0;     var rotateSpeed = 0.004;     function render() {         texture.needsUpdate = true;         timer+=rotateSpeed;         camera.position.x = (Math.cos( timer ) *  1800);         camera.position.z = (Math.sin( timer ) *  1800);         camera.lookAt( scene.position );          light.position = camera.position;         light.lookAt(scene.position);          renderer.render( scene, camera );         requestAnimationFrame( render );     }   This was the most important part. We can now draw and update circles directly on a sphere.   Show an overview of the five largest cities   To show the number of largest cities we alter the addCities function we just saw:   function addCities(data, year) {          var yearIndex = getYearIndex(year);  \t...          var renderedValues = [];         for (var i = 0 ; i &lt; data.length-1 ; i++) {  \t     ...             // push interesting values into array to be sorted             renderedValues.push({value: valueToSet, city: data[i][0]});         }          // sort according to size         renderedValues.sort(function(a,b){return b.value-a.value});          // draw the overview of largest cities         var ctx2 = $('#largest')[0].getContext(\"2d\");         ctx2.globalAlpha = 0.5;         ctx2.clearRect(0,0,400,600);         for (var j = 0; j &lt; 6 ; j++) {             var x = 70;             var y = (j*80)+100;              // draw circles             ctx2.beginPath();             ctx2.fillStyle = \"#cc3333\";             ctx2.arc(x,y,renderedValues[j].value/1000.0,0,2*Math.PI,false);             ctx2.fill();              // output text             ctx2.fillStyle = \"#aaaaaa\";             ctx2.font = \"16px Arial\";             ctx2.fillText(renderedValues[j].city,x+50,y-10);             ctx2.fillText(Math.round(renderedValues[j].value)+'000',x+50,y+10);           }     }   Not too difficult. Each city that is drawn as a circle is pushed into an array. This array is sorted based on size, and the top 6 cities are drawn to a canvas. This canvas is shown to the right of the 3D globe.   With both the visualization components done, the next thing to do is kick of the animation. For this we use JQuery.   Step through the years from 1950 to 2050 and update the animation       function stepYears(elements) {         var input = {           year: 1950         };          var currentYear = 0;         $(input).animate(                 {year: 2050},                 {step: function(now)                     {                         var newYear = Math.round(input.year);                         if (newYear != currentYear) {                             $(\"#currentYear\").text(\"Year: \" + newYear);                              currentYear = newYear;                         }                         addCities(elements, input.year);                     },                  duration: 45000,                  easing: 'linear'                 }         );     }   This piece of JQuery updates the input variable from 1950 to 2050 over a period of 45 seconds. Since we want a linear transition we explicitly define the easing. For every change we call the addCities function we saw earlier to update the canvas we use as a texture.   The final function I wanted to add was a simple control for the rotation of the sphere.   Control the rotation of the world by using some simple keyboard controls   In this example I don’t rotate the sphere, but rotate the cameras. As we’ve seen earlier our render loop looks like this:         // render the scene     var timer = 0;     var rotateSpeed = 0.004;     function render() {         texture.needsUpdate = true;         timer+=rotateSpeed;         camera.position.x = (Math.cos( timer ) *  1800);         camera.position.z = (Math.sin( timer ) *  1800);         camera.lookAt( scene.position );          light.position = camera.position;         light.lookAt(scene.position);          renderer.render( scene, camera );         requestAnimationFrame( render );     }   The speed and direction of the rotation is managed by the rotateSpeed variable. If we register a keypress to update this value we can easily control the rotation.To register the keypresses I use JQuery.               // read all the elements             $(document).keypress(function(e) {                   switch (e.which) {                     case 97 : {                         // rotate globe step to left                         rotateSpeed+=0.001;                         break;                     };                      case 115 : {                         // rotate globe step to right                         rotateSpeed-=0.001;                         break;                     };                      case 112 : {                         rotateSpeed=0;                         break;                     };                   }             });   And thats it. A working example of this demo can be found here.  ","categories": ["posts","html5","three.js"],
        "tags": [],
        "url": "http://www.smartjava.org/content/html5-render-urban-population-growth-3d-world-globe-threejs-and-canvas/",
        "teaser":null},{
        "title": "Tutorial: Getting started with scala and scalatra - Part I",
        "excerpt":"In this series of tutorials we’re going to look a bit closer at scalatra. Scalatra is a lightweight scala based micro-web framework, that can be used to create high performance websites and APIs. In this first tutorial we’ll just get started with the installation of scalatra and import our test project into Eclipse.   SBT and giter8  Before you can get started you need to install a couple of tools (I’m assuming you’ve got a JDK 1.6+ installed). I’ll give you the condensed installation instructions, and extensive version of this can be found at the following scalatra page (http://www.scalatra.org/getting-started/first-steps.html). This approach should work for most environments, for my own however, it didn’t work… It downloaded old versions of giter8 and an old version of sbt. For sbt I used macports to get the latest version:    port install sbt    And I also downloaded giter8 manually.    curl https://raw.github.com/n8han/conscript/master/setup.sh | sh cs n8han/giter8    This last command will install g8 in your /bin directory. I've tested this with this g8 version:    ~/bin/g8  giter8 0.5.0 Usage: g8 [TEMPLATE] [OPTION]... Apply specified template.  OPTIONS     -b, --branch         Resolves a template within a given branch     --paramname=paramvalue         Set given parameter value and bypass interaction.   Apply template and interactively fulfill parameters.     g8 n8han/giter8  Or     g8 git://github.com/n8han/giter8.git  Apply template from a remote branch     g8 n8han/giter8 -b some-branch  Apply template from a local repo     g8 file://path/to/the/repo  Apply given name parameter and use defaults for all others.     g8 n8han/giter8 --name=template-test    Create initial project  Go to the root directory where you keep you projects and run the following:    jos@Joss-MacBook-Pro.local:~/dev/scalatra/firststeps$ g8 scalatra/scalatra-sbt organization [com.example]: org.smartjava package [com.example.app]: org.smartjava.scalatra      name [scalatra-sbt-prototype]: hello-scalatra servlet_name [MyScalatraServlet]: HelloScalatraServlet scala_version [2.9.1]:  version [0.1.0-SNAPSHOT]:    This will create a hello-scalatra folder in the directory your in that contains the project.    ./build.sbt ./project ./project/build.properties ./project/plugins.sbt ./README.md ./src ./src/main ./src/main/resources ./src/main/resources/logback.xml ./src/main/scala ./src/main/scala/org ./src/main/scala/org/smartjava ./src/main/scala/org/smartjava/scalatra ./src/main/scala/org/smartjava/scalatra/HelloScalatraServlet.scala ./src/main/scala/Scalatra.scala ./src/main/webapp ./src/main/webapp/WEB-INF ./src/main/webapp/WEB-INF/layouts ./src/main/webapp/WEB-INF/layouts/default.scaml ./src/main/webapp/WEB-INF/views ./src/main/webapp/WEB-INF/views/hello-scalate.scaml ./src/main/webapp/WEB-INF/web.xml ./src/test ./src/test/scala ./src/test/scala/org ./src/test/scala/org/smartjava ./src/test/scala/org/smartjava/scalatra ./src/test/scala/org/smartjava/scalatra/HelloScalatraServletSpec.scala    To test if everything is working, go into this directory and use sbt to start the application from sbt with ‘container:start’. This will download a lot of stuff, and finally start the application:    jos@Joss-MacBook-Pro.local:~/dev/scalatra/firststeps/hello-scalatra$ sbt [info] Loading project definition from /Users/jos/Dev/scalatra/firststeps/hello-scalatra/project [info] Set current project to hello-scalatra (in build file:/Users/jos/Dev/scalatra/firststeps/hello-scalatra/) &gt; container:start [info] jetty-8.1.5.v20120716 [info] NO JSP Support for /, did not find org.apache.jasper.servlet.JspServlet [info] started o.e.j.w.WebAppContext{/,[file:/Users/jos/Dev/scalatra/firststeps/hello-scalatra/src/main/webapp/]} [info] started o.e.j.w.WebAppContext{/,[file:/Users/jos/Dev/scalatra/firststeps/hello-scalatra/src/main/webapp/]} 15:12:44.604 [pool-6-thread-4] INFO  o.scalatra.servlet.ScalatraListener - Initializing life cycle class: Scalatra [info] started o.e.j.w.WebAppContext{/,[file:/Users/jos/Dev/scalatra/firststeps/hello-scalatra/src/main/webapp/]} 15:12:44.727 [pool-6-thread-4] INFO  o.f.s.servlet.ServletTemplateEngine - Scalate template engine using working directory: /var/folders/mc/vvzshptn22lg5zpp7fdccdzr0000gn/T/scalate-5609005579304140836-workdir [info] Started SelectChannelConnector@0.0.0.0:8080 [success] Total time: 1 s, completed Sep 7, 2012 3:12:44 PM &gt; [success] Total time: 1 s, completed Sep 7, 2012 3:10:05 PM    To test if everything is correct point the browser to localhost:8080 and you’ll see the following screen:      Import in Eclipse   I usually develop with Eclipse, so in this case, lets make sure we can edit the source code in Eclipse. For this we can use sbt eclipse plugin. Adding this plugin is very easy. Go to the ‘project’ folder and add the following (with an empty line before it) to the plugins.sbt file.    addSbtPlugin(\"com.typesafe.sbteclipse\" % \"sbteclipse-plugin\" % \"2.1.0\")    When you next run sbt, you’ll see a lot of stuff being downloaded. From sbt run ‘eclipse’ and eclipse configuration files (.classpath and .project) will be created. Now start up the Eclipse IDE and you can import the project you just created (make sure you also install the scala-ide for Eclipse).      Now we can start the container, and let sbt listen for changes in our resources.    $ sbt &gt; container:start &gt; ~ ;copy-resources;aux-compile    Any time we save a file in Eclipse, sbt will copy and recompile the resources. So we can directly see our changed files in the browser. Open the file HelloScalatraServlet and change the welcome text. If you save this, sbt will reload the application and you’ll directly see the changed files.      That’s it for part one of this tutorial. Next week we’ll look at how we can use scalatra to create a REST based service.   ","categories": ["posts","scalatra","scala"],
        "tags": [],
        "url": "http://www.smartjava.org/content/tutorial-getting-started-scala-and-scalatra-part-i/",
        "teaser":null},{
        "title": "Tutorial: Getting started with scala and scalatra - Part II",
        "excerpt":"In the previous part of the tutorial we created a simple application from scratch and setup Eclipse so we could edit the scala files for scalatra. In this second part of the tutorial you’ll learn how to do the following:      How to start scalatra with embedded Jetty for easy testing and debugging   Create a simple REST API that returns JSON data   Test your service using specs2   We will start by changing the way we start scalatra. Instead of running it using sbt like we did in the previous part, we’ll start scalatra directly from Eclipse. You can download the project for this tutorial from here. Remember to run the following from the project directory before importing in Eclipse.    $ sbt &gt; update &gt; eclipse    How to start scalatra with embedded Jetty for easy testing and debugging   We’ve seen that you can start scalatra (and your service) directly using sbt.    $ sbt &gt; container:start &gt; ~ ;copy-resources;aux-compile    This will start a Jetty server and automatically copy over the resources and compile them. Even though this works fine you can sometimes run into memory problems where reloading stops, debugging is very hard to do and when an exception is thrown you can’t just click on the exception to jump to the relevant source code. This is something we can easily fix. Scalatra uses Jetty internally, and is itself nothing more than a servlet. So what we can do is just run an embedded Jetty instance that points to the servlet. For this we create the following scala object.   package org.smartjava.scalatra.server;  import org.eclipse.jetty.server.Server import org.eclipse.jetty.webapp.WebAppContext  object JettyEmbedded {    def main(args: Array[String]) {     val server = new Server(9080)     val context: WebAppContext = new WebAppContext();     context.setServer(server)     context.setContextPath(\"/\");     context.setWar(\"src/main/webapp\")     server.setHandler(context);      try {       server.start()       server.join()     } catch {       case e: Exception =&gt; {         e.printStackTrace()         System.exit(1)       }     }   } }   Before you run this, also create a logback.xml file to control the logging. This is just a basic logging configuration that only logs the message at info level or greater. If you don’t have this, you’ll see a whole lot of Jetty log messages. For our own log messages we set the level to debug.    &lt;configuration&gt;   &lt;appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt;     &lt;!-- encoders are assigned the type          ch.qos.logback.classic.encoder.PatternLayoutEncoder by default --&gt;     &lt;encoder&gt;       &lt;pattern&gt;%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n&lt;/pattern&gt;     &lt;/encoder&gt;   &lt;/appender&gt;    &lt;logger name=\"org.smartjava.scalatra\" level=\"DEBUG\"/&gt;    &lt;root level=\"info\"&gt;     &lt;appender-ref ref=\"STDOUT\" /&gt;   &lt;/root&gt; &lt;/configuration&gt;    You can run this as an application directly from Eclipse: “Run-&gt;Run As-&gt;Scala Application”. The output you’ll see will be something like this:    21:37:33.421 [main] INFO  org.eclipse.jetty.server.Server - jetty-8.1.5.v20120716 21:37:33.523 [main] INFO  o.e.j.w.StandardDescriptorProcessor - NO JSP Support for /, did not find org.apache.jasper.servlet.JspServlet 21:37:33.589 [main] INFO  o.e.j.server.handler.ContextHandler - started o.e.j.w.WebAppContext{/,file:/Users/jos/Dev/scalatra/firststeps/hello-scalatra/src/main/webapp/},src/main/webapp 21:37:33.590 [main] INFO  o.e.j.server.handler.ContextHandler - started o.e.j.w.WebAppContext{/,file:/Users/jos/Dev/scalatra/firststeps/hello-scalatra/src/main/webapp/},src/main/webapp 21:37:33.631 [main] INFO  o.scalatra.servlet.ScalatraListener - Initializing life cycle class: Scalatra 21:37:33.704 [main] INFO  o.e.j.server.handler.ContextHandler - started o.e.j.w.WebAppContext{/,file:/Users/jos/Dev/scalatra/firststeps/hello-scalatra/src/main/webapp/},src/main/webapp 21:37:33.791 [main] INFO  o.f.s.servlet.ServletTemplateEngine - Scalate template engine using working directory: /var/folders/mc/vvzshptn22lg5zpp7fdccdzr0000gn/T/scalate-6431313014401266228-workdir 21:37:33.812 [main] INFO  o.e.jetty.server.AbstractConnector - Started SelectChannelConnector@0.0.0.0:9080    Now you can work with scalatra directly from Eclipse. Well that was easy. Next step, lets create a REST API. Before we do that though, if you work with Chrome, install “Dev HTTP Client”. This is a great HTTP Client that works directly from Chrome. Very easy to use.      Create a simple REST API that returns JSON data   For this part of the tutorial we’ll start by creating a very simple REST API. We’ll create an API that allows us to bid on items. A sort of mini eBay. We won’t make it very large at the moment and only provide three operations:      Get auction item based on id.   Make a bid on a specific item.   Get a bid a user has made.   We won’t add persistency yet (that’s something for the next tutorial), we’ll only look at the API side of things. We’ll start with the first one.   Get auction item based on id   For this we want to be able to do the following:   Request:    GET /items/123    Response:    200 OK Content-Length: 434 Server: Jetty(8.1.5.v20120716) Content-Type: application/vnd.smartbid.item+json;charset=UTF-8  {  \"name\":\"Monty Python and the search for the holy grail\",   \"id\":123,  \"startPrice\":0.69,  \"currency\":\"GBP\",  \"description\":\"Must have item\",  \"links\":[    {     \"linkType\":\"application/vnd.smartbid.item\",     \"rel\":\"Add item to watchlist\",     \"href\":\"/users/123/watchlist\"    },    {     \"linkType\":\"application/vnd.smartbid.bid\",      \"rel\":\"Place bid on item\",     \"href\":\"/items/123/bid\"    },    {     \"linkType\":\"application/vnd.smartbid.user\",     \"rel\":\"Get owner's details\",     \"href\":\"/users/123\"    }  ] }    As you can see we make a simple GET request to a specific URL and what we get back are the details of an item. This item has some properties and a number of links. These links can be followed by the user of your API to explore other resources or execute some action on your API. I won’t go into much detail here, but if you want to know what to do to create an easy to use and flexible API look at my presentation.   We know what our client needs to do to get this resource. The scalatra code is very easy:   package org.smartjava.scalatra  import grizzled.slf4j.Logger import org.scalatra._ import scalate.ScalateSupport import net.liftweb.json.compact import net.liftweb.json.render import net.liftweb.json.JsonDSL._ import net.liftweb.json.Serialization.{read, write} import org.smartjava.scalatra.repository.ItemRepository import net.liftweb.json.Serialization import net.liftweb.json.NoTypeHints import org.scalatra.Ok import org.scalatra.NotFound import org.smartjava.scalatra.repository.BidRepository import org.scalatra.Created import scala.collection.immutable.Map import org.smartjava.scalatra.model.Bid  class HelloScalatraServlet extends ScalatraServlet with ScalateSupport {      // simple logger   val logger = Logger(classOf[HelloScalatraServlet]);    // repo stores our items   val itemRepo = new ItemRepository;   val bidRepo = new BidRepository;      // implicit value for json serialization format   implicit val formats = Serialization.formats(NoTypeHints);     get(\"/items/:id\") {     // set the result content type     contentType = \"application/vnd.smartbid.item+json\"            // convert response to json and return as OK     itemRepo.get(params(\"id\").toInt) match {       case Some(x) =&gt; Ok(write(x));       case None =&gt; NotFound(\"Item with id \" + params(\"id\") + \" not found\");     }   } }   For this first REST operation I list the complete class, for the rest I’ll only show the relevant functions. To handle the request we need to define a ‘route’.     get(\"/items/:id\") {     // set the result content type     contentType = \"application/vnd.smartbid.item+json\"            // convert response to json and return as OK     itemRepo.get(params(\"id\").toInt) match {       case Some(x) =&gt; Ok(write(x));       case None =&gt; NotFound(\"Item with id \" + params(\"id\") + \" not found\");     }   This route listens for GET operations on the /items/:id url. Whenever a request is received, this function is invoked. In this function we first set the resulting content-type. I’m a proponent of creating custom media-types for my resources, so we set our result content-type to “application/vnd.smartbid.item+json”. Next we need to retrieve our item from our repository and serialize it to JSON.   For JSON serialization I’ve used lift-json. With this library you can automatically serialize case classes (or create and parse the json by hand). To use lift-json you need to add the following line to the libraryDependencies in build.sbt file and update the eclipse project from sbt..    \"net.liftweb\" %% \"lift-json\" % \"2.4\",    The code that writes our the class files as json is this one-liner     case Some(x) =&gt; Ok(write(x));   If we can find the item in the repository we write it out as json using the write function. We return this JSON as a “200 OK” response by using the scalatra OK function. If the resource can’t be find, we sent out a 404 using this one-liner.     case None =&gt; NotFound(\"Item with id \" + params(\"id\") + \" not found\");   For completeness sake I’ll list model and dummy repo implementation:   Model:  case class Item(     name:String,     id: Number,     startPrice: Number,     currency: String,     description: String,     links: List[Link] );  case class Link(     linkType: String,     rel: String,     href: String );  case class Bid(     id: Option[Long],     forItem: Number,     minimum: Number,     maximum: Number,     currency: String,     bidder: String,     date: Long );   Dummy repo:    class ItemRepository {     def get(id: Number) : Option[Item] = {              id.intValue() match {         case 123 =&gt; {            val l1 = new Link(\"application/vnd.smartbid.item\",\"Add item to watchlist\",\"/users/123/watchlist\");            val l2 = new Link(\"application/vnd.smartbid.bid\",\"Place bid on item\",\"/items/\" + id + \"/bid\");            val l3 = new Link(\"application/vnd.smartbid.user\",\"Get owner's details\",\"/users/123\");                   val item = new Item(         \t\t   \"Monty Python and the search for the holy grail\",         \t\t   id,         \t\t   0.69,         \t\t   \"GBP\",         \t\t   \"Must have item\",         \t\t   List(l1,l2,l3));                        Option(item);         };                  case _ =&gt; Option(null);       }     }          def delete(item: Item) = println(\"deleting user: \" + item) }   With this code we’ve got our first REST operation finished. We can test this service easily using Chrome’s Dev HTTP client I mentioned earlier:      In the response you can see a number of links, one of them is the following:      {     \"linkType\":\"application/vnd.smartbid.bid\",      \"rel\":\"Place bid on item\",     \"href\":\"/items/123/bid\"    }    You can see a href attribute here. We can follow this link to place a bid.   Make a bid on a specific item.   To do this we need to make a POST to “/items/123/bid” with a bid of type “application/vnd.smartbid.bid”. The format looks like this:        {     \"forItem\":123,     \"minimum\":20,     \"maximum\":10,     \"currency\":\"GBP\",     \"bidder\":\"jdirksen\",     \"date\":1347269593301     }    Let’s look once again at the code for this operation.     post(\"/items/:id/bid\", request.getContentType == \"application/vnd.smartbid.bid+json\") {     contentType = \"application/vnd.smartbid.bid+json\" \tvar createdBid = bidRepo.create(read[Bid](request.body)); \tCreated(write(createdBid), Map(\"Location\"-&gt;(\"/users/\" + createdBid.bidder + \"/bids/\"+createdBid.id.get)));   }   As you can see by the name, this operation listens for a POST on “/items/:id/bid”. Since I want the API to be media-type driven I added an extra condition to this route. With “ request.getContentType == “application/vnd.smartbid.bid+json” we require the clients of this operation to indicate that the type of resource they sent is of this specific type. The operation itself isn’t so complex. We set the content-type of the result and use a repository to create a bid. For this we use the read operation from lift-json to convert the incoming JSON to a scala object. The created object is returned with a “201 Created” status message and contains a location header that points to the resource that we just created.   Get a bid a user has made.   The final operation we support for now is a simple one where we can view the bid we just created. We know where to look, because the location of the just created resource was returned in the location header. The scala code for this function is shown here:     /**    * Route that matches retrieval of bids    */   get(\"/users/:user/bids/:bid\") {     contentType = \"application/vnd.smartbid.bid+json\"     bidRepo.get(params(\"bid\").toInt,params(\"user\")) match {       case Some(x) =&gt; Ok(write(x));       case None =&gt; NotFound(\"Bid with id \" + params(\"bid\") + \" not found for user: \" + params(\"user\") );     }   }   Pretty much the same as we’ve seen for the retrieval of the items. We retrieve the resource from a repository, if it exists we return it with a “200 OK”, if not we return a 404.   Test your service using specs2   In the final section for this part of the tutorial we’ll have a quick look at testing. When you create a new scalatra project as we’ve shown in the previous part, we also get a stub test we can extend to test our service. For this exampe I won’t write low level, simple JUnit tests, but we’ll create a specification that describes how our API should work. The code for a (part) of the specification is listed here:   package org.smartjava.scalatra  import org.scalatra.test.specs2._ import org.junit.runner.RunWith import org.scalatra.test.Client import org.specs2.SpecificationWithJUnit import org.eclipse.jetty.util.log.Log  /**  * Set of JUnit test to test our API  */ class HelloScalatraServletSpec extends ScalatraSpec {         // add the servlet so we can start testing                                              \t\t\t\t\t      addServlet(classOf[HelloScalatraServlet], \"/*\")      // some constants   val EXPECTED_BID = \"\"\"{\"id\":345,\"forItem\":123,\"minimum\":20,\"maximum\":10,\"currency\":\"GBP\",\"bidder\":\"jdirksen\",\"date\":1347285103671}\"\"\"   val BID_URL = \"/users/jdirksen/bids/345\";   val MED_TYPE = \"application/vnd.smartbid.bid+json\"         def is =     \"Calling an unknown url on the API \"            ^       \"returns status 404\"                           ! statusResult(\"/unknown\",404)^     \t\t\t\t\t\t\t\t\t\t\t\tend ^ p ^     \"Calling a GET on \" + BID_URL + \" should\"       ^     \"return status 200\"                             ! statusResult(BID_URL,200)^     \"and body should equal: \" + EXPECTED_BID        ! {get(BID_URL){response.body must_== EXPECTED_BID}}^     \"and media-type should equal: \" + MED_TYPE\t\t! {get(BID_URL){response.getContentType must startWith(MED_TYPE)}}     \t\t\t\t\t\t\t\t\t\t\t\tend     \t\t\t\t\t\t\t\t\t\t\t\t   def statusResult(url:String,code:Int) =      get(url) { \t  status must_== code   } }   For a more complete introduction into specs2 look at their website, I’ll just explain the code shown here. In this code we create a scenario the “def is” part. “is” contains a number of statements that must be true.      \"Calling an unknown url on the API \"            ^       \"returns status 404\"                           ! statusResult(\"/unknown\",404)^     \t\t\t\t\t\t\t\t\t\t\t\tend ^ p ^   The first test we do is checking what happens when we call an unknown URL on our API. We define that for this we expect a 404. We check this by calling the statusResult function. If 404 is returned this check will pass, if not we’ll see this in the results. The actual “statusResult” function is also defined in this file. This function uses the build in “get” function to make a call to our API, which runs embedded from this test.   Next we’re going to check how the get bid URL should work.      \"Calling a GET on \" + BID_URL + \" should\"       ^     \"return status 200\"                             ! statusResult(BID_URL,200)^     \"and body should equal: \" + EXPECTED_BID        ! {get(BID_URL){response.body must_== EXPECTED_BID}}^     \"and media-type should equal: \" + MED_TYPE\t\t! {get(BID_URL){response.getContentType must startWith(MED_TYPE)}}   As you can see, the same basic structure is followed. We run a number of checks that should pass. If we run this we can instantly see how our API should behave (instant documentation) and whether it confirms to our specification. This is the output from the test.    HelloScalatraServletSpec  Calling an unknown url on the API + returns status 404   Calling a GET on /users/jdirksen/bids/345 should + return status 200 + and body should equal: {\"id\":345,\"forItem\":123,\"minimum\":20,\"maximum\":10,\"currency\":\"GBP\",\"bidder\":\"jdirksen\",\"date\":1347285103671} + and media-type should equal: application/vnd.smartbid.bid+json   Total for specification HelloScalatraServletSpec Finished in 846 ms 4 examples, 0 failure, 0 error    Specs2 has a number of different ways it can be run. It can be run directly as a JUnit testcase, from maven or using it’s own launcher. Since I’m developping in Eclipse I wanted to run these tests directly from Eclipse. So I started out with the JUnit testrunner. The problem, however, with this runner is that it seems to conflict with the internally used Jetty from Eclipse. When I rant this the test tried to contact a Jetty instance on port 80, instead of using the embedded one it had started itself. To fix this I created a simple launcher that ran this test directly. To do this make the following launch configuration to get the output I just showed.   Run configuration part 1    Run configuration part 2    Now whenever you run this configuration the specs2 tests are run.   That’s it for this part of the tutorial. In the next part we’ll look at database access and using akka.   ","categories": ["posts","scala","scalatra"],
        "tags": [],
        "url": "http://www.smartjava.org/content/tutorial-getting-started-scala-and-scalatra-part-ii/",
        "teaser":null},{
        "title": "Hot code replace for scala / scalatra with free JRebel",
        "excerpt":"One of the (many) advantages of working with the Play framework is that you can start the server and have your changes automatically reloaded. This makes developing much easier and much less frustating. Recently I’ve started experimenting with scalatra as a lightweight framework for creating REST APIs. Even though there is some standard rudimentary support through sbt:    $ sbt &gt; container:start &gt; ~ ;copy-resources;aux-compile    This doesn’t really work that great. Sometimes scala / jetty / scalatra stops responding, I get out-of-memory errors and the code changes aren’t instantanious. When looking around for solutions I ran across JRebel. I knew it existed for Java, but they also have support for Scala. Better yet, zeroturnaround, offers a free version of JRebel specifically aimed at scala developers: http://zeroturnaround.com/software/jrebel/buy/   If you follow the instructions at their site (I installed JRebel using the Eclipse marketplace) you can enable this for your scala project. I enabled this for the sbt generated Eclipse project for my previous scala/scaltra article.   I used an embedded Jetty to start scalatra and whenever I now change a scala file it’s automatically reloaded:    [2012-09-12 20:53:53]  [2012-09-12 20:53:53] ############################################################# [2012-09-12 20:53:53]  [2012-09-12 20:53:53]  JRebel 5.0.1 (201207181843) [2012-09-12 20:53:53]  (c) Copyright ZeroTurnaround OU, Estonia, Tartu. [2012-09-12 20:53:53]  [2012-09-12 20:53:53]  Over the last 1 days JRebel prevented  [2012-09-12 20:53:53]  at least 1 redeploys/restarts saving you about 0 hours. [2012-09-12 20:53:53]  [2012-09-12 20:53:53]  This product is licensed to Jos Dirksen [2012-09-12 20:53:53]  for use with Scala classes only  [2012-09-12 20:53:53]  [2012-09-12 20:53:53]  License acquired through MyJRebel server. [2012-09-12 20:53:53]  [2012-09-12 20:53:53]  You are subscribed for the plan \"JRebel Scala Plan\", [2012-09-12 20:53:53]  subscription ends on 2013-09-12, [2012-09-12 20:53:53]  current subscription token is valid until 2012-10-12. [2012-09-12 20:53:53]  [2012-09-12 20:53:53]  The following plugins are disabled at the moment:  [2012-09-12 20:53:53]  * Apache MyFaces plugin (set -Drebel.myfaces_plugin=true to enable) [2012-09-12 20:53:53]  * Click plugin (set -Drebel.click_plugin=true to enable) [2012-09-12 20:53:53]  * JRuby Plugin (set -Drebel.jruby_plugin=true to enable) [2012-09-12 20:53:53]  * Jersey plugin (set -Drebel.jersey_plugin=true to enable) [2012-09-12 20:53:53]  * Oracle ADF Core plugin (set -Drebel.adf_core_plugin=true to enable) [2012-09-12 20:53:53]  * Oracle ADF Faces plugin (set -Drebel.adf_faces_plugin=true to enable) [2012-09-12 20:53:53]  * RESTlet plugin (set -Drebel.restlet_plugin=true to enable) [2012-09-12 20:53:53]  * Seam-Wicket plugin (set -Drebel.seam_wicket_plugin=true to enable) [2012-09-12 20:53:53]  * Spring Data Plugin (set -Drebel.spring_data_plugin=true to enable) [2012-09-12 20:53:53]  * WebObjects plugin (set -Drebel.webobjects_plugin=true to enable) [2012-09-12 20:53:53]  [2012-09-12 20:53:53] ############################################################# [2012-09-12 20:53:53]  [2012-09-12 20:53:53] JRebel: Directory '/Users/jos/Dev/scalatra/firststeps/hello-scalatra/target/scala-2.9.1/classes' will be monitored for changes. [2012-09-12 20:53:53] JRebel: Directory '/Users/jos/Dev/scalatra/firststeps/hello-scalatra/target/scala-2.9.1/test-classes' will be monitored for changes. [2012-09-12 20:53:53] JRebel: Directory '/Users/jos/Dev/scalatra/firststeps/hello-scalatra/bin' will be monitored for changes. [2012-09-12 20:55:10] JRebel: Reloading class 'org.smartjava.scalatra.HelloScalatraServlet'. [2012-09-12 20:55:16] JRebel: Reloading class 'org.smartjava.scalatra.routes.WebRoutes$class'. [2012-09-12 20:55:16] JRebel: Reloading class 'org.smartjava.scalatra.routes.RESTRoutes$class'. [2012-09-12 20:55:16] JRebel: Reloading class 'org.smartjava.scalatra.routes.RESTRoutes'. [2012-09-12 20:55:16] JRebel: Reloading class 'org.smartjava.scalatra.repository.BidRepository'. [2012-09-12 20:55:16] JRebel: Reloading class 'org.smartjava.scalatra.model.Bid'. [2012-09-12 20:55:54] JRebel: Reloading class 'org.smartjava.scalatra.repository.BidRepository'. [2012-09-12 20:58:19] JRebel: Reloading class 'org.smartjava.scalatra.routes.RESTRoutes'. [2012-09-12 20:58:19] JRebel: Reloading class 'org.smartjava.scalatra.repository.ItemRepository'. [2012-09-12 20:58:39] JRebel: Reloading class 'org.smartjava.scalatra.routes.RESTRoutes'. [2012-09-12 20:58:43] JRebel: Reloading class 'org.smartjava.scalatra.routes.RESTRoutes'. [2012-09-12 20:59:09] JRebel: Reloading class 'org.smartjava.scalatra.routes.RESTRoutes'. [2012-09-12 20:59:21] JRebel: Reloading class 'org.smartjava.scalatra.routes.RESTRoutes'. [2012-09-12 20:59:24] JRebel: Reloading class 'org.smartjava.scalatra.routes.RESTRoutes'.    Note though, that at least for scalatra, it doesn’t do everything. When I add a new route I still need to restart the server before I can  make a REST call to this route. Code changes for existing routes work normally though. I’ll have a look at the plugin mechanism for JRebel to see whether I can create a plugin for scalatra.  ","categories": ["posts"],
        "tags": [],
        "url": "http://www.smartjava.org/content/hot-code-replace-scala-scalatra-free-jrebel/",
        "teaser":null},{
        "title": "Access the Twitter REST API (v1.1) from Scala and Java using signpost",
        "excerpt":"If you’ve read some other articles on this blog you might know that I like creating visualizations of various datasets. I’ve just started a small project where I want to visualize some data from Twitter. For this I want to retrieve information about followers and profile information directly from twitter. I actually started looking for a set of all twitter accounts, but could only find one that was two years old. So, only option left, directly access the twitter API and get the data myself.   There are a couple of open source libraries out that we can use directly from Scala (or java) but as far as I could see they use the old v1 API and not the v1.1 API. The old API has a very strict data rate limit, which is a bit lighter in the new API. And besides that I’m more interested in the raw data and parsing the returning JSON isn’t that hard with Scala (or Java for that matter).   Register an application at twitter  First thing to do, and easiest way to get started is registing a new application for your twitter account. Go to https://dev.twitter.com/apps/new and create a new application. Don’t worry about the urls, since we won’t be using the OAuth callback mechanism:      Depending on what you want to do with the API you need to give additional permissions to this app. Default is “read-only”, if you want to allow your new application to post or access your direct messages you need to update the permissions. This is done from the settings page of your application:      Once you’ve created the application and setup the correct permissions you can generate an access token. Doing this will avoid having to go through the complete OAuth dance. You do this by going to your new app details and at the bottom select the “create my access token” option.      Now you’ll have a set of tokens (see the details part of your applications):      We’ll use these tokens to authenticate the requests we’ll make to twitter.   Using a OAuth library   The OAuth protocol is a pretty good documented protocol, but implementing it yourself is a lot of work and very error-prone. Luckily there are many OAuth libraries that can help you. I’ve tried a couple and the one that’s most easy to use (at least for me) was signpost. The examples below show how to do this from Scala, but you can follow the same approach for Java.   First the dependencies. I’ve used sbt and from signpost I use the client with support for HTTP commons. In sbt add the following:    .. libraryDependencies ++= Seq( \t\"oauth.signpost\" % \"signpost-core\" % \"1.2\", \t\"oauth.signpost\" % \"signpost-commonshttp4\" % \"1.2\",  \t\"org.apache.httpcomponents\" % \"httpclient\" % \"4.2\",          ... )    For maven you can use the same libraries. Next we can write a simple test to see if everything is working. In java it looks like this:   import oauth.signpost.OAuthConsumer; import oauth.signpost.commonshttp.CommonsHttpOAuthConsumer;  import org.apache.commons.io.IOUtils; import org.apache.http.HttpResponse; import org.apache.http.client.HttpClient; import org.apache.http.client.methods.HttpGet; import org.apache.http.impl.client.DefaultHttpClient;   public class Tw { \t \t  static String AccessToken = \"access token for your app\"; \t  static String AccessSecret = \"access secret for your app\"; \t  static String ConsumerKey = \"consumer key for your app\"; \t  static String ConsumerSecret = \"consumer secret for your app\"; \t   \t/** \t * @param args \t */ \tpublic static void main(String[] args) throws Exception { \t\tOAuthConsumer consumer = new CommonsHttpOAuthConsumer(                 ConsumerKey,                 ConsumerSecret);          consumer.setTokenWithSecret(AccessToken, AccessSecret);         HttpGet request = new HttpGet(\"http://api.twitter.com/1.1/followers/ids.json?cursor=-1&amp;screen_name=josdirksen\");         consumer.sign(request);          HttpClient client = new DefaultHttpClient();         HttpResponse response = client.execute(request);          int statusCode = response.getStatusLine().getStatusCode();         System.out.println(statusCode + \":\" + response.getStatusLine().getReasonPhrase());         System.out.println(IOUtils.toString(response.getEntity().getContent())); \t} }   And in Scala it looks pretty much the same:   import org.apache.http.client.HttpClient import org.apache.http.impl.client.DefaultHttpClient import org.apache.http.client.methods.HttpGet import oauth.signpost.commonshttp.CommonsHttpOAuthConsumer import org.apache.commons.io.IOUtils   object TwitterPull {  \t  val AccessToken = \"access token for your app\"; \t  val AccessSecret = \"access secret for your app\"; \t  val ConsumerKey = \"consumer key for your app\"; \t  val ConsumerSecret = \"consumer secret for your app\";          def main(args: Array[String]) { \t \t val consumer = new CommonsHttpOAuthConsumer(ConsumerKey,ConsumerSecret); \t consumer.setTokenWithSecret(AccessToken, AccessSecret); \t       val request = new HttpGet(\"http://api.twitter.com/1.1/followers/ids.json?cursor=-1&amp;screen_name=josdirksen\");      consumer.sign(request);      val client = new DefaultHttpClient();      val response = client.execute(request);            println(response.getStatusLine().getStatusCode());      println(IOUtils.toString(response.getEntity().getContent()));   } }   When you run this the output will look something like this:    200 {\"previous_cursor_str\":\"0\",\"next_cursor\":0,\"ids\":[48342167,21011010,824959303,97242821,16953163,218083367,20869799,5234221,13604142,804783128,271050984,405121284,26470609,50201837,1723451,374494377,120867838,14311946,253114713,39554511,7375412,42507395,112806109,92787154,218238023,110443797,76922155,198798790,294104985,305625416,217698029,21803482,14927822,15453445,15715866,15657036,186956616,36028164,70380613,326158542,573546312,14401332,521488579,9108612,576970378,293236313,16398366,16220300,15234937,32000283,439444353,14300622,67204409,155850135,14198255,32264673,15852981,313248158,20123099,608942046,234930032,36896958,18466675,45496942,330899833,18980755,88253383,461023805,31175627,11044952,142780445,63175189,107991607,94830953,600993241,6195002,115391430,550080945,381418927,168603682,142388604,8258462,218411138,30450578,77728346,2521381,182867524,494119147,29426983,572417260,94344849,325413275,389354525,501438275,164346498,22730282,8293302,21085554,341645357,56978853,180507788,10074002,22536424,14247654,581293627,15259428,483317230,462826270,4774641,15366832,96850673,278486993,22273826,17716679,14566626,158473088,20461042,161242434,43756629,40163100,141165981,5325152,7620782,266749648,524476136,557713614,39602637,18843154,1623,565954426,39639621,166672305,18683074,233118689,44876099,235258223,219310062,10699922,12660502,218030046,91552210,19361980,206645598,35346200,58440021,470388557,26495649,59066453,40292255,543375441,33242290,6015852,317150447,22935775,232300346,476045917,90913482,249088920,67658976,614873,522722520,186766721,285517705,71683175,131444964,166501605,477920664,38154550,18738205,8861832,15594932,18536741,7595202,465378842,11838952,14848133,431696576,14358671,414520167,222578501,67058139,28976735,95601387,426582611,24874129,418762594,128157235,106030956,31352215,18733178,260196778,153179029,91842580,229494512,83414433,285579699,19957600,54295155,14929418,51516573,200076011,18758733,17776895,59397841,216802709,149834999,327507356,8200322,174345369,108636400,27504001,326877592,139919716,49949338,215035403,118421144,49410665,149550914,18446431,25662335,261725134,267634174,57737391,146506056,126964949,71055234,20870640,210196418,222806923,13290742,72247756,180410163,14784480,36684216,25611502,95614691,54629161,112967594,181656257,17994312,72918901,140082918,149087212,137272324,99534020,121755576,93964779,35848342,43059008,34704029,87672717,113137792,17863333,90407665,90591814,54297023,57924897,87551006,28300354,48990752,26188013],\"previous_cursor\":0,\"next_cursor_str\":\"0\"}    If you get a 403 check whether the tokens match.  ","categories": ["posts","twitter","scala"],
        "tags": [],
        "url": "http://www.smartjava.org/content/access-twitter-rest-api-v11-scala-and-java-using-signpost/",
        "teaser":null},{
        "title": "Tutorial: Getting started with scala and scalatra - Part III",
        "excerpt":"This post is the third on a series of articles I’m writing on scalatra. In “part I” we created the initial environment, and in “part II” we created the first part of a REST API and added some tests. In this third part of the scalatra tutorial we’re going to look at the following topics:      Persistency: we use scalaquery to persist elements from our model.   Security: handle a security header containing an API key.   Frist we’ll look at the persistency part. For this part we’ll be using scalaquery. Note that the code we show here is pretty much the same for scalaquery’s successor slick. Slick, however, requires scala 2.10.0-M7 and this would mean we have to alter our complete scala setup. So for this example we’ll just use scalaquery (whose syntax is the same of slick). If you haven’t done so already, install JRebel so your changes are reflected instantly without having to restart the service.   Persistency   I’ve used postgresql for this example, but any of the databases supported by scalaquery can be used. The database model I’ve used is a very simple one:    CREATE TABLE sc_bid (   id integer NOT NULL DEFAULT nextval('sc_bid_id_seq1'::regclass),   \"for\" integer,   min numeric,   max numeric,   currency text,   bidder integer,   date numeric,   CONSTRAINT sc_bid_pkey1 PRIMARY KEY (id ),   CONSTRAINT sc_bid_bidder_fkey FOREIGN KEY (bidder)       REFERENCES sc_user (id) MATCH SIMPLE       ON UPDATE NO ACTION ON DELETE NO ACTION,   CONSTRAINT sc_bid_for_fkey FOREIGN KEY (\"for\")       REFERENCES sc_item (id) MATCH SIMPLE       ON UPDATE NO ACTION ON DELETE NO ACTION )  CREATE TABLE sc_item (   id integer NOT NULL DEFAULT nextval('sc_bid_id_seq'::regclass),   name text,   price numeric,   currency text,   description text,   owner integer,   CONSTRAINT sc_bid_pkey PRIMARY KEY (id ),   CONSTRAINT sc_bid_owner_fkey FOREIGN KEY (owner)       REFERENCES sc_user (id) MATCH SIMPLE       ON UPDATE NO ACTION ON DELETE NO ACTION )  CREATE TABLE sc_user (   id serial NOT NULL,   username text,   firstname text,   lastname text,   CONSTRAINT sc_user_pkey PRIMARY KEY (id ) )    As you can a simple model, with a couple of foreign keys and primary keys that are autogenerated. We define a table for the users, for the items and for the bids. Note that this is database specific so this will only work for postgresql. An additional note on postgresql and scalaquery. Scalaquery doesn’t support schemas. This means that we have to define the tables in the ‘public’ schema.   Before we can start working with scalaquery we first have to add it to our project. In the build.sbt add the following dependencies      \"org.scalaquery\" %% \"scalaquery\" % \"0.10.0-M1\",   \"postgresql\" % \"postgresql\" % \"9.1-901.jdbc4\"    After updating you’ll have the scalaquery and postgres jars you need. Lets look at one of the repositories: the bidrepository and the RepositoryBase trait.   // the trait import org.scalaquery.session.Database  trait RepositoryBase {   val db = Database.forURL(\"jdbc:postgresql://localhost/dutch_gis?user=jos&amp;password=secret\", driver = \"org.postgresql.Driver\") }  // simple implementation of the bidrepository package org.smartjava.scalatra.repository import org.smartjava.scalatra.model.Bid import org.scalaquery.session._ import org.scalaquery.ql.basic.{BasicTable =&gt; Table} import org.scalaquery.ql.TypeMapper._ import org.scalaquery.ql._ import org.scalaquery.ql.extended.PostgresDriver.Implicit._ import org.scalaquery.session.Database.threadLocalSession  class BidRepository extends RepositoryBase {      object BidMapping extends Table[(Option[Long], Long, Double, Double, String, Long, Long)](\"sc_bid\") {       def id = column[Option[Long]](\"id\", O PrimaryKey)       def forItem = column[Long](\"for\", O NotNull)       def min = column[Double](\"min\", O NotNull)       def max = column[Double](\"max\", O NotNull)       def currency = column[String](\"currency\")       def bidder = column[Long](\"bidder\", O NotNull)       def date = column[Long](\"date\", O NotNull)              def noID = forItem ~ min ~ max ~ currency ~ bidder ~ date       def * = id ~ forItem ~ min ~ max ~ currency ~ bidder ~ date   }     /**    * Return a Option[Bid] if found or None otherwise    */   def get(bid: Long, user: String) : Option[Bid] = {       var result:Option[Bid] = None;              db withSession {           // define the query and what we want as result     \t  val query = for (u &lt;-BidMapping if u.id === bid) yield u.id ~ u.forItem ~ u.min ~ u.max ~ u.currency ~ u.bidder ~ u.date     \t       \t  // map the results to a Bid object     \t  val inter = query mapResult {     \t    case(id,forItem,min,max,currency,bidder,date) =&gt; Option(new Bid(id,forItem, min, max, currency, bidder, date));     \t  }     \t       \t  // check if there is one in the list and return it, or None otherwise     \t  result = inter.list match {     \t    case _ :: tail =&gt; inter.first     \t    case Nil =&gt; None     \t  }       }              // return the found bid       result     }          /**      * Create a bid using scala query. This will always create a new bid      */     def create(bid: Bid): Bid = {       var id: Long = -1;              // start a db session       db withSession {         // create a new bid         val res = BidMapping.noID insert (bid.forItem.longValue, bid.minimum.doubleValue, bid.maximum.doubleValue, bid.currency, bid.bidder.toLong, System.currentTimeMillis());         // get the autogenerated bid         val idQuery = Query(SimpleFunction.nullary[Long](\"LASTVAL\"));         id = idQuery.list().head;       }       // create a bid to return       val createdBid = new Bid(Option(id), bid.forItem, bid.minimum, bid.maximum, bid.currency, bid.bidder, bid.date);       createdBid;     }          /**      * Delete a bid      */     def delete(user:String, bid: Long) : Option[Bid] = {       // get the bid we're deleting       val result = get(bid,user);              // delete the bid       val toDelete = BidMapping where (_.id === bid)       db withSession {         toDelete.delete       }              // return deleted bid       result     } }   Looks complex, right? We’ll it isn’t once you’ve got the hang of how scalaquery works. With scalaquery you create a table mapping. In this mapping you specify the type of fields you expect. In this example our mapping table looks like this:     object BidMapping extends Table[(Option[Long], Long, Double, Double, String, Long, Long)](\"sc_bid\") {       def id = column[Option[Long]](\"id\", O PrimaryKey)       def forItem = column[Long](\"for\", O NotNull)       def min = column[Double](\"min\", O NotNull)       def max = column[Double](\"max\", O NotNull)       def currency = column[String](\"currency\")       def bidder = column[Long](\"bidder\", O NotNull)       def date = column[Long](\"date\", O NotNull)              def noID = forItem ~ min ~ max ~ currency ~ bidder ~ date       def * = id ~ forItem ~ min ~ max ~ currency ~ bidder ~ date   }   Here we define the mapping of the table “sc_bid”. For each field, we define the name of the column and it’s type. If we want we can add specific options that are taken into account when you create your ddl from this (not something I’ve used for this example). The last two defs define the ‘constructors’ for this mapping. The “def *” is the default constructor, where we have all the fields beforehand, the “def noID” is the one we’ll use when we create a bid for this first time and we don’t have an id yet. Remember the ids are autogenerated by the database. With this mapping we can start writing our repository functions. Lets start with the first one: get     /**    * Return a Option[Bid] if found or None otherwise    */   def get(bid: Long, user: String) : Option[Bid] = {       var result:Option[Bid] = None;              db withSession {           // define the query and what we want as result     \t  val query = for (u &lt;-BidMapping if u.id === bid) yield u.id ~ u.forItem ~ u.min ~ u.max ~ u.currency ~ u.bidder ~ u.date     \t       \t  // map the results to a Bid object     \t  val inter = query mapResult {     \t    case(id,forItem,min,max,currency,bidder,date) =&gt; Option(new Bid(id,forItem, min, max, currency, bidder, date));     \t  }     \t       \t  // check if there is one in the list and return it, or None otherwise     \t  result = inter.list match {     \t    case _ :: tail =&gt; inter.first     \t    case Nil =&gt; None     \t  }       }              // return the found bid       result     }   Here you can see that we use the standard scala for construct to create a query iterate over the table mapped with BidMapping. To make sure we only get the field we want we apply a filter using the “if u.id === bid” statement. In the yield statement we specify the fields we want to return. By using the mapResult on the query we can process the results from the query and convert it to our case object and add it to a list. We then check whether there really is something in the list and return an Option[Bid]. Note that this can be written more concise, but this nicely explains the steps you need to take.   The next function is create   def create(bid: Bid): Bid = {       var id: Long = -1;              // start a db session       db withSession {         // create a new bid         val res = BidMapping.noID insert (bid.forItem.longValue, bid.minimum.doubleValue, bid.maximum.doubleValue, bid.currency, bid.bidder.toLong, System.currentTimeMillis());         // get the autogenerated bid         val idQuery = Query(SimpleFunction.nullary[Long](\"LASTVAL\"));         id = idQuery.list().head;       }       // create a bid to return       val createdBid = new Bid(Option(id), bid.forItem, bid.minimum, bid.maximum, bid.currency, bid.bidder, bid.date);       createdBid;     }   We now use the custom BidMapping ‘constructor’ noID to generate an insert statement. If we didn’t specify noID we are required to already specify an id. Now that we’ve inserted a new Bid object in the database, we need to return the just created Bid, with the new id, to the user. For this we need to execute a simple query called “LASTVAL”, which returns the last autogenerated value. In our case, this is the id of the bid that was created. From this information we create a new Bid, which we return.   The last operation for our repository is the delete function. This function first checks whether the specified bid is present, and if it is, it deletes it.       def delete(user:String, bid: Long) : Option[Bid] = {       // get the bid we're deleting       val result = get(bid,user);              // delete the bid       val toDelete = BidMapping where (_.id === bid)       db withSession {         toDelete.delete       }              // return deleted bid       result     }   Here we use the ‘where’ filter to create the query we want to execute. When we call delete on this filter all matching elements are deleted. And that’s the most basic use of scalaquery for persistency. If you need more complex operations (like joins) look at the scalaquery.org website for examples.   We now have functionality to create and delete bids. So it would also be nice if we have some way to authenticate our users. For this tutorial we’re going to create a very simple API Key based authentication scheme. For every request the user has to add a specific header with its API key. Then we can use the information from this key to determine who this user is, and whether he can delete or access specific information.   Security   We’ll start with the key generation part. When someone wants to use our API we require them to specify an application name and the hostname from which the request will be made. This information we’ll use to generate a key they have to use in each request. This key is just a simple HMAC hash.   package org.smartjava.scalatra.util  import javax.crypto.spec.SecretKeySpec import javax.crypto.Mac import org.apache.commons.codec.binary.Base64  object SecurityUtil {    def calculateHMAC(secret: String, applicationName: String , hostname: String ) : String  = {     val signingKey = new SecretKeySpec(secret.getBytes(),\"HmacSHA1\");     val mac = Mac.getInstance(\"HmacSHA1\");     mac.init(signingKey);     val rawHmac = mac.doFinal((applicationName + \"|\" + hostname).getBytes());          new String(Base64.encodeBase64(rawHmac));   }      def checkHMAC(secret: String, applicationName: String, hostname: String, hmac: String) : Boolean  = {     return calculateHMAC(secret, applicationName, hostname) == hmac;   }      def main(args: Array[String]) {     val hmac = SecurityUtil.calculateHMAC(\"The passphrase to calculate the secret with\",\"App 1\",\"localhost\");     println(hmac);     println(SecurityUtil.checkHMAC(\"The passphrase to calculate the secret with\",\"App 1\",\"localhost\",hmac));   } }   The above helper object is used to calculate the initial hash we send to the user and can be used to validate an incoming hash.  To use this in our REST API we need to intercept all the incoming requests and check these headers before invoking the specific route. With scalatra we can do this by using the before() function:   package org.smartjava.scalatra.routes import org.scalatra.ScalatraBase import org.smartjava.scalatra.repository.KeyRepository  /**  * When this trait is used, the incoming request  * is checked for authentication based on the  * X-API-Key header.  */ trait Authentication extends ScalatraBase {      val ApiHeader = \"X-API-Key\";   val AppHeader = \"X-API-Application\";   val KeyChecker = new KeyRepository;       /**    * A simple interceptor that checks for the existence     * of the correct headers    */   before() {     // we check the host where the request is made     val servername = request.serverName;     val header = Option(request.getHeader(ApiHeader));     val app = Option(request.getHeader(AppHeader));          List(header,app) match {       case List(Some(x),Some(y)) =&gt; isValidHost(servername,x,y);       case _ =&gt; halt(status=401, headers=Map(\"WWW-Authenticate\" -&gt; \"API-Key\"));     }   }      /**    * Check whether the host is valid. This is done by checking the host against    * a database with keys.    */   private def isValidHost(hostName: String, apiKey: String, appName: String): Boolean = {     KeyChecker.validateKey(apiKey, appName, hostName);   }  }   This trait, which we include in our main scalatra servlet, gets the correct information from the request and checks whether the supplied hash corresponds to the one generated by the code you saw previously. If this is the case the request is passed on, if not, we halt the processing of the request and send back a 401 explaining how to authenticate with this API.   If a client omits these headers he’ll get this as a response:    If a client sends the correct headers he’ll get this response:  That’s it for this part. In the next part we’ll look at Depdency Injection, CQRS, Akka and running this code in the cloud.  ","categories": ["posts","scalatra","scala"],
        "tags": [],
        "url": "http://www.smartjava.org/content/tutorial-getting-started-scala-and-scalatra-part-iii/",
        "teaser":null},{
        "title": "Deep dive into chrome web intents",
        "excerpt":"In a previous article I already wrote about the web intents API. At that time the web intents feature was very experimental, but already usuable. In this article we’ll look a bit deeper at the newly added feature to chrome d (23+). In the latest dev versions of chrome, Google changed the bookmark star, to a plus icon:      When you click this button, you can still add the site to your bookmarks, but you can also share this page (or send it to one of your mobile devices. What happens when you click the + button? When I do this on my Chrome installation I see the following:      Chrome will show you all the services you’ve installed from the Chrome store that can handle this sharing request. For me locally it lists two services I’ve already installed and shows a couple from the Chrome web store I can install to handle the sharing request. The two local ones are:     The simple sharing application I created in a previous article.    The web intents debugger, which you can use to see the details of the sharing request.   This will look something like this:      So what happens when we click the + button on Chrome and select a service to handle the request? In the screenshot you can see that using the web intents API the selected service receives the following data:    Action: http://webintents.org/share Type: text/uri-list Data: 'url-to-be-shared'    Now that we now what is shared by the + button we can create our own application to respons to this share request. In the previous example I created a simple application that used web intents to share a link on twitter. This time lets create a simple service that you can use to directly add a page to your del.icio.us list. In the previous example  I used Twitter, but the concepts are pretty much the same.  To share from the ‘+’ button we need to do the following:      Create the code to share to delicious   Register as plugin in Chrome   Access from the '+' button   First lets look at the sharing code, the complete code is shown here:   &lt;html&gt; &lt;head&gt;     &lt;title&gt;Post to Delicious&lt;/title&gt;     &lt;script type=\"text/javascript\" src=\"js/jquery-1.7.2.js\"&gt;&lt;/script&gt; &lt;/head&gt; &lt;body&gt;  &lt;script type=\"text/javascript\"&gt;     // redirect to delicious sharing page     window.location.href='https://www.delicious.com/save?v=5&amp;noui&amp;jump=close&amp;url='             +encodeURIComponent(window.webkitIntent.data)             +'&amp;title=Enter title',             'delicious','toolbar=no,width=550,height=550';  &lt;/script&gt; &lt;/body&gt; &lt;/html&gt;   Not much explanation is needed I think. We get the information from the received intent (the window.webkitIntent element) and use it to create an url. This user is then redirect  to this url, which shows something like this:      Which you can use to directly post the link to Delicious. Easy, right?   All we need to do next is register this code as an extension in chrome, so that it can be called from the ‘+’ sharing option. For this we need to create a chrome extension. For details see my other web intents article but let me just list the highlights here.   First you need to create a manifest which tells chrome where to share the link to, and in what webintents types you’re interested. In the manifest I used I point to a site running on my local machine:    {     \"name\":\"Delicious share\",     \"version\":\"4\",     \"icons\":{         \"128\":\"share-this-icon.png\"     },     \"app\":{         \"urls\":[             \"http://127.0.0.1\"         ],         \"launch\":{             \"web_url\":\"http://127.0.0.1\"         }     },     \"manifest_version\":3,     \"intents\":{         \"http://webintents.org/share\":[             {                 \"type\":[\"text/uri-list\"],                 \"href\":\"http://127.0.0.1/dev/webintentsdelicious/index.html\",                 \"disposition\":\"window\",                 \"title\":\"Post to Delicious\"             }         ]     } }    The interesting part is the intents section. Here is specify which intents I want to process and what I want to do with them. In this case we open the local webpage which I showed earlier. To get this into chrome open the extension screen (Window-&gt;Extensions) and enable developer mode:      From here use the “load unpacked extension…” and navigate to the folder where your created your manifest. This will load the extension and register the intent. Now when you’re on an interesting link you can use the ‘+’ button to share the current page using the intent you just added.      And when you select this service, you’re shown the delicous share interface.      Very easy to do, and I can’t wait for other types of intent actions to come available.  ","categories": ["posts","chrome","web intents"],
        "tags": [],
        "url": "http://www.smartjava.org/content/deep-dive-chrome-web-intents/",
        "teaser":null},{
        "title": "Record audio using webrtc in chrome and speech recognition with websockets",
        "excerpt":"There are many different web api standards that are turning the web browser into a complete application platform. With websockets we get nice asynchronous communication, various standards allow us access to sensors in laptops and mobile devices and we can even determine how full the battery is. One of the standards I’m really interested in is webrtc. With webrtc we can get real-time audio and video communication between browsers without needing plugins or additional tools. A couple of months ago I wrote about how you can use webrtc to access the webcam and use it for face recognition. At that time, none of the browser allowed you to access the microphone. A couple of months later though, and both the developer version of firefox and developer version of chrome, allow you to access the microphone!   So let’s see what we can do with this. Most of the examples I’ve seen so far focus on processing the input directly, within the browser, using the Web Audio API. You get synthesizers, audio visualizations, spectrometers etc. What was missing, however, was a means of recording the audio data and storing it for further processing at the server side. In this article I’ll show you just that. I’m going to show you how you can create the following (you might need to enlarge it to read the response from the server):     In this screencast you can see the following:      A simple HTML page that access your microphone   The speech is recorded and using websockets is sent to a backend   The backend combines the audio data   And sends it to google's speech to text API   The result from this API call is returned to the browser   And all this is done without any plugins in the browser! So what’s involved to accomplish all this.   Allowing access to your microphone   The first thing you need to do is make sure you’ve got an up to date version of chrome. I use the dev build, and am currently on this version:      Since this is still an experimental feature we need to enable this using the chrome flags.      Make sure the “Web Audio Input” flag is enabled. With this configuration out of the way we can start to access our microphone.   Access the audio stream from the microphone   This is actually very easy:              function callback(stream) {                var context = new webkitAudioContext();                var mediaStreamSource = context.createMediaStreamSource(stream);                ...            }             $(document).ready(function() {                navigator.webkitGetUserMedia({audio:true}, callback);                ...            }    As you can see I use the webkit prefix functions directly, you could, of course, also use a shim so it is browser independent. What happens in the code above is rather straightforward. We ask, using getUserMedia, for access to the microphone. If this is successful our callback gets called with the audio stream as its parameter. In this callback we use the Web Audio specification to create a mediaStreamSource from our microphone. With this mediaStreamSource we can do all the nice Web Audio tricks you can see here.   But we don’t want that, we want to record the stream and send it to a backend server for further processing. In future versions this will probably be possible directly from the webrtc API, at this time, however, this isn’t possible yet. Luckily, though, we can use a feature from the Web Audio API to get access to the raw data. With the JavaScriptAudioNode we can create a custom node, which we can use to access the raw data (which is PCM encoded). Before I started my own work on this I searched around a bit and came across the recoder.js project from here: https://github.com/mattdiamond/Recorderjs. Matt created a recorder that can record the output from Web Audio nodes, and that’s exactly what I needed.   All I needed to do now was connect the stream we just created to the recorder library:               function callback(stream) {                var context = new webkitAudioContext();                var mediaStreamSource = context.createMediaStreamSource(stream);                rec = new Recorder(mediaStreamSource);            }    With this code, we create a recorder from our stream. This recorder provides the following functions:      record: Start recording from the input   stop: Stop recording   clear: Clear the current recording   exportWav: Export the data as a wav file   Connect the recorder to the buttons   I’ve created a simple webpage with an output for the text and two buttons to control the recording:      The ‘record’ button starts the recording, and once you hit the ‘Export’ button the recording stops, and is sent to the backend for processing.   Record button:               $('#record').click(function() {                    rec.record();                    ws.send(\"start\");                    $(\"#message\").text(\"Click export to stop recording and analyze the input\");                                        // export a wav every second, so we can send it using websockets                    intervalKey = setInterval(function() {                        rec.exportWAV(function(blob) {                            rec.clear();                            ws.send(blob);                        });                    }, 1000);                });    This function (using jQuery to connect it to the button) when clicked starts the recording. It also uses a websocket (ws), see further down on how to setup the websocket, to indicate to the backend server to expect a new recording (more on this later). Finally when the button is clicked an interval is created that passes the data to the backend, encoded as wav file, every second. We do this to avoid sending too large chunks of data to the backend and improve performance.   Export button:                   $('#export').click(function() {                    // first send the stop command                    rec.stop();                    ws.send(\"stop\");                    clearInterval(intervalKey);                    ws.send(\"analyze\");                    $(\"#message\").text(\"\");                });    The export button, bad naming I think when I’m writing this, stops the recording, the interval and informs the backend server that it can send the received data to the google api for further processing.   Connecting the frontend to the backend   To connect the webapplication to the backend server we use websockets. In the previous code fragments you’ve already seen how they are used. We create them with the following:                   var ws = new WebSocket(\"ws://127.0.0.1:9999\");                ws.onopen = function () {                    console.log(\"Openened connection to websocket\");                };                 ws.onmessage = function(e) {                    var jsonResponse = jQuery.parseJSON(e.data );                    console.log(jsonResponse);                    if (jsonResponse.hypotheses.length &gt; 0) {                       var bestMatch = jsonResponse.hypotheses[0].utterance;                        $(\"#outputText\").text(bestMatch);                    }                }    We create a connection, and when we receive a message from the backend we just assume it contains the response to our speech analysis. And that’s it for the complete front end of the application. We use getUserMedia to access the microphone, use the Web Audio API to get access to the raw data and communicate with websockets with the backend server.   The backend server   Our backend server needs to do a couple of things. It first needs to combine the incoming chunks to a single audio file, next it needs to convert this to a format google APIs expect, which is flac. Finally we make a call to the google api and return the response.   I’ve used Jetty as the websocket server for this example. If you want to know the details about setting this up, look at the facedetection example. In this article I’ll only show the code to process the incoming messages.   First step, combine the incoming data   The data we receive is encoded as wav (thanks to the recorder.js library we don’t have to do this ourselves). In our backend we thus receive sound fragments with a length of one second. We can’t just concatenate these together, since wav files have a header that tells how long the fragment is (amongst other things), so we have to combine them, and rewrite the header. Lets first look at the code (ugly code, but works good enough for now :)   public void onMessage(byte[] data, int offset, int length) { \t\t\t \t\t\tif (currentCommand.equals(\"start\")) { \t\t\t\ttry { \t\t\t\t\t// The temporary file that contains our captured audio stream \t\t\t\t\tFile f = new File(\"out.wav\"); \t\t\t\t\t \t\t\t\t\t// if the file already exists we append it. \t\t\t\t\tif (f.exists()) { \t\t\t\t\t\tLOG.info(\"Adding received block to existing file.\"); \t\t\t\t\t\t \t\t\t\t\t\t// two clips are used to concat the data \t\t\t\t\t\t AudioInputStream clip1 = AudioSystem.getAudioInputStream(f); \t\t\t\t\t\t AudioInputStream clip2 = AudioSystem.getAudioInputStream(new ByteArrayInputStream(data)); \t\t\t\t\t\t  \t\t\t\t\t\t // use a sequenceinput to cat them together \t\t\t\t\t\t AudioInputStream appendedFiles =  \t\t                            new AudioInputStream( \t\t                                new SequenceInputStream(clip1, clip2),      \t\t                                clip1.getFormat(),  \t\t                                clip1.getFrameLength() + clip2.getFrameLength()); \t \t\t\t\t\t\t // write out the output to a temporary file \t\t                    AudioSystem.write(appendedFiles,  \t\t                            AudioFileFormat.Type.WAVE, \t\t                            new File(\"out2.wav\")); \t\t                     \t\t                    // rename the files and delete the old one \t\t                    File f1 = new File(\"out.wav\"); \t\t                    File f2 = new File(\"out2.wav\"); \t\t                    f1.delete(); \t\t                    f2.renameTo(new File(\"out.wav\")); \t\t\t\t\t} else { \t\t\t\t\t\tLOG.info(\"Starting new recording.\"); \t\t\t\t\t\tFileOutputStream fOut = new FileOutputStream(\"out.wav\",true); \t\t\t\t\t\tfOut.write(data); \t\t\t\t\t\tfOut.close(); \t\t\t\t\t}\t\t\t \t\t\t\t} catch (Exception e) {\t...} \t\t\t} \t\t}   This method gets called for each chunk of audio we receive from the browser. What we do here is the following:      First, we check whether we have a temp audio file, if not we create it   If the file exists we use Java's AudioSystem to create an audio sequence   This sequence is then written to another file   The original is deleted and the new one is renamed.   We repeat this for each chunk   So at this point we have a wav file that keeps on growing for each added chunk. Now before we convert this, lets look at the code we use to control the backend.   public void onMessage(String data) { \t\t\tif (data.startsWith(\"start\")) { \t\t\t\t// before we start we cleanup anything left over \t\t\t\tcleanup(); \t\t\t\tcurrentCommand = \"start\"; \t\t\t} else if (data.startsWith(\"stop\")) { \t\t\t\tcurrentCommand = \"stop\"; \t\t\t} else if (data.startsWith(\"clear\")) { \t\t\t\t// just remove the current recording \t\t\t\tcleanup(); \t\t\t} else if (data.startsWith(\"analyze\")) {                         // convert to flac \t               ... \t                // send the request to the google speech to text service \t                ...                        }            }   The previous method responded to binary websockets messages. The one shown above responds to string messages. We use this to control, from the browser, what the backend should do. Let’s look at the analyze command, since that is the interesting one. When this command is issued from the frontend the backend needs to convert the wav file to flac and send it to the google service.   convert to flac   For the conversion to flac we need an external library since Java standard has no support for this. I used the javaflacencoder from here for this.   // get an encoder FLAC_FileEncoder flacEncoder = new FLAC_FileEncoder(); \t                 // point to the input file File inputFile = new File(\"out.wav\"); File outputFile = new File(\"out2.flac\");  // encode the file LOG.info(\"Start encoding wav file to flac.\"); flacEncoder.encode(inputFile, outputFile); LOG.info(\"Finished encoding wav file to flac.\");   Easy as that. Now we got a flac file that we can send to google for analysis.   Send to google for analysis   A couple of weeks ago I ran across an article that explained how someone analyzed chrome and found out about an undocumented google API you can use for speech to text. If you post a flac file to this url: https://www.google.com/speech-api/v1/recognize?xjerr=1&amp;client=chromium&amp;lang=en-US you receive a response like this:    {     \"status\": 0,     \"id\": \"ae466ffa24a1213f5611f32a17d5a42b-1\",     \"hypotheses\": [     {         \"utterance\": \"the quick brown fox\",         \"confidence\": 0.857393     }] }    To do this from java code, using HTTPClient, you do the following:    // send the request to the google speech to text service LOG.info(\"Sending file to google for speech2text\"); HttpClient client = new DefaultHttpClient(); HttpPost p = new HttpPost(URL); p.addHeader(\"Content-Type\", \"audio/x-flac; rate=44100\");  p.setEntity(new FileEntity(outputFile, \"audio/x-flac; rate=44100\")); \t                 HttpResponse response = client.execute(p); f (response.getStatusLine().getStatusCode() == 200) { \tLOG.info(\"Received valid response, sending back to browser.\"); \tString result = new String(IOUtils.toByteArray(response.getEntity().getContent())); \tthis.connection.sendMessage(result); }   And that are all the steps that are needed.     ","categories": ["posts","websockets","chrome","webrtc"],
        "tags": [],
        "url": "http://www.smartjava.org/content/record-audio-using-webrtc-chrome-and-speech-recognition-websockets/",
        "teaser":null},{
        "title": "Tutorial: Getting started with scala and scalatra - Part IV",
        "excerpt":"Welcome to the last part of this series of tutorials on scala and scalatra. In this part we’ll look at how you can use Akka to handle your requests using an asynchronous dispatcher, how to use subcut for dependency injection and finally how you can run the complete API in the cloud. In this example I’ve used openshift from JBoss to run the API on JBoss Application Server 7.1. Now what have we seen in the previous tutorials:      Tutorial I: Setup scala and scalatra for use within Eclipse and create your first application.   Tutorial II: Start scalatra embedded, create a REST api that used JSON and test with specs2   Tutorial III: Add persistency with scalaquery and add a hmac based security layer   The examples in this tutorial assume you’ve completed the previous three tutorials. We won’t show all the details, but focus on adding new functionality to the existing application (from part III). To be precise in this example we’ll show you the following steps:      First, we'll introduce subcut to the application for dependency injection   Next, we'll make our requests asynchronous by using Akka's futures   Fiinally, we'll enable CORS, package the application and deploy it to openshift   And we’ll have an API we can call on the openshift cloud.      Let’s start with subcut   Adding dependency injection to the application   In java there are many dependency injection frameworks. Most people have heard of Spring and Guice and dependency injection even has its own JSR and specifications. In scala, however, this isn’t the case. There has been lots of talk whether scala application need a DI framework, since these concepts can also be applied using standard Scala language constructs. When you start investigating dependency injection for Scala you’ll quickly run into the cake pattern (see here for a very extensive explanation). I won’t go into the details why you should or should not use the cake pattern, but for me personally it felt like it introduced too much cruft and glue code and I wanted something simpler. For this article I’m going to use subcut. Subcut is a really small and simple to use framework, which makes using DI in scala very easy and unobtrusive.   Nothing works like examples. So what do you need to do have subcut manage your dependencies. First of, we of course need to nicely separate our implementation from our interface/trait. In part III we create a set of repositories which we used directly from the scalatra routes by creating them as class variables:     // repo stores our items   val itemRepo = new ItemRepository;   val bidRepo = new BidRepository;   The problem is this binds our routes directly to the implementation, which is something we don’t want. So first lets expand the repositories by defining a trait for these repositories.   trait BidRepo {    def get(bid: Long, user: String) : Option[Bid]   def create(bid: Bid): Bid    def delete(user:String, bid: Long) : Option[Bid]  }  trait ItemRepo {   def get(id: Number) : Option[Item]   def delete(id: Number) : Option[Item] }  trait KeyRepo {   def validateKey(key: String, app:String, server: String): Boolean }   Nothing out of the ordinary. We use this trait from our implementations, like shown below, and we’re done.   class BidRepository extends RepositoryBase with BidRepo {  ... }   Now that we’ve defined our traits, we can start using subcut to manage our dependencies. For this we need a couple of things:      Which implementation are bound to what trait   Which classes need to have resources injected   Bootstrap the 'root' object with our configuration   Before we start. We first need to update our build.sbt with the subcut dependency and add the correct repository.    libraryDependencies ++= Seq(   \"com.escalatesoft.subcut\" %% \"subcut\" % \"2.0-SNAPSHOT\",   \"org.scalaquery\" %% \"scalaquery\" % \"0.10.0-M1\",   \"postgresql\" % \"postgresql\" % \"9.1-901.jdbc4\",   \"net.liftweb\" %% \"lift-json\" % \"2.4\",   \"org.scalatra\" % \"scalatra\" % \"2.1.0\",   \"org.scalatra\" % \"scalatra-scalate\" % \"2.1.0\",   \"org.scalatra\" % \"scalatra-specs2\" % \"2.1.0\",   \"org.scalatra\" % \"scalatra-akka\" % \"2.1.0\",   \"ch.qos.logback\" % \"logback-classic\" % \"1.0.6\" % \"runtime\",   \"org.eclipse.jetty\" % \"jetty-webapp\" % \"8.1.5.v20120716\" % \"container\",   \"org.eclipse.jetty\" % \"test-jetty-servlet\" % \"8.1.5.v20120716\" % \"test\",   \"org.eclipse.jetty.orbit\" % \"javax.servlet\" % \"3.0.0.v201112011016\" % \"container;provided;test\" artifacts (Artifact(\"javax.servlet\", \"jar\", \"jar\")) )  resolvers ++= Seq(\"Scala-Tools Maven2 Snapshots Repository\" at \"https://oss.sonatype.org/content/groups/public/\",                   \"Typesafe Repository\" at \"http://repo.typesafe.com/typesafe/releases/\")    This not only adds the subcut dependencies, but also the akka onces we’ll see further in this article.   Bind implementations to a trait   Bindings in subcut are defined in a binding module. So by extending a module you create a configuration for your application. For instance you could define a configuration for test, one for QA and another one for production.       // this defines which components are available for this module     // for this example we won't have that much to inject. So lets     // just inject the repositories.     object ProjectConfiguration extends NewBindingModule(module =&gt; {       import module._   // can now use bind directly        // in our example we only need to bind to singletons, all bindings will       // return the same instance.       bind [BidRepo] toSingle new BidRepository       bind [ItemRepo] toSingle new ItemRepository              // with subcut however, we have many binding option as an example we bind the keyrepo and want a new       // instance every time the binding is injected. We'll use the toProvider option for this       bind [KeyRepo] toProvider {new KeyRepository}     }  )   Without diving too deep into subcut. What we do in this code fragment is that we bind an implementation to a trait. We do this for all the resources we want to inject, so subcut knows which implementation to create when it encounters a specific interface. If we want to inject different implementations of a specific trait we can also add an id to the binding, so we can uniquely reference them.   Configure classes that need to have resources injected   Now that we have a set of traits bound to an implementation we can let subcut inject the resources. For this we need to do two things. First we need to add an implicit val to the HelloScalatraServlet class.   class HelloScalatraServlet(implicit val bindingModule: BindingModule) extends ScalatraServlet with Authentication                                                    with RESTRoutes {  .... }   This needs to be added to all classes that want to have there resources injected by subcut. With this implicit value subcut has access to the configuration and can use it to inject dependencies. We’ve defined our routes in the RESTRoutes trait, so lets look at how we configure this trait to work with subcut:   trait RESTRoutes extends ScalatraBase with Injectable {        // simple logger   val logger = Logger(classOf[RESTRoutes]);    // This repository is injected based on type. If no type can be found an exception is thrown   val itemRepo = inject[ItemRepo]   // This repo is injected optionally. If none is provided a standard one will be created   val bidRepo = injectOptional[BidRepo] getOrElse {new BidRepository};    ... }   We added the Injectable trait from subcut so we can use the inject functions (of which there are multiple variants). In this example the itemRepo is injected using the inject function. If no suitable implementation can be found an error message is thrown. And the bidRepo is injected using injectOptional. If nothing was bound, a default is used. Since this trait is used by the servlet we just saw (the one with the implicit bindingmodule) it has access to the binding configuration and subcut will inject the required dependencies.   Bootstrap the 'root' object with our configuration   All we need to do now is tell our root object (the servlet) which configuration it should use, and everything will be wired together. We do this from the generated Scalatra listener, where we add the following:      ...   override def init(context: ServletContext) {          // reference the project configuation, this is implicatly passed into the      // helloScalatraServlet     implicit val bindingModule = ProjectConfiguration      // Mount one or more servlets, this will inject the projectconfiguration     context.mount(new HelloScalatraServlet, \"/*\")   }    ...   Here we create the bindingModule, which is implicitly passed into the constructor of the HelloScalatraServlet. And that’s it, when you now start the application, subcut will determine which dependency needs to be injected. And thats it. If we now start the application subcut will handle the dependencies. If all goes well, and all dependencies are found, the application will start up successfully. If one of the dependencies can’t be found an error will be thrown like this:    15:05:51.112 [main] WARN  o.eclipse.jetty.webapp.WebAppContext - Failed startup of context o.e.j.w.WebAppContext{/,file:/Users/jos/Dev/scalatra/firststeps/hello-scalatra/src/main/webapp/},src/main/webapp org.scala_tools.subcut.inject.BindingException: No binding for key BindingKey(org.smartjava.scalatra.repository.ItemRepo,None) \tat org.scala_tools.subcut.inject.BindingModule$class.inject(BindingModule.scala:66) ~[subcut_2.9.1-2.0-SNAPSHOT.jar:2.0-SNAPSHOT]    On to the next item on the list, Akka.   Add asynchronous processing with Akka   Akka provides you with a complete Actor framework you can use to create scalable, multi-threading applications. Scalatra has support for Akka out of the box, so getting it to work is very easy. Just add the correct trait, wrap functions with the Future function and you’re pretty much done. All the action happens in the RESTRoutes trait where we’ve defined our routes. Lets enable a couple of these methods to use Akka.   trait RESTRoutes extends ScalatraBase with Injectable with AkkaSupport{       ...    /**    * Handles get based on items id. This operation doesn't have a specific    * media-type since we're doing a simple GET without content. This operation    * returns an item of the type application/vnd.smartbid.item+json    */   get(\"/items/:id\") {     // set the result content type \tcontentType = \"application/vnd.smartbid.item+json\"  \t// the future can't access params directly, so extract them first \tval id = params(\"id\").toInt;   \t       Future { \t    // convert response to json and return as OK \t    itemRepo.get(id) match { \t      case Some(x) =&gt; Ok(write(x)); \t      case None =&gt; NotFound(\"Item with id \" + id + \" not found\"); \t    }     }   }      /**    * Delete the specified item    */   delete(\"/items/:id\") {    val id = params(\"id\").toInt;          Future {        itemRepo.delete(id) match {         case Some(x) =&gt; NoContent();         case None =&gt; NotFound(\"Item with id \" + id + \" not found\");       }     }   }   ... }   Not much too see here. We just added the AkkaSupport trait and wrap our method body with the Future function. This will run the code block asynchronously. Scalatra will wait until this block is done, and return the result. One thing to note here, is that you don’t have access to the request context variables provided by scalatra. So if you want to set the response content-type you need to do this outside the future. The same goes for instance for accessing parameters or the request body.  All you need to do now is setup an Akka ActorSystem. The easiest way to do this, is by just using the default actor system. See the Akka documentation for the advanced options.   class HelloScalatraServlet(implicit val bindingModule: BindingModule) extends ScalatraServlet with Authentication                                                    with AkkaSupport                                                    with RESTRoutes {      // create a default actor system. This is used from the futures in the web routes   val system = ActorSystem() }   Now when you run the servlet container you’ll be using Akka futures to handle the requests.   Add CORS and deploy on the cloud   As a final step lets add CORS. with CORS you can open up your API for use from other domains. This avoids the need for JSONP. Using this in scalatra is suprisingly simple. Just add the trait CorsSupport and you’re done. You’ll see something like this, when you start the application:    15:31:28.505 [main] DEBUG o.s.scalatra.HelloScalatraServlet - Enabled CORS Support with: allowedOrigins: \t* allowedMethods: \tGET, POST, PUT, DELETE, HEAD, OPTIONS, PATCH allowedHeaders: \tCookie, Host, X-Forwarded-For, Accept-Charset, If-Modified-Since, Accept-Language, X-Forwarded-Port, Connection, X-Forwarded-Proto, User-Agent, Referer, Accept-Encoding, X-Requested-With, Authorization, Accept, Content-Type    You can fine tune what you support by using a set of init parameters explained here.   Now all that is left is to package everything up, and deploy it to openshift. If you haven’t done so already, register on openshift (it’s free). For my example I use a standard “JBoss Application Server 7.1” application, without any cartridges.      I didn’t want to configure postgresql, so I created a dummy repo implementation:   class DummyBidRepository extends BidRepo{      val dummy = new Bid(Option(10l),10,10,20,\"FL\",10l,12345l, List());      def get(bid: Long, user: String) : Option[Bid] = {     Option(dummy);   }   def create(bid: Bid): Bid = {     dummy;   }   def delete(user:String, bid: Long) : Option[Bid] = {     Option(dummy);   } }   And used subcut to inject this one, instead of the repo that requires a database:         bind [BidRepo] toSingle new DummyBidRepository   With this small change we can use sbt to create war file.    jos@Joss-MacBook-Pro.local:~/Dev/scalatra/firststeps/hello-scalatra$ sbt package &amp;&amp; cp target/scala-2.9.1/hello-scalatra_2.9.1-0.1.0-SNAPSHOT.war ~/dev/git/smartjava/deployments/ [info] Loading project definition from /Users/jos/Dev/scalatra/firststeps/hello-scalatra/project [info] Set current project to hello-scalatra (in build file:/Users/jos/Dev/scalatra/firststeps/hello-scalatra/) [info] Compiling 2 Scala sources to /Users/jos/Dev/scalatra/firststeps/hello-scalatra/target/scala-2.9.1/classes... [info] Packaging /Users/jos/Dev/scalatra/firststeps/hello-scalatra/target/scala-2.9.1/hello-scalatra_2.9.1-0.1.0-SNAPSHOT.war ... [info] Done packaging. [success] Total time: 7 s, completed Oct 5, 2012 1:57:12 PM    And use git to deploy it to openshift    jos@Joss-MacBook-Pro.local:~/git/smartjava/deployments$ git add hello-scalatra_2.9.1-0.1.0-SNAPSHOT.war &amp;&amp; git commit -m 'update' &amp;&amp; git push [master b1c6eae] update  1 files changed, 0 insertions(+), 0 deletions(-) Counting objects: 7, done. Delta compression using up to 8 threads. Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 11.16 KiB, done. Total 4 (delta 3), reused 0 (delta 0) remote: Stopping application... remote: Done remote: ~/git/smartjava.git ~/git/smartjava.git remote: ~/git/smartjava.git remote: Running .openshift/action_hooks/pre_build ... remote: Emptying tmp dir: /var/lib/stickshift/3bc81f5b0d7c48ad84442698c9da3ac4/smartjava/jbossas-7/standalone/tmp/work remote: Running .openshift/action_hooks/deploy remote: Starting application... remote: Done remote: Running .openshift/action_hooks/post_deploy To ssh://3bc81f5b0d7c48ad84442698c9da3ac4@smartjava-scalatra.rhcloud.com/~/git/smartjava.git/    a45121a..b1c6eae  master -&gt; master    You’ll probably see something similar, and now you’re done. Or at least, almost done. Cause what happens when you access a resource:      Hmm.. something went wrong. This is the message that’s interesting to us:    java.lang.IllegalStateException: The servlet or filters that are being used by this request do not support async operation    Hmmm.. apparently JBoss AS handles servlets a bit different from Jetty. The reason we see this message is that by default, according to the servlet 3.0 spec, servlets aren’t enabled to support async operations. Since we use Akka Futures as a result for our routes, we need this async support. Normally you enable this support in a web.xml or using annotations on a servlet. In our case, however, our servlet is started from a listener:     override def init(context: ServletContext) {          // reference the project configuation, this is implicatly passed into the      // helloScalatraServlet     implicit val bindingModule = ProjectConfiguration      // Mount one or more servlets, this will inject the projectconfiguration     context.mount(new HelloScalatraServlet, \"/*\")   }   Context.mount is a convenience method provided by scalatra that registers the servlet. However, this doesn’t enable async support. If we register the servlet ourself we can enable this async support. So replace the previous function with this function:     override def init(context: ServletContext) {          // reference the project configuation, this is implicatly passed into the      // helloScalatraServlet     implicit val bindingModule = ProjectConfiguration       val servlet = new HelloScalatraServlet          val reg = context.addServlet(servlet.getClass.getName,servlet);     reg.addMapping(\"/*\");     reg.setAsyncSupported(true);   }   Now we explicitly enable async support. Create a package again, and use git to deploy the web app to openshift    sbt package  git add hello-scalatra_2.9.1-0.1.0-SNAPSHOT.war &amp;&amp; git commit -m 'update' &amp;&amp; git push    And now you’ve got a working version of your API running on openshift!  ","categories": ["posts","cloud","scala","scalatra"],
        "tags": [],
        "url": "http://www.smartjava.org/content/tutorial-getting-started-scala-and-scalatra-part-iv/",
        "teaser":null},{
        "title": "HTML5: Use the pointer lock API to restrict mouse movement to an element",
        "excerpt":"In this article we’ll look a bit closer at one of the new web APIs that recently has landed in Chrome: the pointer lock API (http://www.w3.org/TR/pointerlock/). With this set API it is possible to lock the mouse on a specific HTML element. With the mouse locked, you can move your mouse all around, and it will never leave the focus of the element. Great for games, 3D representations and probably a lot of other things. The definition from the mozilla site nicely explains this API:   \"Pointer Lock (formerly called mouse lock) provides input methods based on the movement of the mouse over time (i.e., deltas), not just the absolute position of the mouse cursor. It gives you access to raw mouse movement, locks the target of mouse events to a single element, eliminates limits on how far mouse movement can go in a single direction, and removes the cursor from view.\"  A note for those of you using Firefox. Firefox does have an implementation of the Pointer lock API. But when you try out the examples on this page you’ll notice that it won’t work. The reason is that, at the time of writing, the pointer lock API in firefox is tied into the full screen API. So for pointer lock to work on firefox, the element you want to lock the pointer on, has to be in fullscreen mode. Once that requirement is lifted, the example on this page, will also work on firefox.   Demo time   To demonstrate this API I created a simple example:    (click the image to run the demo)   This example shows two canvas elements. The first canvas element, when you move your mouse over it, shows a spaceship on a starry background. Using CSS the cursor is hidden and the spaceship follows your mouse. We simulate movement by using a sprite sheet to shows different ships. What you’ll notice is that when you move your mouse to far, the element loses focus and the ship stops moving and the stars stop updating: the mouse isn’t locked to the element.   When you click on the second element, assuming you use chrome (b.t.w I’m using version 24.0.1284.2 dev), you’ll see the popup shown in the previous screenshot.   Your mouse pointer is now locked to the element and you can move your mouse around all you want, and it’ll stay locked on the canvas updating the spaceship and the starry background. Hit ‘esc’ to release the pointer.   I’ll work you throught the steps you need to take to recreate this demo:      Add event listener for the pointerlockchange event   Register the onclick on the canvas element   Handle the callback when the pointer is locked   Draw the canvas based on mouse location and movement   Handle the callback when the pointer is unlocked   Add event listener for the pointerlockchange event   The first thing we’re going to do is register a callback handler for the pointerlock change event. Since this API is still in it’s early days we have to prefix the calls with ‘moz’ or ‘webkit’. To register the callback we use the following code:           // register the callback when a pointerlock event occurs         document.addEventListener('pointerlockchange', changeCallback, false);         document.addEventListener('mozpointerlockchange', changeCallback, false);         document.addEventListener('webkitpointerlockchange', changeCallback, false);         Whenever we receive an event the changeCallback function is called. Before we look at how to handle this event, lets first register the onclick on the canvas element so that we can request a pointer lock.   Register onclick event for the canvas   I just use jQuery for this, since I’m used to it.           // when element is clicked, we're going to request a         // pointerlock         $(\"#pointerLock\").click(function () {             var canvas = $(\"#pointerLock\").get()[0];             canvas.requestPointerLock = canvas.requestPointerLock ||                     canvas.mozRequestPointerLock ||                     canvas.webkitRequestPointerLock;              // Ask the browser to lock the pointer)             canvas.requestPointerLock();         });   As you can see in this callback we call the requestPointerLock() function, when someone clicks on the canvas.   Handle the callback for the pointerlock   If we click on the canvas, the requestPointerLock() function will be called. This function will fire the ‘pointerlockchange’ event for which we registered the changeCallback function earlier. In this function we do the following:       // called when the pointer lock has changed. Here we check whether the     // pointerlock was initiated on the element we want.     function changeCallback(e) {         var canvas = $(\"#pointerLock\").get()[0];         if (document.pointerLockElement === canvas ||                 document.mozPointerLockElement === canvas ||                 document.webkitPointerLockElement === canvas) {              // we've got a pointerlock for our element, add a mouselistener             document.addEventListener(\"mousemove\", moveCallback, false);         } else {              // pointer lock is no longer active, remove the callback             document.removeEventListener(\"mousemove\", moveCallback, false);              // and reset the entry coordinates             entryCoordinates = {x:-1, y:-1};         }     };   We check whether the pointerLockElement, the element that requested the lock, is the one we expect. If this is the case we add a mousemove listener to the canvas element. If this isn’t the case, it implies that the lock is no longer active on our canvas. In that case we remove the mouse listener. In this last case, we also reset a global variable that defines the current coordinates of the spaceship in the canvas (more on that later).   So when we click on the canvas, a pointer lock event is fired, this event is handled by the callback we specified. In the callback we register (or deregister) a mousemove listener, and can start drawing the spaceship and the background whenever we receive a mouse event.   Draw the canvas based on mouse location and movement   In the moveCallback() function we do the following:       // handles an event on the canvas for the pointerlock example     var entryCoordinates = {x:-1, y:-1};     function moveCallback(e) {          var canvas = $(\"#pointerLock\").get()[0];         var ctx = canvas.getContext('2d');          // if we enter this for the first time, get the initial position         if (entryCoordinates.x == -1) {             entryCoordinates = getPosition(canvas, e);         }           //get a reference to the canvas         var movementX = e.movementX ||                 e.mozMovementX ||                 e.webkitMovementX ||                 0;          var movementY = e.movementY ||                 e.mozMovementY ||                 e.webkitMovementY ||                 0;           // calculate the new coordinates where we should draw the ship         entryCoordinates.x = entryCoordinates.x + movementX;         entryCoordinates.y = entryCoordinates.y + movementY;          if (entryCoordinates.x &gt; $('#pointerLock').width() -65) {             entryCoordinates.x = $('#pointerLock').width()-65;         } else if (entryCoordinates.x &lt; 0) {             entryCoordinates.x = 0;         }          if (entryCoordinates.y &gt; $('#pointerLock').height() - 85) {             entryCoordinates.y = $('#pointerLock').height() - 85;         } else if (entryCoordinates.y &lt; 0) {             entryCoordinates.y = 0;         }           // determine the direction         var direction = 0;         if (movementX &gt; 0) {             direction = 1;         } else if (movementX &lt; 0) {             direction = -1;         }          // clear and render the spaceship         ctx.clearRect(0,0,400,400);         generateStars(ctx);         showShip(entryCoordinates.x, entryCoordinates.y, direction,ctx);     }    This might seem like a lot of javascript. But it shouldn’t be that hard to follow.   If we enter for the first time (entryCoordinates.x == -1) we get the inital position we want to draw on. We use the getPosition() function for this:       // Returns a position based on a mouseevent on a canvas. Based on code     // from here: http://miloq.blogspot.nl/2011/05/coordinates-mouse-click-canvas.html     function getPosition(canvas, event) {         var x = new Number();         var y = new Number();          if (event.x != undefined &amp;&amp; event.y != undefined) {             x = event.x;             y = event.y;         }         else // Firefox method to get the position         {             x = event.clientX + document.body.scrollLeft +                     document.documentElement.scrollLeft;             y = event.clientY + document.body.scrollTop +                     document.documentElement.scrollTop;         }          x -= canvas.offsetLeft;         y -= canvas.offsetTop;          return {x:x, y:y};     }    Once we have the position, we can use the movementX and movementY properties to determine the relative mouse movement.           //get a reference to the canvas         var movementX = e.movementX ||                 e.mozMovementX ||                 e.webkitMovementX ||                 0;          var movementY = e.movementY ||                 e.mozMovementY ||                 e.webkitMovementY ||                 0;     }   And based on these properties we determine the position where we need to draw the spaceship:           // calculate the new coordinates where we should draw the ship         entryCoordinates.x = entryCoordinates.x + movementX;         entryCoordinates.y = entryCoordinates.y + movementY;          if (entryCoordinates.x &gt; $('#pointerLock').width() -65) {             entryCoordinates.x = $('#pointerLock').width()-65;         } else if (entryCoordinates.x &lt; 0) {             entryCoordinates.x = 0;         }          if (entryCoordinates.y &gt; $('#pointerLock').height() - 85) {             entryCoordinates.y = $('#pointerLock').height() - 85;         } else if (entryCoordinates.y &lt; 0) {             entryCoordinates.y = 0;         }   The final step we need to take before we can draw the ship, is to determine the direction the ship is moving in. We’ve got the following spritesheet:      So we can move to the right (direction = 1), move to the left (direction = -1) or have no movement (direction = 0). Based on the relative movementX we determine this value.           // determine the direction         var direction = 0;         if (movementX &gt; 0) {             direction = 1;         } else if (movementX &lt; 0) {             direction = -1;         }   Now all that is left is drawing the canvas. We first clear the context, and then use the following to generate a random starry background:       // generate a set of random stars for the canvas.     function generateStars(ctx) {         for (var i = 0; i &lt; 50; i++) {              x = Math.random() * 400;             y = Math.random() * 400;             radius = Math.random() * 3;              ctx.fillStyle = \"#FFF\";             ctx.beginPath();             ctx.arc(this.x, this.y, this.radius, 0, Math.PI * 2, false);             ctx.closePath();             ctx.fill();         }     }   And draw the ship using this function:       // Render a ship at the specified position. The direction determines how to render the ship     // based on a sprite sheet. The sprite was taken frome here:     // http://atomicrobotdesign.com/blog/web-development/how-to-use-sprite-sheets-with-html5-canvas/     sprites = new Image();     sprites.src = 'ships2.png';     function showShip(ship_x, ship_y, direction, ctx) {          //    srcX = 83;         if (direction == -1) {             srcX = 156;         } else if (direction == 1) {             srcX = 83;         } else if (direction == 0) {             srcX = 10;         }           // 10 is normal  156 is left         srcY = 0;         ship_w = 65;         ship_h = 85;         ctx.drawImage(sprites, srcX, srcY, ship_w, ship_h, ship_x, ship_y, ship_w, ship_h);     }   The only thing left to do is handle the unregistration of the pointer lock. When you click ‘esc’ the pointer lock is released and the callback we saw earlier is called:       function changeCallback(e) {         var canvas = $(\"#pointerLock\").get()[0];         if (document.pointerLockElement === canvas ||                 document.mozPointerLockElement === canvas ||                 document.webkitPointerLockElement === canvas) {              // we've got a pointerlock for our element, add a mouselistener             document.addEventListener(\"mousemove\", moveCallback, false);         } else {              // pointer lock is no longer active, remove the callback             document.removeEventListener(\"mousemove\", moveCallback, false);              // and reset the entry coordinates             entryCoordinates = {x:-1, y:-1};         }     };   This time the pointerLockElement won’t point to the canvas, which tells us we can remove the mouselistener. And with all this we’ve got a canvas element that uses pointer lock.   The unlocked variant   In the example you can also see a canvas that doesn’t use pointer lock. The javascript for that is shown here:       // This function sets up the nopointerlock example. This function creates     // a simple mousemove listener for the canvas element, and used that information     // to determine where to draw the ship.     function setupNoPointerLock() {          // when we have a canvas without a mouse lock, we need to create a mouse listener         var canvas2 = $(\"#noPointerLock\").get()[0];         var context2 = canvas2.getContext('2d');          var entryCoordinates2 = {x:-1,y:-1};          canvas2.addEventListener('mousemove', function(evt) {              if (entryCoordinates2.x == -1) {                 entryCoordinates2 = getPosition(canvas2,evt);             }              var newPos = getPosition(canvas2, evt);             movementX = newPos.x - entryCoordinates2.x;              // calculate the direction             var direction = 0;             if (movementX &gt; 0) {                 direction = 1;             } else if (movementX &lt; 0) {                 direction = -1;             }               // clear and render the spaceship             context2.clearRect(0,0,400,400);             generateStars(context2);             showShip(newPos.x-32, newPos.y-42, direction, context2);             entryCoordinates2 = newPos;          });     }   If you want to look at the sources for this example just go to the example here and use view-source.  ","categories": ["posts","pointer lock","javascript","html5"],
        "tags": [],
        "url": "http://www.smartjava.org/content/html5-use-pointer-lock-api-restrict-mouse-movement-element/",
        "teaser":null},{
        "title": "Exploring the HTML5 Web Audio: visualizing sound",
        "excerpt":"If you’ve read some of my other articles on this blog you probably know I’m a fan of HTML5. With HTML5 we get all this interesting functionality, directly in the browser, in a way that, eventually, is standard across browsers. One of the new HTML5 APIs that is slowly moving through the standardization process is the Web Audio API. With this API, currently only supported in Chrome and as of October 2013 also in Firefox, we get access to all kinds of interesting audio components you can use to create, modify and visualize sounds (such as the following spectrogram).   Update 30-11-2013: Firefox also has support for this API but a couple of API elements have changed. I’ve updated this article to reflect those changes and the examples now work on Firefox and Chrome. Additionally you can download the sources from this article from github: https://github.com/josdirksen/smartjava/tree/master/webaudio      So why do I start with visualizations? It looks nice, that’s one reason, but not the important one. This API provides a number of more complex components, whose behavior is much easier to explain when you can see what happens. With a filter you can instantly see whether some frequencies are filtered, instead of trying to listen to the resulting audio for thse changes.   There are many interesting examples that use this API. The problem is, though, that getting started with this API and with digital signal processing (DSP) usually isn’t explained. In this article I’ll walk you through a couple of steps that shows how to do the following:      Create a signal volume meter   Visualize the frequencies using a spectrum analyzer   And show a time based spectrogram   We start with the basic setup that we can use as the basis for the components we’ll create.   Setting up the basic   If we want to experiment with sound, we need some sound source. We could use the microphone (as we’ll do later in this series), but to keep it simple, for now we’ll just use an mp3 as our input. To get this working using web audio we have to take the following steps:      Load the data   Read it in a buffer node and play the sound   Load the data   With the web audio we can use different types of audio sources. We’ve got a MediaElementAudioSourceNode that can be used to use the audio provided by a media element. There’s also a MediaStreamAudioSourceNode. With this audio source node we can use the microphone as input (see my previous article on sound recognition). Finally there is the AudioBufferSourceNode. With this node we can load the data from an existing audio file (e.g mp3) and use that as input. For this example we’ll use this last approach.      // check if the default naming is enabled, if not use the chrome one.     if (! window.AudioContext) {         if (! window.webkitAudioContext) {             alert('no audiocontext found');         }         window.AudioContext = window.webkitAudioContext;     }     var context = new AudioContext();     var audioBuffer;     var sourceNode;      // load the sound     setupAudioNodes();     loadSound(\"wagner-short.ogg\");      function setupAudioNodes() {         // create a buffer source node         sourceNode = context.createBufferSource();         // and connect to destination         sourceNode.connect(context.destination);     }      // load the specified sound     function loadSound(url) {         var request = new XMLHttpRequest();         request.open('GET', url, true);         request.responseType = 'arraybuffer';          // When loaded decode the data         request.onload = function() {              // decode the data             context.decodeAudioData(request.response, function(buffer) {                 // when the audio is decoded play the sound                 playSound(buffer);             }, onError);         }         request.send();     }       function playSound(buffer) {         sourceNode.buffer = buffer;         sourceNode.start(0);     }      // log if an error occurs     function onError(e) {         console.log(e);     }    In this example you can see a couple of functions. The setupAudioNodes function creates a BufferSource audio node and connects it to the destination. The loadSound function shows how you can load an audio file. The buffer which is passed into the playSound function contains decoded audio that can be used by the web audio API.   In this example I use an .ogg file, for a complete overview of the formats supported look at: https://sites.google.com/a/chromium.org/dev/audio-video   Play the sound   To play this audio file, all we have to do is turn the source node on, this is done in the playSound function:       function playSound(buffer) {         sourceNode.buffer = buffer;         sourceNode.noteOn(0);     }   You can test this out at the following page:      Example 1: Loading and playing a sound with Web Audio API.   When you open that page, you’ll hear some music. Nothing to spectacular for now, but nevertheless an easy way to load audio that’ll use for the rest of this article.  The first item on our list was the volume meter.   Create a volume meter   One of the basic scenario’s, and often one of the first steps someone new to this API tries to create, is a simple signal volume meter (or an UV meter). I expected this to be a standard component in this API, where I could just read off the signal strength as a property. But, no such node exists. But not to worry, with the components that are available, it’s pretty easy (not straightforward, but easy nevertheless) to get an indication of the signal strength of your audio file. Int this section we’ll create the following simple volume meter:      As you can see this is a simple volume meter where we measure the signal strength for the left and the right audio channel. This is drawn on the canvas, but you could have also used divs or svg to visualize this. Lets start with a single volume meter, instead of one for each channel. For this we need to do the following:      Create an analyzer node: With this node we get realtime information about the data that is processed. This data we use to determine the signal strength   Create a javascript node: We use this node as a timer to update the volume meters with new information   Connect everything together   Analyser node   With the analyser node we can perform real-time frequency and time domain analysis. From the specification:   a node which is able to provide real-time frequency and time-domain analysis information. The audio stream will be passed un-processed from input to output.   I won’t go into the mathematical details behind this node, since there are many articles out there that explain how this works (a good one is the chapter on fourier transformation from here). What you should now about this node is that it splits up the signal in frequency buckets and we get the amplitude (the signal strenght) for each set of frequencies (the bucket). The best way to understand this, is to skip a bit ahead in this article and look at the frequency distribution we’ll create later on.      This image plots the result from the analyser node. The frequencies increase from left to right, and the height of the bar shows the strength of that specific frequency bucket. More on this later on in the article. For now we don’t want to see the strength of the separate frequency buckets, but the strength of the total signal. For this we’ll just add all the strenghts from each bucket and divide it by the number of buckets.   First we need to create an analyzer node          // setup a analyzer         analyser = context.createAnalyser();         analyser.smoothingTimeConstant = 0.3;         analyser.fftSize = 1024;   This creates an analyzer node whose result will be used to create the volume meter. We use a smoothingTimeConstant to make the meter less jittery. With this variable we use input from a longer time period to calculate the amplitudes, this results in a more smooth meter. The fftSize determine how many buckets we get containing frequency information. If we have a fftSize of 1024 we get 512 buckets (more info on this in the book on DPS and fourier transformations).   When this node receives a stream of data, it analyzes this stream and provides us with information about the frequencies in that signal and their strengths. We now need a timer to update the meter at regular intervals. We could use the standard javascript setInterval function, but since we’re looking at the Web Audio API lets use one of its nodes. The ScriptProcessorNode.   The ScriptProcessor node   With the ScriptProcessorNode we can process the raw audio data directly from javascript. We can use this to write our own analyzers or complex components. We’re not going to do that, though. When creating the javascript node, you can specify the interval at which it is called. We’ll use that feature to update the meter at regulat intervals.   Creating a ScriptProcessor node is very easy.          // setup a javascript node         javascriptNode = context.createScriptProcessor(2048, 1, 1);   This will create a ScriptProcessor that is called whenever the 2048 frames have been sampled. Since our data is sampled at 44.1k, this function will be called approximately 21 times a second. Now what happens when this function is called:      // when the javascript node is called     // we use information from the analyzer node     // to draw the volume     javascriptNode.onaudioprocess = function() {          // get the average, bincount is fftsize / 2         var array =  new Uint8Array(analyser.frequencyBinCount);         analyser.getByteFrequencyData(array);         var average = getAverageVolume(array)          // clear the current state         ctx.clearRect(0, 0, 60, 130);          // set the fill style         ctx.fillStyle=gradient;          // create the meters         ctx.fillRect(0,130-average,25,130);     }      function getAverageVolume(array) {         var values = 0;         var average;          var length = array.length;          // get all the frequency amplitudes         for (var i = 0; i &lt; length; i++) {             values += array[i];         }          average = values / length;         return average;     }   In these two functions we calculate the average and draw the meter directly on the canvas (using a gradient so we have nice colors). Now all we have to do is connect the output from the audiosource to the analyser, the analyser to the ScriptProcessor node (and if we want audio to hear, we also need to connect something to the destination).   Connect everything together   Connecting everything together is easy:    function setupAudioNodes() {          // setup a javascript node         javascriptNode = context.createScriptProcessor(2048, 1, 1);         // connect to destination, else it isn't called         javascriptNode.connect(context.destination);          // setup a analyzer         analyser = context.createAnalyser();         analyser.smoothingTimeConstant = 0.3;         analyser.fftSize = 1024;          // create a buffer source node         sourceNode = context.createBufferSource();          // connect the source to the analyser         sourceNode.connect(analyser);          // we use the javascript node to draw at a specific interval.         analyser.connect(javascriptNode);          // and connect to destination, if you want audio        sourceNode.connect(context.destination);     }   And that’s it. This will draw a single volume meter, for the complete signal. Now what do we do when we want to have a volume meter for each channel. For this we use a ChannelSplitter. Let’s dive right into the code to connect everything:       function setupAudioNodes() {          // setup a javascript node         javascriptNode = context.createScriptProcessor(2048, 1, 1);         // connect to destination, else it isn't called         javascriptNode.connect(context.destination);          // setup a analyzer         analyser = context.createAnalyser();         analyser.smoothingTimeConstant = 0.3;         analyser.fftSize = 1024;          analyser2 = context.createAnalyser();         analyser2.smoothingTimeConstant = 0.0;         analyser2.fftSize = 1024;          // create a buffer source node         sourceNode = context.createBufferSource();         splitter = context.createChannelSplitter();          // connect the source to the analyser and the splitter         sourceNode.connect(splitter);          // connect one of the outputs from the splitter to         // the analyser         splitter.connect(analyser,0,0);         splitter.connect(analyser2,1,0);          // we use the javascript node to draw at a         // specific interval.         analyser.connect(javascriptNode);          // and connect to destination         sourceNode.connect(context.destination);     }   As you can see we don’t really change much. We introduce a new node, the splitter node. This node splits the sound into a left and a right channel. These channels can be processed separately. With this layout the following happens:      The audiosource creates a signal based on the buffered audio.   This signal is sent to the splitter, who splits the signal into a left and right stream.   Each of these two streams is processed by their own realtime analyser.   From the javascript node, we now get the information from both analysers and plot both meters   I’ve shown step 1 through 3, let’s quickly move on the step 4. For this we simply add the following to the onaudioprocess node:        javascriptNode.onaudioprocess = function() {          // get the average for the first channel         var array =  new Uint8Array(analyser.frequencyBinCount);         analyser.getByteFrequencyData(array);         var average = getAverageVolume(array);          // get the average for the second channel         var array2 =  new Uint8Array(analyser2.frequencyBinCount);         analyser2.getByteFrequencyData(array2);         var average2 = getAverageVolume(array2);          // clear the current state         ctx.clearRect(0, 0, 60, 130);          // set the fill style         ctx.fillStyle=gradient;          // create the meters         ctx.fillRect(0,130-average,25,130);         ctx.fillRect(30,130-average2,25,130);     }   And now we’ve got two signal meters, one for each channel.      Example 2: Visualize the signal strength with a volume meter.   Or view the result on youtube:     Now lets see how we can get the view of the frequencies I showed earlier.   Create a frequency spectrum   With all the work we already did in the previous section, creating a frequency spectrum overview is now very easy. We’re going to aim for this:      We setup the nodes just like we did in the first example:      function setupAudioNodes() {          // setup a javascript node         javascriptNode = context.createScriptProcessor(2048, 1, 1);         // connect to destination, else it isn't called         javascriptNode.connect(context.destination);          // setup a analyzer         analyser = context.createAnalyser();         analyser.smoothingTimeConstant = 0.3;         analyser.fftSize = 512;          // create a buffer source node         sourceNode = context.createBufferSource();         sourceNode.connect(analyser);         analyser.connect(javascriptNode);  //        sourceNode.connect(context.destination);     }   So this time we don’t split the channels and we set the fftSize to 512. This means we get 256 bars that represent our frequency. We now just need to alter the onaudioprocess method and the gradient we use:       var gradient = ctx.createLinearGradient(0,0,0,300);     gradient.addColorStop(1,'#000000');     gradient.addColorStop(0.75,'#ff0000');     gradient.addColorStop(0.25,'#ffff00');     gradient.addColorStop(0,'#ffffff');      // when the javascript node is called     // we use information from the analyzer node     // to draw the volume     javascriptNode.onaudioprocess = function() {          // get the average for the first channel         var array =  new Uint8Array(analyser.frequencyBinCount);         analyser.getByteFrequencyData(array);          // clear the current state         ctx.clearRect(0, 0, 1000, 325);          // set the fill style         ctx.fillStyle=gradient;         drawSpectrum(array);      }     function drawSpectrum(array) {     for ( var i = 0; i &lt; (array.length); i++ ){             var value = array[i];             ctx.fillRect(i*5,325-value,3,325);         }     };   In the drawSpectrum function we iterate over the array, and draw a vertical bar based on the value. That’s it. For a live example, click on the following link:      Example 3: Visualize the frequency spectrum.   Or view it on youtube:     And then the final one. The spectrogram.   Time based spectrogram   When you run the previous demo you see the strength of the various frequency buckets in real time. While this is a nice visualization, it doesn’t allow you to analyze information over a period of time. If you want to do that you can create a spectrogram.      With a spectrogram we plot a single line for each measurement. The y-axis represents the frequency, the x-asis the time and the color of a pixel the strength of that frequency. It can be used to analyze the received audio, and also creates nice looking images.   The good thing, is that to output this data we don’t have to change much from what we’ve already got in place. The only function that’ll change is the onaudioprocess node and we’ll create a slightly different analyser.           analyser = context.createAnalyser();         analyser.smoothingTimeConstant = 0;         analyser.fftSize = 1024;   The enalyser we create here has an fftSize of 1024, this means we get 512 frequency buckets with strengths. So we can draw a spectrogram that has a height of 512 pixels. Also note that the smoothingTimeConstant is set to 0. This means we don’t use any of the previous results in the analysis. We want to show the real information, not provide a smooth volume meter or frequency spectrum analysis.   The easiest way to draw a spectrogram is by just start drawing the line at the left, and for each new set of frequencies increase the x-coordinate by one. The problem is that this will quickly fill up our canvas, and we’ll only be able to see the first half a minute of the audio. To fix this, we need some creative canvas copying. The complete code for drawing the spectrogram is shown here:       // create a temp canvas we use for copying and scrolling     var tempCanvas = document.createElement(\"canvas\"),         tempCtx = tempCanvas.getContext(\"2d\");     tempCanvas.width=800;     tempCanvas.height=512;      // used for color distribution     var hot = new chroma.ColorScale({         colors:['#000000', '#ff0000', '#ffff00', '#ffffff'],         positions:[0, .25, .75, 1],         mode:'rgb',         limits:[0, 300]     });     ...      // when the javascript node is called     // we use information from the analyzer node     // to draw the volume     javascriptNode.onaudioprocess = function () {          // get the average for the first channel         var array = new Uint8Array(analyser.frequencyBinCount);         analyser.getByteFrequencyData(array);          // draw the spectrogram         if (sourceNode.playbackState == sourceNode.PLAYING_STATE) {             drawSpectrogram(array);         }     }      function drawSpectrogram(array) {          // copy the current canvas onto the temp canvas         var canvas = document.getElementById(\"canvas\");          tempCtx.drawImage(canvas, 0, 0, 800, 512);          // iterate over the elements from the array         for (var i = 0; i &lt; array.length; i++) {             // draw each pixel with the specific color             var value = array[i];             ctx.fillStyle = hot.getColor(value).hex();              // draw the line at the right side of the canvas             ctx.fillRect(800 - 1, 512 - i, 1, 1);         }          // set translate on the canvas         ctx.translate(-1, 0);         // draw the copied image         ctx.drawImage(tempCanvas, 0, 0, 800, 512, 0, 0, 800, 512);          // reset the transformation matrix         ctx.setTransform(1, 0, 0, 1, 0, 0);     }   To draw the spectrogram we do the following:      We copy what is currently drawn to a hidden canvas   Next we draw a line of the current values at the far right of the canvas   We set the translate on the canvas to -1   We copy the copied information back to the original canvas (that is now drawn 1 pixel to the left)   And reset the transformation matrix   See a running example here:     Example 4: Create a spectrogram   Or view it here:    Last thing I’d like to mention regarding the code is the chroma.js library I used for the colors. If you ever need to draw something color or gradient related (e.g maps, strengths, levels) you can easily create color scales with this library.   Two final pointers, I know I’ll get questions about:      Volume could be represented as a magnitude, just didn't want to complicate matters for this.   The spectogram doesn't use logarithmic scales. Once again, didn't want to complicate things  ","categories": ["posts","visualization","webaudio","html5"],
        "tags": [],
        "url": "http://www.smartjava.org/content/exploring-html5-web-audio-api-filters/",
        "teaser":null},{
        "title": "Copy and paste images into your browser using W3C Clipboard API",
        "excerpt":"Just a quick article about the Clipboard API  and how you can use this to quickly add copy and paste support for your applications. The clipboard API is a bit of an old one and has been supported by most browsers in one way or the other. With this API it has been possible to register event listeners for copy, cut and paste events and access the system’s clipboard (in a minimal, very restrictive way).   Usually when using this API you are only allowed to paste directly into contentEditable fields. Chrome, however, also supports the copy event on other DOM elements. With this support we can very easily add copy and paste support to our web applications. In this short article I’ll show you how you can use this functionality to copy and paste images to a jQuery slider. If you have chrome you can test this yourself by clicking on the following image (or just view the video);       For those of you that don’t have Chrome, a quick video of what we’re going to create here (best viewed in fullscreen):     This is actually very easy to do, all we have to do, is register a “paste” event, and when the event occurs, we can access the clipboard. We walk through the elements in the clipboard and when we find an image we add it to the slider (slider is based on http://basic-slider.com/ ,which offers a very simple to use and integrate slider).   The code for this is shown here:              $(document).ready(function() {                 window.addEventListener(\"paste\",processEvent);                  function processEvent(e) {                     for (var i = 0 ; i &lt; e.clipboardData.items.length ; i++) {                          // get the clipboard item                         var clipboardItem = e.clipboardData.items[i];                         var type = clipboardItem.type;                          // if it's an image add it to the image field                         if (type.indexOf(\"image\") != -1) {                              // get the image content and create an img dom element                             var blob = clipboardItem.getAsFile();                             var blobUrl = window.webkitURL.createObjectURL(blob);                             var img = $(\"&lt;img/&gt;\");                             img.attr(\"src\",blobUrl);                              // our slider requires an li item.                             var li = $(\"&lt;li&gt;&lt;/li&gt;\");                              // add the correct class and add the image                             li.addClass(\"bjqs-slide\");                             li.append(img);                              // add this image to the list of images                             $(\".bjqs\").append(li);                              // reset the basic-slider added elements                             $(\".bjqs-controls\").remove();                             $(\".bjqs-markers\").remove();                              // reset the image slider                             $('#banner-fade').bjqs({                                 height      : 320,                                 width       : 620,                                 responsive  : true                             });                         } else {                             console.log(\"Not supported: \" + type);                         }                      }                 }             });   In this listing we do a couple of things. First we register an event listener for the “paste” event. So whenever you paste something in this window the provided callback (processEvent in this case) is called.   In this callback, we retrieve all the items from the clipboard and check the type of each clipboard item. If the content-type of this element starts with “image” (e.g image/png, image/gif or image/jpeg) we get the content of that item as a blob, and create a new img dom element. Now all we need to do is some “basic-slider” specifics. We add the img element wrapped in a li element and reset the current slider. Now you can add images by copy and pasting. For those using a non-chrome browser, there is a workaround. You can add a hidden input element with auto focus and register the paste event on that element. More information can be found on this site.  ","categories": ["posts","javascript","webapi","html5"],
        "tags": [],
        "url": "http://www.smartjava.org/content/copy-and-paste-images-your-browser-using-w3c-clipboard-api/",
        "teaser":null},{
        "title": "Use COLOURlovers top patterns as random background for you web application",
        "excerpt":"A couple of days ago I stumbled on the COLOURlovers site from the Android Play Store. This site provides a whole lot of beautiful patterns, colors and palettes and has tools to create them. I downloaded an app that randomly changes the background for my tablet, and really like the result. They really have a very beautiful set of patterns to use as a background:      When browsing their site I noticed they had an API. From this API you can search through all the content on their site, and retrieve links to the patterns:   For instance a GET on http://www.colourlovers.com/api/patterns/top?format=json gives the following result:    ... { \"id\":50713, \"title\":\"pat\", \"userName\":\"florc\", \"numViews\":40404, \"numVotes\":1557, \"numComments\":42, \"numHearts\":0, \"rank\":4, \"dateCreated\":\"2008-03-03 13:24:06\", \"colors\":[ \"FFFFFF\", \"F2EFEB\", \"FAF5ED\", \"EDE9E4\", \"F0ECE9\" ], \"description\":\"\", \"url\":\"http:\\/\\/www.colourlovers.com\\/pattern\\/50713\\/pat\", \"imageUrl\":\"http:\\/\\/colourlovers.com.s3.amazonaws.com\\/images\\/patterns\\/50\\/50713.png\", \"badgeUrl\":\"http:\\/\\/www.colourlovers.com\\/images\\/badges\\/n\\/50\\/50713_pat.png\", \"apiUrl\":\"http:\\/\\/www.colourlovers.com\\/api\\/pattern\\/50713\" } ...    As a simple experiment I decided to create a web page that uses this API to set a random background based on the top patterns from COLOURlovers. The first problem though was that this API doesn’t support CORS, so I couldn’t directly access the API. Luckily though, this API supports JSONP, so with JSONP I can call this API directly from the web app. Now what do you need to do to get all this working? Not that much, first thing to do, is get the url of a random pattern and set it as the background:       // We retrieve a set of the 50 top backgrounds     $.getJSON('http://www.colourlovers.com/api/patterns/top?jsonCallback=?&amp;numResults=50', function(data) {          // we randomly select one of the elements         var element = data[Math.floor(Math.random()*50)];          // we update the body background with the url         $('body').css({             \"background-image\": \"url(\"+ element.imageUrl +\")\"         });     });   This however, doesn’t make the pattern repeat itself. To do this we need to add the following css directives:            body {             background-repeat: repeat;         }    And that’s it, you’ve now got a random repeating background. I created a simple example which you can see here:     ","categories": ["posts","css","jquery"],
        "tags": [],
        "url": "http://www.smartjava.org/content/use-colourlovers-top-patterns-random-background-you-web-application/",
        "teaser":null},{
        "title": "HAL 9000 Soundboard with HTML5, Canvas and HTML5 Audio",
        "excerpt":"In the last month I’ve been experimenting with the Web Audio API. Currently I’m looking at how you can use this API to create robot sounds and was looking for a nice way to represent this. So as a quick side project I decided to create an animated HAL 9000 using HTML5 Canvas. In this article I’ll quickly walk you through the steps how I did this. If you click on the image below you can find a simple soundboard I created that animates HAL’s glowing light each time you select a sound bite. The sound bites are played back using the HTML5 Audio element, so should work across most modern browsers. You can see the example right away by following this link, or clicking the image:        Nothing to special happens in this demo, but there are a couple of interesting things here. In this article we’ll look at the following subjects:      Recreating HAL9000 animation using Canvas   Rotating and printing text   Playing sound using the HTML5 Audio element   Lets start with the Canvas based HAL9000 implementation.   Recreating HAL9000 animation using Canvas   The HAL9000 animation was created using a couple of static images and a radial context. First lets look at the static images. Based on a standard background that showed HAL, I used photoshop to extract the static parts. Basically this consisted of the HAL9000 border:      And a transparent image showing the ring:      These two elements are added to the canvas by using the context.drawImage function. So what we do is the following:      First we draw the background   Next we draw the 'eye' as a radialGradient   Finally we overlay the ring   In code this looks something like this:    var canvas = document.getElementById('myCanvas');     var context = canvas.getContext('2d');     var img = new Image();      var width = 220;     var height = 220;      var bck = new Image();     bck.src= \"background.png\";     bck.onload = function() {          img.onload = function(){             // create radial gradient             var grd = context.createRadialGradient(15+width/2, 300+height/2, 10, 15+width/2, 300+height/2, height/2);             grd.addColorStop(0, '#faf4c3');             grd.addColorStop(0.025, '#fecf00');             grd.addColorStop(0.1, '#d00600');             grd.addColorStop(0.3, '#e30900');             grd.addColorStop(0.5, '#3c0300');             grd.addColorStop(1, '#0e0e0e');              context.drawImage(bck,0,0);             context.fillStyle = grd;             context.fillRect(10,300,width,height);             context.drawImage(img,15,300);         };         img.src = 'hal-ring-2.png';     };   As you can see we first draw the background, next we draw the gradient, and finally we overlay the ring. Now, by playing with the “colorstops” we can animate the eye.           $(change).animate({             pos1: 0,             pos2: 0.025,             pos3: 0.1,             pos4: 0.4,             pos5: 0.87         },{ duration: duration/2, step:function(now, fx) {              context.clearRect ( 0 , 0 , 578 , 600 );             context.drawImage(bck,0,0);             grd = context.createRadialGradient(15+width/2, 300+height/2, 10, 15+width/2, 300+height/2, height/2);             grd.addColorStop(change.pos1, '#faf4c3');             grd.addColorStop(change.pos2, '#fecf00');             grd.addColorStop(change.pos3, '#d00600');             grd.addColorStop(change.pos4, '#e30900');             grd.addColorStop(change.pos5, '#3c0300');             grd.addColorStop(1, '#0e0e0e');              context.fillStyle = grd;             context.fillRect(10,300,width,height);             context.drawImage(img,15,300);          }})   Rotating and printing text   I wanted to create something like a frequency analyzer for the quotes. You can see this in the image as the vertical bars to the left and to the right of HAL. I started out by rotating divs, but because CSS3 rotations are applied after layout, I had trouble getting the correct layout. So I did the same thing I always do when I can’t get the desired effect using divs, and turn to svg. More specifically to d3.js.   With d3.js it’s very easy to create SVG elements and manipulate them. To get the desired effect I had to take the following steps:      First, render each elemetn as an svg:text with a specific font   Use the boundingbox to determine how much we need to move it to align to the bottom   Rotate and translate to correct position   Use the boundingbox to draw an exact rectangle around the text   In code it looks like this, where this function takes the id of the div to add the text elements to. Texts contains a list of elements ({txt: “\"I feel much better now, I really do.\"“,file:”better.wav”}) that can be used to set the label and indicate the audio file to play. xOffset is used to position the text nicely in the divs and the scale is a colorscale used to determine the background color of the rectangles.      function addTexts(addTo, texts, xOffset, scale) {          // create the svg element         var svg = d3.select(addTo).append(\"svg:svg\")                 .attr(\"width\", svgWidth)                 .attr(\"id\",addTo+\"svg\")                 .attr(\"height\", svgHeight);          // for this element create text elements to hold the quotes         svg.selectAll(\"text\")                  .data(texts)                  .enter().append(\"svg:text\")                  .attr(\"x\", 0)                  .attr(\"y\", 0)                  .attr(\"dy\", \".35em\")                  .attr(\"fill\",\"white\")                  .attr(\"text-anchor\", \"middle\")                  .style(\"font\", \"100 16px Helvetica Neue\")                  .text(function(d) {return d.txt});           // Now we can update all the texts based on their rendered size         var bboxs = [];         svg.selectAll(\"text\")                 .attr(\"transform\", function(d,i) {                     bboxs[i]=this.getBBox();                     return \"rotate(-90) translate(\" + -(svgHeight+this.getBBox().x) + \",\" + (xOffset + (i*20)) +\")\"}         );          svg.selectAll(\"rect\")                 .data(texts)                 .enter().append(\"svg:rect\")                     .attr(\"x\", 0)                     .attr(\"y\", 0)                     .attr(\"width\", function(d,i) {return bboxs[i].width})                     .attr(\"height\", function(d,i) {return bboxs[i].height})                     .attr(\"transform\",function(d,i) {return \"rotate(-90) translate(\" + -(svgHeight+bboxs[i].height-21) + \",\" + (xOffset-10 + (i*20)) +\")\" })                     .on(\"click\", function(d,i) { playSound(d.file) })                     .style(\"fill\", function(d,i) {return scale.getColor(i/texts.length).hex()})                     .style(\"fill-opacity\", \".5\")   As you can see we also added an addClick event on the rectangle in this piece of code. When this is clicked the playSound function is called with the name of the audio file as it’s parameter.   Playing sound using the HTML5 Audio element   What happens when the rectangle is clicked is shown here:       var snd = new Audio();     snd.addEventListener(\"play\",onPlay);     snd.addEventListener(\"durationchange\",onDurationChange);      function playSound(file) {         snd.setAttribute(\"src\",\"sounds/\" + file);         snd.load();         snd.play();     }      function onDurationChange(e) {         console.log(snd.duration);         if (snd.duration != undefined) {               animate(snd.duration*1000);         }     }   The sound is loaded, and when the sound is loaded the animation starts. And we start the animation as soon as we know how long the sound will play. We do this so the animation plays as long as the sound is heard.   And that’s it. Nothing to complex, but it nicely shows how easy it is to create nice effects using Canvas, HTML5 Audio and SVG.  ","categories": ["posts","audio","html5","canvas"],
        "tags": [],
        "url": "http://www.smartjava.org/content/hal-9000-soundboard-html5-canvas-and-html5-audio/",
        "teaser":null},{
        "title": "Tutorial: Node.js and Express.js - Part I - Getting started",
        "excerpt":"A couple of months ago a wrote a series of tutorials on scalatra. In this article, and a couple of follow ups, I’ll do the same for Node.js together with Express.js. I don’t think Node.js needs an introduction, since a lot has been written about this framework this last year. Express.js on the other hand, might not be so well known. Express.js is a lightweight node.js framework that allows you to create web applications and APIs much easier on top of node.js. In this series of articles we’ll look at a lot of the features Express.js provides.   In this first article though, we’ll keep things simple. I’ll show you the following things:      Installing Node.js and Express.js   Create our first 'hello world' application   Expose the first operation of a REST API   Installing Node.js and Express.js   The first thing we need to do is install Node.js. There are packages for all platforms availabel from the node.js site at: http://nodejs.org/download/      Choose the package for your environment, and install node.js. This will install, together with some other stuff, the npm and node binaries. To test whether everything is installed correctly open up a terminal and type “node”. This will open the interactive node.js shell. In this shell type (or copy) the following (to exit hit CTRL+C twice):    jos@Joss-MacBook-Pro.local:~$ node &gt; console.log('hello world'); hello world undefined &gt;  (^C again to quit) &gt;     This was your first node.js program. Hello world in one line. For a more complex example, you could copy the following (from the nodejs.org site):    jos@Joss-MacBook-Pro.local:~$ node &gt; var http = require('http'); undefined &gt; http.createServer(function (req, res) { ...   res.writeHead(200, {'Content-Type': 'text/plain'}); ...   res.end('Hello World\\n'); ... }).listen(1337, '127.0.0.1'); { _connections: 0,   connections: [Getter/Setter],   allowHalfOpen: true,   _handle: null,   _events:     { request: [Function],      connection: [Function: connectionListener] },   httpAllowHalfOpen: false } &gt; console.log('Server running at http://127.0.0.1:1337/'); Server running at http://127.0.0.1:1337/ undefined &gt;     This starts a complete webserver, running at localhost port 1337. If you open this in your browser, you’ll see the following:      For now that’s enough to know that node.js is working correctly. Now we need to setup express. Luckily this isn’t that difficult,since npm, a package manager, allows us to easily install express in a specific directory. So before we get started, we first need to create a directory that’ll hold our project files.    mkdir express    We can now use npm to install the correct packages (express in our case). For this npm check for a package.json file in this specific directory. So enter this directory and create a package.json file with the following content:    {   \"name\": \"express-smartjava\",   \"description\": \"Smartjava Express App\",   \"version\": \"0.0.1\",   \"private\": true,   \"dependencies\": {     \"express\": \"3.x\"   } }    Now we can use npm to download the express dependencies by using “npm install”    jos@Joss-MacBook-Pro.local:~/Dev/WebstormProjects/express$ npm install npm WARN package.json express-smartjava@0.0.1 No README.md file found! npm http GET https://registry.npmjs.org/express ... ├── debug@0.7.0 ├── mkdirp@0.3.3 ├── send@0.1.0 (mime@1.2.6) └── connect@2.7.0 (pause@0.0.1, bytes@0.1.0, formidable@1.0.11, qs@0.5.1)    Now you’ll have a node_modules directory that’ll contain express. (If you want to know exactly what’s installed run ‘npm ls’ in this directory). To test if all went correctly you can do the following:    jos@Joss-MacBook-Pro.local:~/Dev/WebstormProjects/express$ node &gt; var express = require('express'); undefined &gt; var app = express(); undefined &gt;     If no errors are thrown, the express module is downloaded correctly.   Create our first 'hello world' application   Now that we’ve got the basics installed, we can create our first application. We can create our complete application from scratch or let express generate a skeleton for us. For this tutorial we’ll let express generate the skeleton. To do this, we first need to install express globally. We do this using npm:    jos@Joss-MacBook-Pro.local:~/Dev/WebstormProjects/express$ sudo npm install -g express Password: npm http GET https://registry.npmjs.org/express npm http 304 https://registry.npmjs.org/express npm http GET https://registry.npmjs.org/connect/2.7.0 npm http GET https://registry.npmjs.org/commander/0.6.1 npm http GET https://registry.npmjs.org/range-parser/0.0.4 npm http GET https://registry.npmjs.org/mkdirp/0.3.3 npm http GET https://registry.npmjs.org/cookie/0.0.5 npm http GET https://registry.npmjs.org/crc/0.2.0 npm http GET https://registry.npmjs.org/fresh/0.1.0 npm http GET https://registry.npmjs.org/methods/0.0.1 npm http GET https://registry.npmjs.org/send/0.1.0 npm http GET https://registry.npmjs.org/cookie-signature/0.0.1 npm http GET https://registry.npmjs.org/debug npm http 304 https://registry.npmjs.org/connect/2.7.0 npm http 304 https://registry.npmjs.org/commander/0.6.1 npm http 304 https://registry.npmjs.org/mkdirp/0.3.3 npm http 304 https://registry.npmjs.org/range-parser/0.0.4 npm http 304 https://registry.npmjs.org/cookie/0.0.5 npm http 304 https://registry.npmjs.org/crc/0.2.0 npm http 304 https://registry.npmjs.org/fresh/0.1.0 npm http 304 https://registry.npmjs.org/methods/0.0.1 npm WARN package.json methods@0.0.1 No README.md file found! npm http 304 https://registry.npmjs.org/send/0.1.0 npm http 304 https://registry.npmjs.org/cookie-signature/0.0.1 npm http 304 https://registry.npmjs.org/debug npm http GET https://registry.npmjs.org/mime/1.2.6 npm http GET https://registry.npmjs.org/qs/0.5.1 npm http GET https://registry.npmjs.org/formidable/1.0.11 npm http GET https://registry.npmjs.org/bytes/0.1.0 npm http GET https://registry.npmjs.org/pause/0.0.1 npm http 304 https://registry.npmjs.org/mime/1.2.6 npm http 304 https://registry.npmjs.org/bytes/0.1.0 npm http 304 https://registry.npmjs.org/formidable/1.0.11 npm http 304 https://registry.npmjs.org/pause/0.0.1 npm http 304 https://registry.npmjs.org/qs/0.5.1 /opt/local/bin/express -&gt; /opt/local/lib/node_modules/express/bin/express express@3.0.3 /opt/local/lib/node_modules/express ├── methods@0.0.1 ├── fresh@0.1.0 ├── range-parser@0.0.4 ├── cookie-signature@0.0.1 ├── cookie@0.0.5 ├── crc@0.2.0 ├── commander@0.6.1 ├── debug@0.7.0 ├── mkdirp@0.3.3 ├── send@0.1.0 (mime@1.2.6) └── connect@2.7.0 (pause@0.0.1, bytes@0.1.0, formidable@1.0.11, qs@0.5.1)    Now we’ve got an ‘express’ binary we can use to generate the skeleton. Go up one directory and run the following:    jos@Joss-MacBook-Pro.local:~/Dev/WebstormProjects$ express express           destination is not empty, continue? y     create : express    create : express/package.json    create : express/app.js    create : express/public    create : express/public/images    create : express/routes    create : express/routes/index.js    create : express/routes/user.js    create : express/public/stylesheets    create : express/public/stylesheets/style.css    create : express/views    create : express/views/layout.jade    create : express/views/index.jade    create : express/public/javascripts     install dependencies:      $ cd express &amp;&amp; npm install     run the app:      $ node app    This will override the current files, and setup a basic skeleton we can start with. Go into the directory and install the dependencies (npm install) and then you can run the skeleton (node app).    jos@Joss-MacBook-Pro.local:~/Dev/WebstormProjects$ cd express jos@Joss-MacBook-Pro.local:~/Dev/WebstormProjects/express$ npm install npm WARN package.json application-name@0.0.1 No README.md file found! npm http GET https://registry.npmjs.org/jade npm http 304 https://registry.npmjs.org/jade npm http GET https://registry.npmjs.org/commander/0.6.1 npm http GET https://registry.npmjs.org/mkdirp npm http GET https://registry.npmjs.org/coffee-script npm http 304 https://registry.npmjs.org/commander/0.6.1 npm http 304 https://registry.npmjs.org/coffee-script npm http 304 https://registry.npmjs.org/mkdirp jade@0.27.7 node_modules/jade ├── commander@0.6.1 ├── mkdirp@0.3.4 └── coffee-script@1.4.0 jos@Joss-MacBook-Pro.local:~/Dev/WebstormProjects/express$ node app Express server listening on port 3000    Now we’ve got a basic express web app running on port 3000.      So now that we’ve got the basic skeleton running (by using node app), lets change some of the templates to create our helloworld application. Open the app.js file in your editor of choice (if you don’t have a good editor you should use sublime 2). For now you can ignore most of the stuff in this file. The interesting part, and the one we’ll change for our helloworld example, is the following:   var express = require('express')   , routes = require('./routes')   , user = require('./routes/user')   , http = require('http')   , path = require('path');  ...  app.get('/', routes.index); app.get('/users', user.list);   What this means is, that when a user makes a GET request to the root “/”, routes.index is returned, and if a user makes a request to “/users”, the users.list is returned. Both of these are javascript functions. The routes variable is defined through the “require(./routes)” directive. Since this one points to a directory the “index.js” file is loaded. And the user variable points to the “./routes/user” file, which loads the user.js file from the routes directory.   For this example lets just change the output of the default page. For this we’ll have a look at the routes.index function in the index.js file:   exports.index = function(req, res){   res.render('index', { title: 'Express' }); };   We’ll just change the title variable to “Welcome to the Node.js and Express.js tutorial”.   exports.index = function(req, res){   res.render('index', { title: 'Welcome to the Node.js and Express.js tutorial' }); };   Now just restart node.js (exit using ctrl-c):    $ node app Express server listening on port 3000    And when you now open localhost:3000 you can see the changed text.      Expose the first operation of a REST API   The final step we’ll take in this first part is we’ll make the start for a REST API. We’ll expand on this API in further sections to add persistency, security and more. For now we’ll just add support for one of the functions for this API. An API that allows us to bid on items. A sort of mini eBay. For now we’ll just support the function that allows us to return an auction item based on an id.   For this we want to be able to do the following:   Request:    GET /items/123    Response:    200 OK Content-Length: 434 Content-Type: application/vnd.smartbid.item+json;charset=UTF-8  {  \"name\":\"Monty Python and the search for the holy grail\",   \"id\":123,  \"startPrice\":0.69,  \"currency\":\"GBP\",  \"description\":\"Must have item\",  \"links\":[    {     \"linkType\":\"application/vnd.smartbid.item\",     \"rel\":\"Add item to watchlist\",     \"href\":\"/users/123/watchlist\"    },    {     \"linkType\":\"application/vnd.smartbid.bid\",      \"rel\":\"Place bid on item\",     \"href\":\"/items/123/bid\"    },    {     \"linkType\":\"application/vnd.smartbid.user\",     \"rel\":\"Get owner's details\",     \"href\":\"/users/123\"    }  ] }    So how do we do this in Express.js? We have to start with creating a router that handles these requests. To keep it clean we create a new file called smartbid.js, that’ll hold all the logic for our mini-ebay site. So create a smartbid.js file in the routes directory and add the following to that file:   // just a dummy response for now, persistence we'll add later var dummyResponse =  {     \"name\":\"Monty Python and the search for the holy grail\",     \"id\":123,     \"startPrice\":0.69,     \"currency\":\"GBP\",     \"description\":\"Must have item\",     \"links\":[         {             \"linkType\":\"application/vnd.smartbid.item\",             \"rel\":\"Add item to watchlist\",             \"href\":\"/users/123/watchlist\"         },         {             \"linkType\":\"application/vnd.smartbid.bid\",             \"rel\":\"Place bid on item\",             \"href\":\"/items/123/bid\"         },         {             \"linkType\":\"application/vnd.smartbid.user\",             \"rel\":\"Get owner's details\",             \"href\":\"/users/123\"         }     ] };  // pass in the app so we can register routes module.exports = function(app) {      // called from app.js     this.get = function(req, res){         res.type('application/vnd.smartbid.item+json');         res.send(200,dummyResponse);     };      // register the router     app.get('/items/:itemid', this.get); }   In this file we’ve defined a new route matcher (“app.get(‘/items/:itemid’, get);”), which will call the get method we defined in this same file. In this function we set the content-type of the response and return a dummy response with code 200.  This won’t work yet, we now need to configure express.js to include this route file in its configuration. For this we need to change the smartbid.js to the following:   // just a dummy response for now, persistence we'll add later var dummyResponse =  {     \"name\":\"Monty Python and the search for the holy grail\",     \"id\":123,     \"startPrice\":0.69,     \"currency\":\"GBP\",     \"description\":\"Must have item\",     \"links\":[         {             \"linkType\":\"application/vnd.smartbid.item\",             \"rel\":\"Add item to watchlist\",             \"href\":\"/users/123/watchlist\"         },         {             \"linkType\":\"application/vnd.smartbid.bid\",             \"rel\":\"Place bid on item\",             \"href\":\"/items/123/bid\"         },         {             \"linkType\":\"application/vnd.smartbid.user\",             \"rel\":\"Get owner's details\",             \"href\":\"/users/123\"         }     ] };  module.exports = function(app) {     app.get('/items/:itemid', get); }  // called from app.js get = function(req, res){     res.type('application/vnd.smartbid.item+json');     res.send(200,dummyResponse); };    Here we removed the routes, and split up the dependencies. We also added a new dependency to the file we just created and pass in the ‘app’ variable, so we can use it in our own route.   var express = require('express')   , http = require('http')   , path = require('path');  var app = express();  // pass in express app var smartbid = require('./routes/smartbid')(app);  app.configure(function(){   app.set('port', process.env.PORT || 3000);   app.set('views', __dirname + '/views');   app.set('view engine', 'jade');   app.use(express.favicon());   app.use(express.logger('dev'));   app.use(express.bodyParser());   app.use(express.methodOverride());   app.use(app.router);   app.use(express.static(path.join(__dirname, 'public'))); });  app.configure('development', function(){   app.use(express.errorHandler()); });  http.createServer(app).listen(app.get('port'), function(){   console.log(\"Express server listening on port \" + app.get('port')); });   And that’s it. Restart the node.js application, and you can test whether this call is working. I always test with the chrome App “Dev HTTP Client”, that you can get from here. The request and response for this simple app now look like this:      That’s it for this part of the tutorial. In the next week or so, the next part in this series will be up, where we’ll expand on this REST API and slowly add testing, media-type negotiation, persistence and security.  ","categories": ["posts","rest","express.js","node.js","tutorial"],
        "tags": [],
        "url": "http://www.smartjava.org/content/tutorial-nodejs-and-expressjs-part-i-routers/",
        "teaser":null},{
        "title": "Drag and Drop with AngularJS using jQuery UI",
        "excerpt":"In the last couple of weeks I’ve started working with AngularJS. AngularJS is a great simple framework that allows you to create complete web-apps in javascript. It provides data-binding, templating and a lot of helper functions to create maintainable, readable and extendable browser based web applications. Lets first start with a disclaimer, I’m not an AngularJS guru, so some of the things here could probably have been done better, so if you have any comments, please let me know.   What I want to show in this article is how to create a directive that allows you to create simple drag and drop lists. This was something I needed for an application, but couldn’t find any good, complete and working examples for. There are some examples out there, but mostly for older versions of AngularJS. In this article we’ll look at the two following scenarios:      Drag and drop within a single list: Probably the most common example, where you want to reorder an existing list by dragging items.   Drag and drop from one list to another: Here we have one list that contains items that we want to drag and drop into a second list.      If you want to go directly to the examples:      Drag and Drop within a single list   Drag and Drop from one list to another   Lets look at the code for these examples.   Drag and drop within a single list   We’ll start with the easy one, drag and drop within a single list. We’ll start with the html:    &lt;!DOCTYPE html&gt; &lt;html ng-app=\"dnd\"&gt; &lt;body&gt; &lt;div class=\"container\" id=\"main\"  ng-controller=\"dndCtrl\"&gt;     &lt;div class=\"row\"&gt;         &lt;div class=\"span4 offset4\"&gt;             &lt;ul id=\"single\" dnd-list=\"model\"&gt;                 &lt;li class=\"alert alert-info nomargin\"                     ng-repeat=\"item in model\"&gt;&lt;/li&gt;             &lt;/ul&gt;         &lt;/div&gt;     &lt;/div&gt; &lt;/div&gt;   &lt;!-- load all the scripts --&gt; &lt;script src=\"js/jquery-1.8.2.js\" type=\"text/javascript\"&gt;&lt;/script&gt; &lt;script src=\"js/jquery-ui-1.9.2.custom.min.js\" type=\"text/javascript\"&gt;&lt;/script&gt; &lt;script src=\"js/angular.js\" type=\"text/javascript\"&gt;&lt;/script&gt; &lt;script src=\"js/bootstrap.min.js\" type=\"text/javascript\"&gt;&lt;/script&gt;  &lt;!-- load the app scripts --&gt; &lt;script src=\"app/app.js\" type=\"text/javascript\"&gt;&lt;/script&gt; &lt;script src=\"app/dir-dnd.js\" type=\"text/javascript\"&gt;&lt;/script&gt; &lt;script src=\"app/ctrl-dnd.js\" type=\"text/javascript\"&gt;&lt;/script&gt;  &lt;/body&gt; &lt;/html&gt;    In this page we define a couple of AngularJS specifics. First we use “ng-app=dnd” to define the name of our application and we define an AngularJS controller with the name “dndCtrl”. You can also see that we create a list of items using “ng-repeat”. Last, but not least, we’ve defined a custom directive “dnd-list” to handles the drag and drop functionality. Before we look at this directive, lets quickly look at the the application (app.js) and the controller.   App.js:  var app = angular.module('dnd', []);   Nothing special, we just define the application.   The controller (ctrl-dnd.js):  function dndCtrl($scope) {      $scope.model = [         {             \"id\": 1,             \"value\": \"Who the fuck is Arthur Digby Sellers?\"         },         {             \"id\": 2,             \"value\": \"I've seen a lot of spinals, Dude, and this guy is a fake. \"         },         {             \"id\": 3,             \"value\": \"But that is up to little Larry here. Isn't it, Larry?\"         },         {             \"id\": 4,             \"value\": \" I did not watch my buddies die face down in the mud so that this fucking strumpet.\"         }     ];      // watch, use 'true' to also receive updates when values     // change, instead of just the reference     $scope.$watch(\"model\", function(value) {         console.log(\"Model: \" + value.map(function(e){return e.id}).join(','));     },true);   Also nothing special, just a dummy model, and a watch function to see if our drag and drop functionality works. The only item left is our drag and drop directive.   // directive for a single list app.directive('dndList', function() {      return function(scope, element, attrs) {          // variables used for dnd         var toUpdate;         var startIndex = -1;          // watch the model, so we always know what element         // is at a specific position         scope.$watch(attrs.dndList, function(value) {             toUpdate = value;         },true);          // use jquery to make the element sortable (dnd). This is called         // when the element is rendered         $(element[0]).sortable({             items:'li',             start:function (event, ui) {                 // on start we define where the item is dragged from                 startIndex = ($(ui.item).index());             },             stop:function (event, ui) {                 // on stop we determine the new index of the                 // item and store it there                 var newIndex = ($(ui.item).index());                 var toMove = toUpdate[startIndex];                 toUpdate.splice(startIndex,1);                 toUpdate.splice(newIndex,0,toMove);                  // we move items in the array, if we want                 // to trigger an update in angular use $apply()                 // since we're outside angulars lifecycle                 scope.$apply(scope.model);             },             axis:'y'         })     } });   As you can see from the code, it isn’t that complex. We have a single watch function that’s called whenever our model changes and store the updated model in a local variable. Next we use the JQuery UI sortable function, to enable drag and drop for our element. All we need to do know is make sure our backing model is kept consistent with the state on screen. For this we use the “start” and “stop” properties passed into the sortable function.  In start we keep track of the element that was dragged, and in stop we push the element back at the changed position in the array. The last step we need to take is that we have to inform angular of this change. We’re working outside the lifecycle of angular so with scope.$apply we inform angular that it should re-evaluate the passed in expression. All this together looks like this (look at the console output to see the output from the watch function in our controller):      Next we’ll look at the changes you need to make to allow elements to be dragged between lists.   Drag and Drop from one list to another   The HTML for this looks pretty much the same, the only thing that is changed is that we now have two lists, with a slightly different attribute.        &lt;div class=\"row\"&gt;         &lt;div class=\"span4 offset2\"&gt;             &lt;ul id=\"sourceList\" dnd-between-list=\"source,targetList\"&gt;                 &lt;li class=\"alert alert-error nomargin\"                     ng-repeat=\"item in source\"&gt;&lt;/li&gt;             &lt;/ul&gt;         &lt;/div&gt;         &lt;div class=\"span4\"&gt;             &lt;ul id=\"targetList\" dnd-between-list=\"model,sourceList\"&gt;                 &lt;li class=\"alert alert-info nomargin\"                     ng-repeat=\"item in model\"&gt;&lt;/li&gt;             &lt;/ul&gt;         &lt;/div&gt;     &lt;/div&gt;    What you see here is that besides passing in the model the list is working on, we also pass in the list to which it is connected. In a bit we’ll see how this is used. First though let’s look at our updated controller:   function dndCtrl($scope) {      $scope.model = [         {             \"id\": 1,             \"value\": \"Who the fuck is Arthur Digby Sellers?\"         },         {             \"id\": 2,             \"value\": \"I've seen a lot of spinals, Dude, and this guy is a fake. \"         },         {             \"id\": 3,             \"value\": \"But that is up to little Larry here. Isn't it, Larry?\"         },         {             \"id\": 4,             \"value\": \" I did not watch my buddies die face down in the mud so that this fucking strumpet.\"         }     ];      $scope.source = [         {             \"id\": 5,             \"value\": \"What do you mean \\\"brought it bowling\\\"? I didn't rent it shoes.\"         },         {             \"id\": 6,             \"value\": \"Keep your ugly fucking goldbricking ass out of my beach community! \"         },         {             \"id\": 7,             \"value\": \"What the fuck are you talking about? I converted when I married Cynthia!\"         },         {             \"id\": 8,             \"value\": \"Ja, it seems you forgot our little deal, Lebowski.\"         }     ];      // watch, use 'true' to also receive updates when values     // change, instead of just the reference     $scope.$watch(\"model\", function(value) {         console.log(\"Model: \" + value.map(function(e){return e.id}).join(','));     },true);      // watch, use 'true' to also receive updates when values     // change, instead of just the reference     $scope.$watch(\"source\", function(value) {         console.log(\"Source: \" + value.map(function(e){return e.id}).join(','));     },true); }   Not much has changed, we only added another simpel array we use as input for our source list, and added another listener that shows the content of that list on change. The only other thing we changed is that we added a new directive.   // directive for dnd between lists app.directive('dndBetweenList', function($parse) {      return function(scope, element, attrs) {          // contains the args for this component         var args = attrs.dndBetweenList.split(',');         // contains the args for the target         var targetArgs = $('#'+args[1]).attr('dnd-between-list').split(',');          // variables used for dnd         var toUpdate;         var target;         var startIndex = -1;         var toTarget = true;          // watch the model, so we always know what element         // is at a specific position         scope.$watch(args[0], function(value) {             toUpdate = value;         },true);          // also watch for changes in the target list         scope.$watch(targetArgs[0], function(value) {             target = value;         },true);          // use jquery to make the element sortable (dnd). This is called         // when the element is rendered         $(element[0]).sortable({             items:'li',             start:function (event, ui) {                 // on start we define where the item is dragged from                 startIndex = ($(ui.item).index());                 toTarget = false;             },             stop:function (event, ui) {                 var newParent = ui.item[0].parentNode.id;                  // on stop we determine the new index of the                 // item and store it there                 var newIndex = ($(ui.item).index());                 var toMove = toUpdate[startIndex];                  // we need to remove him from the configured model                 toUpdate.splice(startIndex,1);                  if (newParent == args[1]) {                     // and add it to the linked list                     target.splice(newIndex,0,toMove);                 }  else {                     toUpdate.splice(newIndex,0,toMove);                 }                  // we move items in the array, if we want                 // to trigger an update in angular use $apply()                 // since we're outside angulars lifecycle                 scope.$apply(targetArgs[0]);                 scope.$apply(args[0]);             },             connectWith:'#'+args[1]         })     } });   Not that much different from the previous one. But a couple of things have changed. First we parse the arguments from our own list and from the target list. We do this so we know which items in our controller we need to update:           // contains the args for this component         var args = attrs.dndBetweenList.split(',');         // contains the args for the target         var targetArgs = $('#'+args[1]).attr('dnd-between-list').split(',');   We also add an additional watch so we always have the correct variables whenever these lists change in the backend. Finally, when we look at the sortable function, not that much is changed. We use an additional “connectWith” property to tie these two lists together, based on the supplied arguments to the directive. We’ve also needed to change our “stop” function. Instead of removing and reinserting the item in the same array, we remove it from the source one, and add it to the target one.               stop:function (event, ui) {                 var newParent = ui.item[0].parentNode.id;                  // on stop we determine the new index of the                 // item and store it there                 var newIndex = ($(ui.item).index());                 var toMove = toUpdate[startIndex];                  // we need to remove him from the configured model                 toUpdate.splice(startIndex,1);                  if (newParent == args[1]) {                     // and add it to the linked list                     target.splice(newIndex,0,toMove);                 }  else {                     toUpdate.splice(newIndex,0,toMove);                 }                  // we move items in the array, if we want                 // to trigger an update in angular use $apply()                 // since we're outside angulars lifecycle                 scope.$apply(targetArgs[0]);                 scope.$apply(args[0]);             }   As you can see, all we do here is determine whether it was dropped on the specified target, or whether it was moved in the list itself. Based on this it is added to the correct model, and the models are updated. The output in the console looks like this:      We’re almost there, there is just one thing we need to take care off. What happens when one of our lists is empty? That would mean we can’t drop an item back from one list to another, since we can’t really see the empty list. To solve this we need to make sure, the empty list (the ul element) has a forced height. To do this we use the ng-class attribute, this attribute allows us to conditionally add a class based on the state of our model. To do this, first add the following helper methods to the controller:       $scope.sourceEmpty = function() {         return $scope.source.length == 0;     }      $scope.modelEmpty = function() {         return $scope.model.length == 0;     }   Next update the html with the list definition to this:            &lt;div class=\"span4 offset2\"&gt;             &lt;ul id=\"sourceList\"                 dnd-between-list=\"source,targetList\"                 ng-class=\"{'minimalList':sourceEmpty()}\"&gt;                 &lt;li class=\"alert alert-error nomargin\"                     ng-repeat=\"item in source\"&gt;&lt;/li&gt;             &lt;/ul&gt;         &lt;/div&gt;         &lt;div class=\"span4\"&gt;              &lt;ul id=\"targetList\"                 dnd-between-list=\"model,sourceList\"                 ng-class=\"{'minimalList':sourceEmpty()}\"&gt;                 &lt;li class=\"alert alert-info nomargin\"                     ng-repeat=\"item in model\"&gt;&lt;/li&gt;             &lt;/ul&gt;         &lt;/div&gt;    You can see we added a ng-class attribute that adds the minimalList class whenever a list is empty. The last thing we need to add is this minimalList style, which looks like this:            .minimalList {             min-height: 100px;         }    And now, we can also drag and drop from one list to another, when either one is empty! Simple right? You should be able to use this without changing much.  ","categories": ["posts","jqueryui","angularjs"],
        "tags": [],
        "url": "http://www.smartjava.org/content/drag-and-drop-angularjs-using-jquery-ui/",
        "teaser":null},{
        "title": "HTML5: Server-sent events with Angular.js, Node.js and Express.js",
        "excerpt":"I was playing around with Node.js and Express.js and happened to run across an article on server-sent events. Server-sent events is a W3C specification that describes how a server can push events to browser clients. All this using the standard HTTP protocol. A couple of years ago this was a very promising specification, but then websockets came along and interest for this specification diminished a bit. It is however a very nice, easy to use, light weight way of pushing updates from a server to a number of connected clients. The code, especially on the client side, is very trivial and you can be up and running in a couple of minutes.  So I decided to dive a bit deeper into this specification and I’ll show you in this article how to use server-sent events in Angular.js. I’ll create a simple server in Node.js and Express.js that’ll send the events to our Angular.js frontend.   Basically what we’re going to do in this article are the following two things:      Create a minimal Angular.js application that shows system stats   Build a node.js and Express.js backend that pushes these stats to connected browsers   When completed it will look something like this:      Building the Angular.js application   I won’t go into too much detail on how Angular.js works. If you read this you probably know the basics about Angular.js so I’ll just skip to the server-sent events part. If we want to listen to server-sent events (sse) we only have to include a small piece of javascript:           var source = new EventSource('/stats');         source.addEventListener('message', handleCallback, false);   This will create a listener that automatically tries to connect to /stats, using a standard HTTP request. If it fails or loses its connection it will immediately try to reconnect without you having to do anything. When the server sends an event, the handleCallback function will be called with the received data. Unlike websockets we can only receive text data using sse. This, however, shouldn’t be too big of an issue, since a lot of data is sent as JSON strings anyways.   So how to we use this, lets start by looking at the angular stuff first:       // define the module we're working with     var app = angular.module('sse', []);      // define the ctrl     function statCtrl($scope) {          // the last received msg         $scope.msg = {};          // handles the callback from the received event         var handleCallback = function (msg) {             $scope.$apply(function () {                 $scope.msg = JSON.parse(msg.data)             });         }          var source = new EventSource('/stats');         source.addEventListener('message', handleCallback, false);     }   Edit: changed $scope.$apply as suggested by Jim Hoskins (see first comment) Note: This is a very trivial example. Normally it’s good practice to wrap up this specific event functionality into a service which you inject in the controller, so the controller only contains application logic. As you can see we’ve created a single controller, that, when initiated, registers the listener and adds a callback. In this callback all we do is assign the received data to a scoped variable. This, however, isn’t enough to trigger the updates in our model. Since the events are received outside the Angular.js lifecycle, we need to tell Angular.js that the “msg” value has changed. This is done by using $scope.$apply. With this piece of code, every time we receive an event sent by the server, our model is updated, and any view expressions are updated.   For completeness, lets quickly look at the table definition:    div class=\"container main\" ng-controller=\"statCtrl\"&gt;     &lt;div class=\"row\"&gt;         &lt;div class=\"span10 offset1\" style=\"text-align: center\"&gt;             &lt;h2&gt;System details updated using server-sent events&lt;/h2&gt;         &lt;/div&gt;     &lt;/div&gt;     &lt;div class=\"row\"&gt;         &lt;div class=\"span8 offset2\"&gt;             &lt;table class=\"table table-striped\"&gt;                 &lt;thead&gt;                 &lt;tr&gt;                     &lt;th&gt;Property&lt;/th&gt;                     &lt;th&gt;Value&lt;/th&gt;                 &lt;/tr&gt;                 &lt;/thead&gt;                 &lt;tbody&gt;                 &lt;tr&gt;                     &lt;td&gt;Hostname:&lt;/td&gt;                     &lt;td&gt;&lt;/td&gt;                 &lt;/tr&gt;                 &lt;tr&gt;                     &lt;td&gt;Type:&lt;/td&gt;                     &lt;td&gt;&lt;/td&gt;                 &lt;/tr&gt;                 &lt;tr&gt;                     &lt;td&gt;Platform:&lt;/td&gt;                     &lt;td&gt;&lt;/td&gt;                 &lt;/tr&gt;                 &lt;tr&gt;                     &lt;td&gt;Arch:&lt;/td&gt;                     &lt;td&gt;&lt;/td&gt;                 &lt;/tr&gt;                 &lt;tr&gt;                     &lt;td&gt;Release:&lt;/td&gt;                     &lt;td&gt;&lt;/td&gt;                 &lt;/tr&gt;                 &lt;tr&gt;                     &lt;td&gt;Uptime:&lt;/td&gt;                     &lt;td&gt;&lt;/td&gt;                 &lt;/tr&gt;                 &lt;tr&gt;                     &lt;td&gt;Load avg.:&lt;/td&gt;                     &lt;td&gt;&lt;/td&gt;                 &lt;/tr&gt;                 &lt;tr&gt;                     &lt;td&gt;Total mem:&lt;/td&gt;                     &lt;td&gt;&lt;/td&gt;                 &lt;/tr&gt;                 &lt;tr&gt;                     &lt;td&gt;Free mem:&lt;/td&gt;                     &lt;td&gt;&lt;/td&gt;                 &lt;/tr&gt;                 &lt;/tbody&gt;             &lt;/table&gt;         &lt;/div&gt;     &lt;/div&gt; &lt;/div&gt;    Nothing special. Just a simple table (using bootstrap for the layout) with angular expressions.   Now that we’ve seen the front end, we need to have a server that can send events. For this example I’ve used Node.js and Express.js since I’m experimenting with these two technologies. But, as you’ll see, implementing this in any other HTTP server will be trivial.   Building the server side with node.js and express.js   In the sse specification it is explained how a server should behave if it wants to support sse. It needs to keep the HTTP connection open and respond with a specific answer, so that the browser knows it can expect events to arrive over this open connection. I’ll list the complete ‘app.js’ used in this example and highlight the interesting parts:   // most basic dependencies var express = require('express')   , http = require('http')   , os = require('os')   , path = require('path');  // create the app var app = express();  // configure everything, just basic setup app.configure(function(){   app.set('port', process.env.PORT || 3000);   app.set('views', __dirname + '/views');   app.set('view engine', 'jade');   app.use(express.favicon());   app.use(express.logger('dev'));   app.use(express.bodyParser());   app.use(express.methodOverride());   app.use(app.router);   app.use(express.static(path.join(__dirname, 'public'))); });  // simple standard errorhandler app.configure('development', function(){   app.use(express.errorHandler()); });  //--------------------------------------- // mini app //--------------------------------------- var openConnections = [];  // simple route to register the clients app.get('/stats', function(req, res) {      // set timeout as high as possible     req.socket.setTimeout(Infinity);      // send headers for event-stream connection     // see spec for more information     res.writeHead(200, {         'Content-Type': 'text/event-stream',         'Cache-Control': 'no-cache',         'Connection': 'keep-alive'     });     res.write('\\n');      // push this res object to our global variable     openConnections.push(res);      // When the request is closed, e.g. the browser window     // is closed. We search through the open connections     // array and remove this connection.     req.on(\"close\", function() {         var toRemove;         for (var j =0 ; j &lt; openConnections.length ; j++) {             if (openConnections[j] == res) {                 toRemove =j;                 break;             }         }         openConnections.splice(j,1);         console.log(openConnections.length);     }); });  setInterval(function() {     // we walk through each connection     openConnections.forEach(function(resp) {         var d = new Date();         resp.write('id: ' + d.getMilliseconds() + '\\n');         resp.write('data:' + createMsg() +   '\\n\\n'); // Note the extra newline     });  }, 1000);  function createMsg() {     msg = {};      msg.hostname = os.hostname();     msg.type = os.type();     msg.platform = os.platform();     msg.arch = os.arch();     msg.release = os.release();     msg.uptime = os.uptime();     msg.loadaverage = os.loadavg();     msg.totalmem = os.totalmem();     msg.freemem = os.freemem();      return JSON.stringify(msg); }  // startup everything http.createServer(app).listen(app.get('port'), function(){   console.log(\"Express server listening on port \" + app.get('port')); })   As you can probably see and read from the comments nothing to special happens. We’ve created a “/stats” route that respons to GET requests. This is the endpoint our Angular.js application calls when it setups it’s listener. You can see in this function that we respond with a specific HTTP header. This informs the browser that this server will be using sse to send events.       res.writeHead(200, {         'Content-Type': 'text/event-stream',         'Cache-Control': 'no-cache',         'Connection': 'keep-alive'     });     res.write('\\n');   You can also see that we keep an array of open connections. We use this to push these updates to all open connections and to remove connections when they are closed (done very primitively in this example):       // push this res object to our global variable     openConnections.push(res);      // When the request is closed, e.g. the browser window     // is closed. We search through the open connections     // array and remove this connection.     req.on(\"close\", function() {         var toRemove;         for (var j =0 ; j &lt; openConnections.length ; j++) {             if (openConnections[j] == res) {                 toRemove =j;                 break;             }         }         openConnections.splice(j,1);         console.log(openConnections.length);     });   Updates are sent as a JSON string to the connected browsers every second using the setInterval function:   setInterval(function() {     // we walk through each connection     openConnections.forEach(function(resp) {         var d = new Date();         resp.write('id: ' + d.getMilliseconds() + '\\n');         resp.write('data:' + createMsg() +   '\\n\\n'); // Note the extra newline     });  }, 1000);  function createMsg() {     msg = {};      msg.hostname = os.hostname();     msg.type = os.type();     msg.platform = os.platform();     msg.arch = os.arch();     msg.release = os.release();     msg.uptime = os.uptime();     msg.loadaverage = os.loadavg();     msg.totalmem = os.totalmem();     msg.freemem = os.freemem();      return JSON.stringify(msg); }   And that’s it. With these couple of lines of code we’ve setup a “server-sent event” server and created a client application that automatically updates its view using Angular.js.  ","categories": ["posts","html5","express.js","node.js","angularjs"],
        "tags": [],
        "url": "http://www.smartjava.org/content/html5-server-sent-events-angularjs-nodejs-and-expressjs/",
        "teaser":null},{
        "title": "CPI Interactive index (15 years) created with Angular.js, bootstrap and d3.js",
        "excerpt":"Long time since I posted anything. But it’s been a busy time with the holidays a new job and a week of snowboarding :) So after this short hiatus a quick article to highlight a web application I created a couple of weeks ago before the holidays.    If you click on the image the application will open and you can play around with the “corruption perception index” statistics from the last 15 years. I’ll try to get a couple of article on line the next couple of days or weeks explaining some of the frustations and revelations I had when working with the combination of Bootstrap, Angular.js and D3.js.  ","categories": ["posts","bootstrap","angular.js","d3.js"],
        "tags": [],
        "url": "http://www.smartjava.org/content/cpi-interactive-index-15-years-created-angularjs-bootstrap-and-d3js/",
        "teaser":null},{
        "title": "Chrome and Firefox: Desktop Notifications",
        "excerpt":"Just a short article this time, since I’m kind of swamped with work and the start of a new book on Three.js I’m writing for Packt. In this article we’ll very quickly look at the state of desktop notifications. For desktop notifications there is currently a W3C specification, but there aren’t any implementations of this standard yet.   However, Chrome does have support for Desktop notifications, but for an old version (2006) of the specification. Nevertheless, there are a couple of sites out there that detect whether you’re running Chrome, and if so ask your permission to show desktop notifications.   Creating desktop notifications using this old Chrome API actually is very easy and consists out of two steps:      First, you have to ask permission from the user, whether this site is allowed to send notifications to the destkop   If you've got this permission, you can create a simple notification using the API provided by Chrome.   I’ve created a very simple sample, that you can use to test this functionality:      To accomplish this you only need a couple of lines of code:    &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt;     &lt;title&gt;Simple Webkit notification example&lt;/title&gt; &lt;/head&gt;  &lt;h2&gt;First click the 'request permission' button&lt;/h2&gt; &lt;button id=\"request\"&gt;Request permission&lt;/button&gt; &lt;h2&gt;After permission is granted, click the 'show notification' button&lt;/h2&gt; &lt;button id=\"show\"&gt;Show notification&lt;/button&gt;  &lt;script type=\"text/javascript\"&gt;       document.getElementById('request').addEventListener('click', function() {         window.webkitNotifications.requestPermission();     }, false);       document.getElementById('show').addEventListener('click', function() {         showNotification();     }, false);      function showNotification() {         // only show if we've got the correct permissions         if (window.webkitNotifications.checkPermission() === 0) {             // note the show()             window.webkitNotifications.createNotification('images/email.jpg', 'Plain Text Notification', 'Notification from the browser!').show();         }     } &lt;/script&gt; &lt;/html&gt;   As you can see in the code, by first clicking on the ‘request permission’ button, we ask the user, through the requestPermission() call, to allow our site to create desktop notifications (as you can see in the previous figure). Next we can click on the ‘show notification’ button and a simple notification will be shown:      And that’s all you have to do! At least for those of you who run Chrome. If you run firefox you can still get desktop notifications, but you need an additional plugin: https://addons.mozilla.org/en-us/firefox/addon/html-notifications/   With this plugin, firefox will understand the chrome/webkit provided API and show the permission interface:      And allow you to also send desktop notifications to firefox users:      On a final note, the firefox guys are currently finishing the implementation of the W3C API, so in a couple of days or weeks, firefox will have it’s own set of desktop notifications, without the need for a plugin.  ","categories": ["posts","desktop notifications","chrome","html5"],
        "tags": [],
        "url": "http://www.smartjava.org/content/chrome-and-firefox-desktop-notifications/",
        "teaser":null},{
        "title": "Quickly create GUI for javascript variables with dat.GUI",
        "excerpt":"I’m currently working on a book for Three.js. This book contains lots and lots of examples that show off the various features of this great library. I want readers of this book to experiment with the examples. I want them to see how changing variables change the output and animations. You could of course do this by editing the source code, and reloading the example, but that isn’t really that practical, especially when fine-tuning a specific example. I ran across dat.GUI a couple of months ago, which is a really useful, small library that you can use to change javascript variables from a simple GUI.   In this article I’ll quickly walk you through how to get started with library and as an example I’ll show you how you can use it to control animations in Three.js. I’ve added an example here, or just click on the image below:      The code to accomplish this is really easy to understand. First add the library:   &lt;script type=\"text/javascript\" src=\"libs/dat.gui.js\"&gt;&lt;/script&gt;   The next thing we need to configure is a javascript object that will hold the properties we want to change using data.GUI. In the main part of our javascript code we add the following javascript object.           var controls = new function() {             this.rotationSpeed = 0.02;             this.bouncingSpeed = 0.03;         }   vascript object we define two properties: this.rotationSpeed and this.bouncingSpeed and their default value Next, we pass this object into a new dat.GUI object and define the range for these two properties.            var gui = new dat.GUI();         gui.add(controls, 'rotationSpeed',0,0.5);         gui.add(controls, 'bouncingSpeed',0,0.5);   The rotationSpeed and bouncingSpeed properties are both set to a range of 0 to 0.5. All we need to do now is make sure that in our render loop we reference these two properties directly, so that when we make changes through the dat.GUI user interface it immediately affects the rotation and bounce speed of our objects.    function render() {             ...          cube.rotation.x += controls.rotationSpeed;             cube.rotation.y += controls.rotationSpeed;             cube.rotation.z += controls.rotationSpeed;             step+=controls.bouncingSpeed;             sphere.position.x = 20+( 10*(Math.cos(step)));             sphere.position.y = 2 +( 10*Math.abs(Math.sin(step)));             ...         }   Now when you run this example you’ll see a simple user interface that you can use to control the bouncing and rotation speeds.   ","categories": ["posts","dat.gui","three.js"],
        "tags": [],
        "url": "http://www.smartjava.org/content/quickly-create-gui-javascript-variables-datgui/",
        "teaser":null},{
        "title": "Movie color analysis with XBMC, Boblight, Java and D3.js",
        "excerpt":"It’s been a while since I blogged, but it has been a rather busy time. Lots of big projects at work that need my complete attention, and lots of personal stuff going on. Besides that I’ve finished the first two couple of chapters for Packt on my book on Three.js, so time has been in short supply :) So, finally an update, in this update I want to show and explain a visualization experiment I’ve been working on when I get a couple of moments of time. The result of this can be found at the following two sites:      Vizualization of movies - Part I   Vizualization of movies - Part II - The Batman Trilogy   And looks like this.      I started this because a couple of months ago I saw someone visualizing movies in the form of a barcode. Every couple of seconds a frame’s ‘average’ color was determined, and put together to form a colorful barcode, that nicely shows the color usage in that specific movie. I liked the result, so I wanted to reproduce this for a couple of movies I had laying around.   I started out by looking at ways to play back movies in java/scala and analyzing the video frames myself. I quickly gave up on this approach, though. I did get java and FFMpeg working, but analyzing each frame programmatically proofed a bit more work than I initially thought. So, after some looking around, I ran into boblight. Boblight is a small library that can be used to (from the site):   Its main purpose is to create light effects from an external input, such as a video stream (desktop capture, video player, tv card), an audio stream (jack, alsa), or user input (lirc, http). Currently it only handles video input by desktop capture with xlib, video capture from v4l/v4l2 devices and user input from the commandline with boblight-constant. Boblight uses a client/server model, where clients are responsible for translating an external input to light data, and boblightd is responsible for translating the light data into commands for external light controllers.  And as an added bonus it comes with an XBMC plugin! So now I could just use my XBMC installation to play back my movies and use boblight to convert the screen to light data! Basically I needed to take the following steps to capture the light data:      Compile, install, configure and run the boblight daemon   Install the XBMC boblight plugin   Play a movie   You can find information on how to compile and install boblight on their site. I had a little bit of issues with missing libraries and headers, but some quick googling and actually reading the error messages quickly fixed this. I used the following configuration:    jos@XBMC:~$ cat /etc/boblight.conf [global] interface       127.0.0.1  [device] name            device1 output          dd bs=1 &gt; /home/jos/movie.out 2&gt;&amp;1 channels        3 type            popen interval        41700 debug           off  [color] name            red rgb             FF0000  [color] name            green rgb             00FF00  [color] name            blue rgb             0000FF  [light] name            main color           red     device1 1 color           green   device1 2 color           blue    device1 3 hscan           0 100 vscan           0 100   With this configuration the boblight daemon outputs the light information to a file with the name /home/jos/movie.out in the following (r,g,b) format:    0.282200 0.240661 0.206841  0.280939 0.239639 0.205967  0.279679 0.238619 0.205094  0.278934 0.238013 0.204573  0.276436 0.235238 0.201714  0.273940 0.232466 0.198857    With the default settings I ran a couple of experiments (see here for the first set), but the color were a bit oversaturated and the rgb values often clipped to the maximum values. The output though looked nice (see further down for the explanation how to get this):      But I wan’t completely happy with this. It looks nice, but way to bright. So after some experimenting with the saturation and some other boblight values I got better results. For instance “The Dark Knight” looked like this:      This nicely reflects the dark mood this movie sets. The XBMC boblight configuration used for this was the following:    &lt;settings&gt;     &lt;setting id=\"bobdisable\" value=\"false\" /&gt;     &lt;setting id=\"hostip\" value=\"127.0.0.1\" /&gt;     &lt;setting id=\"hostport\" value=\"19333\" /&gt;     &lt;setting id=\"movie_autospeed\" value=\"0.000000\" /&gt;     &lt;setting id=\"movie_interpolation\" value=\"true\" /&gt;     &lt;setting id=\"movie_preset\" value=\"0\" /&gt;     &lt;setting id=\"movie_saturation\" value=\"0.700000\" /&gt;     &lt;setting id=\"movie_speed\" value=\"100.000000\" /&gt;     &lt;setting id=\"movie_threshold\" value=\"20.000000\" /&gt;     &lt;setting id=\"movie_value\" value=\"2.000000\" /&gt;     &lt;setting id=\"musicvideo_autospeed\" value=\"0.000000\" /&gt;     &lt;setting id=\"musicvideo_interpolation\" value=\"false\" /&gt;     &lt;setting id=\"musicvideo_preset\" value=\"1\" /&gt;     &lt;setting id=\"musicvideo_saturation\" value=\"1.000000\" /&gt;     &lt;setting id=\"musicvideo_speed\" value=\"100.000000\" /&gt;     &lt;setting id=\"musicvideo_threshold\" value=\"0.000000\" /&gt;     &lt;setting id=\"musicvideo_value\" value=\"1.000000\" /&gt;     &lt;setting id=\"networkaccess\" value=\"false\" /&gt;     &lt;setting id=\"other_misc_initialflash\" value=\"true\" /&gt;     &lt;setting id=\"other_misc_notifications\" value=\"true\" /&gt;     &lt;setting id=\"other_static_bg\" value=\"false\" /&gt;     &lt;setting id=\"other_static_blue\" value=\"128.000000\" /&gt;     &lt;setting id=\"other_static_green\" value=\"128.000000\" /&gt;     &lt;setting id=\"other_static_onscreensaver\" value=\"false\" /&gt;     &lt;setting id=\"other_static_red\" value=\"128.000000\" /&gt;     &lt;setting id=\"overwrite_cat\" value=\"false\" /&gt;     &lt;setting id=\"overwrite_cat_val\" value=\"0\" /&gt;     &lt;setting id=\"sep1\" value=\"\" /&gt;     &lt;setting id=\"sep2\" value=\"\" /&gt;     &lt;setting id=\"sep3\" value=\"\" /&gt; &lt;/settings&gt;   At this point I can play back a complete movie, be patient, and at the end of the movie I’ve got a complete set of colors at 24.9 FPS interval for the complete movie in the format I showed previously. With this format we can now easily create the visualizations I showed earlier. The following is my simple (very ugly) experimental java code I used for this (also tried realtime in javascript and canvas, but “The Dark Knight Rises” for instance contains over 260000 measurements and it took a while):   import java.awt.image.BufferedImage; import java.io.File; import java.io.IOException; import java.util.ArrayList; import java.util.List;  import javax.imageio.ImageIO; import org.apache.commons.io.FileUtils;  public class BobLightConverter { \t \t// step through the measure points \tprivate final static int STEP = 12; \t// how wide is the picture \tprivate final static int WIDTH = 1024; \t// how much rows do we print (-1 for automatic) \tprivate final static int HEIGHT = -1; \t// point height and width \tprivate final static int P_WIDTH = 1; \tprivate final static int P_HEIGHT = 50; \t  \tprivate final static String SRC = \"dark.knight.out\"; \t \t/** \t * @param args \t * @throws IOException  \t */ \tpublic static void main(String[] args) throws IOException { \t\tString data = FileUtils.readFileToString(new File(\"/Users/jos/Desktop/\" + SRC)); \t\t \t\t \t\tString[] rows = data.split(\"\\n\"); \t\tList&lt;String&gt; filteredRows = new ArrayList&lt;String&gt;(); \t\t \t\tSystem.out.println(\"Total number of points: \" + rows.length); \t\t// first filter the rows based on the steps \t\tint stepCount = 0; \t\tfor (String row : rows) { \t\t\tstepCount++; \t\t\tif (stepCount%STEP==0) { \t\t\t\tfilteredRows.add(row); \t\t\t} \t\t} \t\t \t\tSystem.out.println(\"Filtered number of points: \" + filteredRows.size()); \t\t// next calculate how many elements we can store into the width \t\tint nWidth = (int) Math.ceil(WIDTH/P_WIDTH); \t\tSystem.out.println(nWidth); \t\tint nHeight = HEIGHT; \t\tif (nHeight == -1) { \t\t\t// calculate the height based on the image width, the P_WIDTH and P_HEIGHT \t\t\tnHeight = (int) Math.ceil(((filteredRows.size())/nWidth)+1)*P_HEIGHT; \t\t} \t\t \t\t \t\tBufferedImage image = new BufferedImage(WIDTH,nHeight, BufferedImage.TYPE_INT_RGB); \t\t \t\tint x = 0; \t\tint y = -P_HEIGHT; \t\tfor (String row : filteredRows) { \t\t\tString[] rgb = row.split(\" \"); \t\t\tint r = Math.round(Float.valueOf(rgb[0])*255); \t\t\tint g = Math.round(Float.valueOf(rgb[1])*255); \t\t\tint b = Math.round(Float.valueOf(rgb[2])*255);  \t\t\t// the size of each line \t\t\tif (x%WIDTH==0) { \t\t\t\tx=0; \t\t\t\ty+=P_HEIGHT; \t\t\t}\t\t\t \t\t\t \t\t\tfor (int i = 0 ; i &lt; P_WIDTH ; i++) { \t\t\t\tfor (int j = 0 ; j &lt; P_HEIGHT ; j++) { \t\t\t\t\timage.setRGB(x+i, y+j, 65536 * r + 256 * g + b);\t\t \t\t\t\t} \t\t\t} \t\t\tx+=P_WIDTH; \t\t} \t\t \t\tFile f = new File(\"/Users/jos/\" + SRC + \".png\"); \t\tImageIO.write(image, \"PNG\", f); \t} }   Not the most complex code, but with this code I can simple state the dimensions I want to have and produce, at least in my eyes, great looking visualizations. That’s pretty much all I wanted to write. As a final note, in the Batman Trilogy example I show three donuts created using D3.js. To create these I first used a simple java program to create a histogram of all the colors used. These colors are first grouped together based on rgb values (to restrict number of values) and next sorted based on their HSV value to sort from dark to light. The output from that looks like this:    r,g,b,count 0,0,0,225 7,0,0,8 0,7,7,1 7,7,7,148 7,7,0,15 14,14,14,353   For each color the count represents how often this specific color is used in the image. This is used in D3.js to create a donut.       var pie = d3.layout.pie()             .sort(null)             .value(function(d) { return d.count; });      var svg = d3.select(\"#donut-3\").append(\"svg\")             .attr(\"width\", width)             .attr(\"height\", height)             .append(\"g\")             .attr(\"transform\", \"translate(\" + width / 2 + \",\" + height / 2 + \")\");      d3.csv(\"data/histogram-darkknight.csv\", function(error, data) {          data.forEach(function(d) {             d.count = parseInt(d.count);         });          var g = svg.selectAll(\".arc\")                 .data(pie(data))                 .enter().append(\"g\")                 .attr(\"class\", \"arc\");            g.append(\"path\")                 .attr(\"d\", arc)                 //.style(\"fill\", function(d) { return color(d.data.r); });                 .style(\"stroke-width\",\"0.1\")                 .style(\"stroke\", function(d) {                     var color =                             (d3.rgb(parseInt(d.data.r)                                     ,parseInt(d.data.g)                                     ,parseInt(d.data.b)).toString())                      return color;                 })                 .style(\"fill\", function(d) {                     var color =                     (d3.rgb(parseInt(d.data.r)                             ,parseInt(d.data.g)                             ,parseInt(d.data.b)).toString())                      return color;                 });      });   Won’t dive into the details of the code here. If you’re interested though, let me know.   That’s it for this article. The examples can be found here:      Vizualization of movies - Part I   Vizualization of movies - Part II - The Batman Trilogy   And if you want to raw data, let me know and I’ll put it online somewhere.  ","categories": ["posts","java","d3.js","visualization"],
        "tags": [],
        "url": "http://www.smartjava.org/content/movie-color-analysis-xbmc-boblight-java-and-d3js/",
        "teaser":null},{
        "title": "The Truth About HTML5",
        "excerpt":"I work for Malmberg, which is a publisher of educational content. Within our company we have a lot of very interactive content that is currently made available as Flash (and some as HTML5). A lot of people within Malmberg hear about HTML5, how it is write once run anywhere, can completely replace Flash and is the solution for all our mobile device requirements. This presentation, with a very pretentious title ;), tries to provide a more level perspective about what HTML5 really is, what it’s strengths are, and where it, at this moment, is still lacking.   This presentation tries to show this from a non (very) technical perspective.       Truth About HTML5  from josdirksen   As a final note, I do believe that HTML5 will be the standard way in which we’ll be developing cross-platform and cross-browser web applications. For the best user interaction though, it’s though to beat a native app.  ","categories": ["posts","overview","html5"],
        "tags": [],
        "url": "http://www.smartjava.org/content/truth-about-html5/",
        "teaser":null},{
        "title": "Create a simpe RESTful service with vert.x 2.0, RxJava and mongoDB",
        "excerpt":"A new article after an hiatus of almost half a year. In this article we’ll have quick look at how you can get started with vert.x and more interestingly how you can use RxJava to make programming asynchronous systems a lot easier. We’ll cover the following subjects:      Create an empty vert.x project using maven   Import in IntelliJ and create a simple HTTP server   Load data from mongoDB using the vert.x mongoDB persistor module   Expose the zips through a REST interface   Replace the callbacks with RxJava observers   The first thing to do is very simple, we just use a standard Maven archetype to create a vert.x project. (note the complete final example can be downloaded from github: https://github.com/josdirksen/smartjava/tree/master/vertx-demo-1)   Create an empty vert.x project using maven   Go to the directory where you want to create your vert.x project, type the following and press enter:    jos@Joss-MacBook-Pro.local:~/Dev/playground$ mvn archetype:generate -Dfilter=io.vertx:   This shows all the available io.vertx archetypes (in this case only 1)   [INFO] Scanning for projects... [INFO]                                                                          [INFO] ------------------------------------------------------------------------ [INFO] Building Maven Stub Project (No POM) 1 [INFO] ------------------------------------------------------------------------ [INFO]  [INFO] &gt;&gt;&gt; maven-archetype-plugin:2.2:generate (default-cli) @ standalone-pom &gt;&gt;&gt; [INFO]  [INFO] &lt;&lt;&lt; maven-archetype-plugin:2.2:generate (default-cli) @ standalone-pom &lt;&lt;&lt; [INFO]  [INFO] --- maven-archetype-plugin:2.2:generate (default-cli) @ standalone-pom --- [INFO] Generating project in Interactive mode [INFO] No archetype defined. Using maven-archetype-quickstart (org.apache.maven.archetypes:maven-archetype-quickstart:1.0) Choose archetype: 1: remote -&gt; io.vertx:vertx-maven-archetype (-) Choose a number or apply filter (format: [groupId:]artifactId, case sensitive contains): :    Since there is only one, just enter ‘1’ and press enter. Next it will show you the versions you can choose. For this example I’ve selected the 2.0.1-final version.    Choose io.vertx:vertx-maven-archetype version:  1: 1.0.0-beta1 2: 1.0.0-beta2 3: 1.0.0-beta3 4: 1.0.0-CR1 5: 1.0.0-CR2 6: 2.0.0-final 7: 2.0.1-final Choose a number: 7:   Enter ‘7’ and press enter. The next steps allow you to define the name and version of your project:    Define value for property 'groupId': : org.smartjava Define value for property 'artifactId': : vertx-demo-1 Define value for property 'version':  1.0-SNAPSHOT: :  Define value for property 'package':  org.smartjava: :  Confirm properties configuration: groupId: org.smartjava artifactId: vertx-demo-1 version: 1.0-SNAPSHOT package: org.smartjava  Y: : Y   Enter the values you can see above (or use your own ones) and finally enter ‘Y’ to confirm your choices. Now a project will be created:    [INFO] ---------------------------------------------------------------------------- [INFO] Using following parameters for creating project from Archetype: vertx-maven-archetype:2.0.1-final [INFO] ---------------------------------------------------------------------------- [INFO] Parameter: groupId, Value: org.smartjava [INFO] Parameter: artifactId, Value: vertx-demo-1 [INFO] Parameter: version, Value: 1.0-SNAPSHOT [INFO] Parameter: package, Value: org.smartjava [INFO] Parameter: packageInPathFormat, Value: org/smartjava [INFO] Parameter: package, Value: org.smartjava [INFO] Parameter: version, Value: 1.0-SNAPSHOT [INFO] Parameter: groupId, Value: org.smartjava [INFO] Parameter: artifactId, Value: vertx-demo-1 [INFO] project created from Archetype in dir: /Users/jos/Dev/playground/vertx-demo-1 [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 5:37.710s [INFO] Finished at: Sun Nov 24 14:55:12 CET 2013 [INFO] Final Memory: 9M/24M [INFO] ------------------------------------------------------------------------   To test whether everything went correct, just go to the directory that was just created, and run ‘mvn install’. this will download all the required libraries, run some tests and install your project to your local maven repo. Now that we’ve got a maven project, we can load it up into our favourite IDE. In my case I use IntelliJ, but Eclipse works in pretty much the same manner.   Import in IntelliJ and create a simple HTTP server   Fire up IntelliJ and select ‘File-&gt;Import Project’, navigate to the directory that was created by maven and import the project.      Just hit ‘next’ on all the questions and you’ve got yourself a project inside IntelliJ. If you create a project based on this archetype you automatically get a number of verticles you can experiment with. A couple of those are defined in groovy. IntelliJ automatically tries to compile those, but since it can’t find a groovy compiler the compile/make process fails. In this example we’ll first focus on the Java part of vert.x so just remove the .groovy files from the ‘src/main/resources’ and the ‘test/resources/integration_tests/groovy’ directory.   We could now run vert.x with the provided handlers directly through maven by installing the module using maven and then calling the ‘vertx:runModIDEA’ goal. Note that you do need to first call ‘mvn:compile’ to see your changes. If you don’t want to use maven to run your project from the IDE, you can also use a different approach, where you use the ‘org.vertx.java.platform.impl.cli.Starter’ class to launch vert.x directly from the IDE. In IntelliJ you create the following launch configuration for this:      If you run this you’ll still see an error. Something like this:    Exception in thread \"main\" java.lang.ClassNotFoundException: org.vertx.java.platform.impl.cli.Starter \tat java.net.URLClassLoader$1.run(URLClassLoader.java:366) \tat java.net.URLClassLoader$1.run(URLClassLoader.java:355) \tat java.security.AccessController.doPrivileged(Native Method) \tat java.net.URLClassLoader.findClass(URLClassLoader.java:354) \tat java.lang.ClassLoader.loadClass(ClassLoader.java:424) \tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) \tat java.lang.ClassLoader.loadClass(ClassLoader.java:357) \tat java.lang.Class.forName0(Native Method) \tat java.lang.Class.forName(Class.java:190) \tat com.intellij.rt.execution.application.AppMain.main(AppMain.java:113)   The reason you see this, is because in the pom.xml created by the vert.x archetype, the vert.x libraries are specified as ‘provided’. As a quick fix, open the pom.xml and change the scope of the three io.vertx dependencies from ‘provided’ to ‘compile’. Now, when you run this launcher from IntelliJ, vert.x will launch correctly.    /Library/Java/JavaVirtualMachines/jdk1.7.0_25.jdk/Contents/Home/bin/java -Didea.launcher.port=7543 \"-Didea.launcher.bin.path=/Applications/IntelliJ IDEA 12.app/bin\" -Dfile.encoding=UTF-8 -classpath \"...\" com.intellij.rt.execution.application.AppMain org.vertx.java.platform.impl.cli.Starter runmod org.smartjava~vertx-demo-1~1.0-SNAPSHOT log4j:WARN No appenders could be found for logger (io.netty.util.internal.logging.InternalLoggerFactory). log4j:WARN Please initialize the log4j system properly. log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. Nov 24, 2013 3:43:26 PM org.vertx.java.core.logging.impl.JULLogDelegate info INFO: Module org.smartjava~vertx-demo-1~1.0-SNAPSHOT successfully installed Nov 24, 2013 3:43:26 PM org.vertx.java.core.logging.impl.JULLogDelegate info INFO: PingVerticle started Nov 24, 2013 3:43:26 PM org.vertx.java.core.logging.impl.JULLogDelegate info INFO: Succeeded in deploying module   Now that we’ve got a project setup in IntelliJ and can easy run it directly from the IDE (and restart it with ctrl-F5), lets start creating a simple HTTP server so that we can see some output from the browser to make testing easier (note that there are much better ways of testing vert.x and verticles than I’m showing here, but that’s something for a different article). Open up the PingVerticle.java file and replace the start method with the following code:   package org.smartjava;  import org.vertx.java.core.Handler; import org.vertx.java.core.http.HttpServerRequest; import org.vertx.java.platform.Verticle;  public class PingVerticle extends Verticle {    public void start() {      vertx.createHttpServer().requestHandler(new Handler&lt;HttpServerRequest&gt;() {         @Override         public void handle(HttpServerRequest httpServerRequest) {             httpServerRequest.response().end(\"Hello smartjava\");         }     }).listen(8888);       container.logger().info(\"Webserver started, listening on port: 8888\");    } }   Run this, and open a browser to localhost:8888, and you’ll see the following.      That’s a webserver you’ve created in vert.x and ran directly from your IDE. Piece of cake so far. Now lets get some data to play around with.   Load data from mongoDB using the vert.x mongoDB persistor module   I won’t dive into how you install mongoDB, there are articles enough on the internet explaining that. If you’ve run on a Mac and have macports installed you can simply use the following command line to install mongoDB:     sudo port install mongodb   In the rest of this article I assume you’ve got mongoDB installed and it’s command line utils are available from the console. The first thing we need to do is get some data to play around with. For this example we’ll use the list of zip codes you can download from the mongoDB site: http://media.mongodb.org/zips.json. Download this file, open a console and run the following to first start mongoDB and then import this list of zips into mongoDB.    jos@Joss-MacBook-Pro.local:~/Dev/playground/vertx-demo-1$ mkdir data jos@Joss-MacBook-Pro.local:~/Dev/playground/vertx-demo-1$ mongod --dbpath ./data/ Sun Nov 24 16:23:51.765 [initandlisten] MongoDB starting : pid=77755 port=27017 dbpath=./data/ 64-bit host=Joss-MacBook-Pro.local Sun Nov 24 16:23:51.765 [initandlisten] db version v2.4.5 Sun Nov 24 16:23:51.765 [initandlisten] git version: nogitversion Sun Nov 24 16:23:51.765 [initandlisten] build info: Darwin Joss-MacBook-Pro.local 12.4.0 Darwin Kernel Version 12.4.0: Wed May  1 17:57:12 PDT 2013; root:xnu-2050.24.15~1/RELEASE_X86_64 x86_64 BOOST_LIB_VERSION=1_54 Sun Nov 24 16:23:51.765 [initandlisten] allocator: tcmalloc Sun Nov 24 16:23:51.765 [initandlisten] options: { dbpath: \"./data/\" } Sun Nov 24 16:23:51.766 [initandlisten] journal dir=./data/journal Sun Nov 24 16:23:51.766 [initandlisten] recover : no journal files present, no recovery needed Sun Nov 24 16:23:51.779 [FileAllocator] allocating new datafile ./data/local.ns, filling with zeroes... Sun Nov 24 16:23:51.779 [FileAllocator] creating directory ./data/_tmp Sun Nov 24 16:23:51.812 [FileAllocator] done allocating datafile ./data/local.ns, size: 16MB,  took 0.031 secs Sun Nov 24 16:23:51.853 [FileAllocator] allocating new datafile ./data/local.0, filling with zeroes... Sun Nov 24 16:23:52.254 [FileAllocator] done allocating datafile ./data/local.0, size: 64MB,  took 0.4 secs Sun Nov 24 16:23:52.260 [initandlisten] command local.$cmd command: { create: \"startup_log\", size: 10485760, capped: true } ntoreturn:1 keyUpdates:0  reslen:37 480ms Sun Nov 24 16:23:52.260 [initandlisten] waiting for connections on port 27017 Sun Nov 24 16:23:52.260 [websvr] admin web console waiting for connections on port 28017   Now we can use mongoImport to import the downloaded zip codes:    jos@Joss-MacBook-Pro.local:~/Dev/playground/vertx-demo-1$ wget http://media.mongodb.org/zips.json --2013-11-24 16:25:45--  http://media.mongodb.org/zips.json Resolving media.mongodb.org... 54.230.131.14, 54.230.131.51, 54.230.128.129, ... Connecting to media.mongodb.org|54.230.131.14|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 2871006 (2.7M) [application/json] Saving to: `zips.json'  100%[======================================&gt;] 2,871,006   2.20M/s   in 1.2s      2013-11-24 16:25:47 (2.20 MB/s) - `zips.json' saved [2871006/2871006]  jos@Joss-MacBook-Pro.local:~/Dev/playground/vertx-demo-1$ mongoimport --db vertx --collection zips --file ./zips.json  connected to: 127.0.0.1 Sun Nov 24 16:26:28.337 check 9 29470 Sun Nov 24 16:26:28.458 imported 29470 objects jos@Joss-MacBook-Pro.local:~/Dev/playground/vertx-demo-1$    If you’ve installed the mongoDB plugin in IntelliJ you can easily test whether it is working:      At this point we only need to call the mongoDB instance from vert.x and load the data. For this we’ll use the mongodb persistor library. First we need to add this module to the maven build (note that this is mainly for when we want to debug, internally, vert.x resolves this module itself):         &lt;dependency&gt;           &lt;groupId&gt;io.vertx&lt;/groupId&gt;           &lt;artifactId&gt;mod-mongo-persistor&lt;/artifactId&gt;           &lt;version&gt;2.1.0-SNAPSHOT&lt;/version&gt;           &lt;scope&gt;compile&lt;/scope&gt;       &lt;/dependency&gt;   Vert.x has a very nice and interesting module system (also something for a different article), to be able to use this mongo-persistor we first need to deploy it as a module. This is actually pretty easy to do:   // load the general config object, loaded by using -config on command line JsonObject appConfig = container.config();  // deploy the mongo-persistor module, which we'll use for persistence container.deployModule(\"io.vertx~mod-mongo-persistor~2.1.0-SNAPSHOT\", appConfig.getObject(\"mongo-persistor\"));   What we do here, is we load in the configuration for this module, and then call deployModule, with the name of the module, and the relevant part of the configuration. First off, lets look at the configuration we use for this:    {     \"mongo-persistor\" : {         \"address\": \"mongodb-persistor\",         \"host\": \"localhost\",         \"port\": 27017,         \"pool_size\": 10,         \"db_name\": \"vertx\"     } }   Nothing to difficult. We just point the mongo-persister unit to our mongoDB instance. The one question you might have is, how do we get this file inside vert.x. For that we just have to make a small change to our launcher and change the program arguments from:   runmod org.smartjava~vertx-demo-1~1.0-SNAPSHOT   to this:   runmod org.smartjava~vertx-demo-1~1.0-SNAPSHOT -conf src/main/resources/config.json   The config.json file, contains the configuration we just showed. So with this setup we’ve got the mongodb-persistor library listening on eventbus address “mongodb-persistor”. Now all that is left to do, is send messages to this endpoint in a format understood by this module. For this first step we’re just going to search for all the zip codes in the state “AL”. If you look through the documentation of https://github.com/vert-x/mod-mongo-persistor/ you can see that we have tell this module the ‘collection’ we want to search through and the type of ‘action’ we want to use. Depending on the action additional configuration is required. To search for all the zipcodes in the state of ‘AL’ we need to create the following json message:    {     \"action\": \"find\",     \"collection\": \"zips\",     \"matcher\": {         \"state\": \"AL\"     } }    Lets change the request handler and look at the complete start method:    public void start() {          // load the general config object, loaded by using -config on command line         JsonObject appConfig = container.config();          // deploy the mongo-persistor module, which we'll use for persistence         container.deployModule(\"io.vertx~mod-mongo-persistor~2.1.0-SNAPSHOT\", appConfig.getObject(\"mongo-persistor\"));          // create and run the server         vertx.createHttpServer().requestHandler(new Handler&lt;HttpServerRequest&gt;() {             @Override             public void handle(final HttpServerRequest httpServerRequest) {                  // we send the response from the mongo query back to the client.                 // first create the query                 JsonObject matcher = new JsonObject().putString(\"state\", \"AL\");                 JsonObject json = new JsonObject().putString(\"collection\", \"zips\")                         .putString(\"action\", \"find\")                         .putObject(\"matcher\", matcher);                  // send it over the bus                 vertx.eventBus().send(\"mongodb-persistor\", json, new Handler&lt;Message&lt;JsonObject&gt;&gt;() {                      @Override                     public void handle(Message&lt;JsonObject&gt; message) {                         // send the response back, encoded as string                         httpServerRequest.response().end(message.body().encodePrettily());                     }                 });             }         }).listen(8888);          // output that the server is started         container.logger().info(\"Webserver started, listening on port: 8888\");     }   Here you can see that we create the correct json message, send it over the bus, and wait with sending the response back until we get a response from mongoDB. We prettify this response and send it back to the client:      Expose the zips through a REST interface   Now that we’ve got the basis backend components in place, lets look at what it takes to create a simple REST based frontend. We’ll skip the mediatype specific filtering (I’ll add that to a later article), for now we’ll just look at the HTTP verbs and the urls. For this part we want to support the following REST calls:    * GET /zips  Show all the zipcode information that are stored in mongoDB * GET /zips/:id  Show the information belonging to the specified zip code * GET /zips?state=:state&amp;city=:city  Simple search service, where you can search for zip codes per city or state  * POST /zips/:id  Update existing zip code information   Very simple, but the main goal here is to show how it is done, not how you can create a full RESTful service. To handle these various URLs and verbs, vert.x provides a route matcher: (method bodies left out for clarity)           RouteMatcher matcher = new RouteMatcher();          // the matcher for the complete list and the search         matcher.get(\"/zips\", new Handler&lt;HttpServerRequest&gt;() {...}          // the matcher for a specific id         matcher.get(\"/zips/:id\", new Handler&lt;HttpServerRequest&gt;() {...}          // the matcher for the update         matcher.post(\"/zips/:id\", new Handler&lt;HttpServerRequest&gt;() {...}             vertx.createHttpServer().requestHandler(matcher).listen(8888);   For those of you who’ve worked with libraries such as sinatra or scalatra, this’ll look familiar. We define the method we want to process (get and post in this case), the url we’re interested in and the handler that will be called when a request is received. As you can see in the last line, we pass in this handler to process the requests for the server we created.   Now lets have a quick look at the implementation of these handlers. This is where we create the mongo-persistor messages that communicate with mongoDB for us. I won’t go into too much detail of these methods since they are pretty much self explanatory:           // the matcher for the complete list and the search         matcher.get(\"/zips\", new Handler&lt;HttpServerRequest&gt;() {             public void handle(final HttpServerRequest req) {                  JsonObject json = new JsonObject();                 MultiMap params = req.params();                  if (params.size() &gt; 0 &amp;&amp; params.contains(\"state\") || params.contains(\"city\")) {                     // create the matcher configuration                     JsonObject matcher = new JsonObject();                     if (params.contains(\"state\")) matcher.putString(\"state\", params.get(\"state\"));                     if (params.contains(\"city\")) matcher.putString(\"city\", params.get(\"city\"));                      // create the message for the mongo-persistor verticle                     json = new JsonObject().putString(\"collection\", \"zips\")                             .putString(\"action\", \"find\")                             .putObject(\"matcher\", matcher);                  } else {                     // create the query                     json = new JsonObject().putString(\"collection\", \"zips\")                             .putString(\"action\", \"find\")                             .putObject(\"matcher\", new JsonObject());                 }                  JsonObject data = new JsonObject();                 data.putArray(\"results\", new JsonArray());                 // and call the event we want to use                 vertx.eventBus().send(\"mongodb-persistor\", json, new ReplyHandler(req, data));             }         });   In this method we retrieve all the zipcodes from mongoDB. since mongo-persistor doesn’t return everything we have to iterate over the response. We do this using the following ReplyHandler:       private static class ReplyHandler implements Handler&lt;Message&lt;JsonObject&gt;&gt; {          private final HttpServerRequest request;         private JsonObject data;          private ReplyHandler(final HttpServerRequest request, JsonObject data) {             this.request = request;             this.data = data;         }          @Override         public void handle(Message&lt;JsonObject&gt; event) {             // if the response contains more message, we need to get the rest             if (event.body().getString(\"status\").equals(\"more-exist\")) {                 JsonArray results = event.body().getArray(\"results\");                  for (Object el : results) {                     data.getArray(\"results\").add(el);                 }                  event.reply(new JsonObject(), new ReplyHandler(request, data));             } else {                  JsonArray results = event.body().getArray(\"results\");                 for (Object el : results) {                     data.getArray(\"results\").add(el);                 }                  request.response().putHeader(\"Content-Type\", \"application/json\");                 request.response().end(data.encodePrettily());             }         }     }   In this replyHandler we just walk through the results and keep asking for more content until we don’t see the status “more-exist” anymore. I’ll skip the handler where we just retrieve a single zip code, since it isn’t that interesting. The next handler processes the post function with which we update an existing element.           matcher.post(\"/zips/:id\", new Handler&lt;HttpServerRequest&gt;() {             public void handle(final HttpServerRequest req) {                  // process the body                 req.bodyHandler(new Handler&lt;Buffer&gt;() {                      @Override                     public void handle(Buffer event) {                         // normally we'd validate the input, for now just assume it is correct.                         final String body = event.getString(0,event.length());                          // create the query                         JsonObject newObject = new JsonObject(body);                         JsonObject matcher = new JsonObject().putString(\"_id\", req.params().get(\"id\"));                         JsonObject json = new JsonObject().putString(\"collection\", \"zips\")                                 .putString(\"action\", \"update\")                                 .putObject(\"criteria\", matcher)                                 .putBoolean(\"upsert\", false)                                 .putBoolean(\"multi\",false)                                 .putObject(\"objNew\",newObject);                          // and call the event we want to use                         vertx.eventBus().send(\"mongodb-persistor\", json, new Handler&lt;Message&lt;JsonObject&gt;&gt;() {                             @Override                             public void handle(Message&lt;JsonObject&gt; event) {                                 // we could handle the errors here, but for now                                 // assume everything went ok, and return the original                                 // and updated json                                 req.response().end(body);                             }                         });                     }                 });             }         });   The code in itself isn’t that complex. We first use a handler to process the request, from this handler we create a new handler that is used to get the body of the request and finally a handler is created that updates the database and sends a response back. Even though it isn’t complex, it gets a bit cumbersome and hard to read when more and more handlers are involved. So in the last section of this article we’ll have a look at how you can replace the nested handlers using rxjava provided functionality.   Replace the callbacks with RxJava observers   For the rxjava code we’ll just add a couple of extra handlers that match a different url. So instead of /zips/90210 the url will be /rxzips/90210. To start first add the following dependency to your maven configuration:          &lt;dependency&gt;           &lt;groupId&gt;io.vertx&lt;/groupId&gt;           &lt;artifactId&gt;mod-rxjava&lt;/artifactId&gt;           &lt;version&gt;1.0.0-beta2-SNAPSHOT&lt;/version&gt;           &lt;scope&gt;compile&lt;/scope&gt;       &lt;/dependency&gt;   Before diving into how rxjava and vert.x can be used together a quick quote (from the rxjava site) that explains why this is useful:   Java Futures are straightforward to use for a single level of asynchronous execution but they start to add non-trivial complexity when they’re nested.  It is difficult to use Futures to optimally compose conditional asynchronous execution flows (or impossible, since latencies of each request vary at runtime). This can be done, of course, but it quickly becomes complicated (and thus error-prone) or it prematurely blocks on Future.get(), which eliminates the benefit of asynchronous execution.  RxJava Observables on the other hand are intended for composing flows and sequences of asynchronous data.  With the RxJava Observables it is easy to compose flows and sequences of asynchronous data. If you look through the last couple of code samples from our example you could see why this would be useful. In the last ‘post’ example we had three nested callbacks, with observables it is much easier to compose this and let the code actually tell what is happening.   Now lets extend the post method to do the following:                    first get the body                  after we have the body, we update the element in the database                  next we get the latest version from the database, after the update has succeeded                  after we've got the latest version we return this in the response.   If we did this using callbacks we’d probably need four nested levels of callbacks. In rxjava we can do this in the following manner:           matcher.post(\"/rxzips/:id\", new Handler&lt;HttpServerRequest&gt;() {             public void handle(final HttpServerRequest req) {                 // first access the buffer as an observable. We do this this way, since                 // we want to keep using the matchhandler and we can't do that with rxHttpServer                 Observable&lt;Buffer&gt; reqDataObservable = RxSupport.toObservable(req);                  // after we have the body, we update the element in the database                 Observable&lt;RxMessage&lt;JsonObject&gt;&gt; updateObservable = reqDataObservable.flatMap(new Func1&lt;Buffer, Observable&lt;RxMessage&lt;JsonObject&gt;&gt;&gt;() {                     @Override                     public Observable&lt;RxMessage&lt;JsonObject&gt;&gt; call(Buffer buffer) {                         System.out.println(\"buffer = \" + buffer);                         // create the message                         JsonObject newObject = new JsonObject(buffer.getString(0, buffer.length()));                         JsonObject matcher = new JsonObject().putString(\"_id\", req.params().get(\"id\"));                         JsonObject json = new JsonObject().putString(\"collection\", \"zips\")                                 .putString(\"action\", \"update\")                                 .putObject(\"criteria\", matcher)                                 .putBoolean(\"upsert\", false)                                 .putBoolean(\"multi\", false)                                 .putObject(\"objNew\", newObject);                          // and return an observable                         return rxEventBus.send(\"mongodb-persistor\", json);                     }                 });                  // use the previous input again, so we could see whether the update was successful.                 Observable&lt;RxMessage&lt;JsonObject&gt;&gt; getLatestObservable = updateObservable.flatMap(new Func1&lt;RxMessage&lt;JsonObject&gt;, Observable&lt;RxMessage&lt;JsonObject&gt;&gt;&gt;() {                     @Override                     public Observable&lt;RxMessage&lt;JsonObject&gt;&gt; call(RxMessage&lt;JsonObject&gt; jsonObjectRxMessage) {                         System.out.println(\"jsonObjectRxMessage = \" + jsonObjectRxMessage);                         // next we get the latest version from the database, after the update has succeeded                         // this isn't dependent on the previous one. It just has to wait till the previous                         // one has updated the database, but we could check whether the previous one was successfully                         JsonObject matcher = new JsonObject().putString(\"_id\", req.params().get(\"id\"));                         JsonObject json2 = new JsonObject().putString(\"collection\", \"zips\")                                 .putString(\"action\", \"find\")                                 .putObject(\"matcher\", matcher);                         return rxEventBus.send(\"mongodb-persistor\", json2);                     }                 });                  // after we've got the latest version we return this in the response.                 getLatestObservable.subscribe(new Action1&lt;RxMessage&lt;JsonObject&gt;&gt;() {                     @Override                     public void call(RxMessage&lt;JsonObject&gt; jsonObjectRxMessage) {                         req.response().end(jsonObjectRxMessage.body().encodePrettily());                     }                 });             }         });   Still a big piece of code, but most are comments and caused by the fact that Java doesn’t support closures (yet). So what happens here?      We first create an observer from the request ``` reqDataObservable = RxSupport.toObservable(req)``` . This means that we want to be informed when data is available in the buffer of our request.  Since we want to do something with this data, we use the ``` reqDataObservable.flatMap```  function. This allows us to specify what happens when some data comes available on the previously created observable. So instead of nesting callbacks, we just specify the flow of data through the various asynchronous calls. When data is received we use that to update the database. Note that we use the ``` rxEventBus.send```  method. This doesn't make the call yet, but once again returns an observable.  As the third step we use the output from the previous one to (possibly) determine whether the update was successful. We then get the latest version from the database. This is once again done using observables.  As long as we don't have a subscriber nothing really happens. Since we're interested in the result from the final asynchronous action we use the ``` getLatestObservable.subscribe```  function and 'wait' (it is still non-blocking) for the result from the final database read. Once that is received we send the response back based on the received message.   In this example we’ve only showed a couple of very small parts of rxjava:      We used observables to make working and sequencing asynchronous actions easier.   We use the flatmap function to pass the result from one asynchronous call into the other, and so eliminating nested callbacks   We used the rxSupport and rxEventbus rxJava vert.x extensions to easily create rxJava observables   And we kicked off the complete sequence by subscribing to the last observable in the chain   In a future article we’ll dive a bit deeper into rxJava how you can also organize more complex asynchronous flows.  ","categories": ["posts","rxjava","rest","mongodb","vert.x"],
        "tags": [],
        "url": "http://www.smartjava.org/content/create-simpe-restful-service-vertx-20-rxjava-and-mongodb/",
        "teaser":null},{
        "title": "All 109 Examples from my book on Three.js for Three.js version r63",
        "excerpt":"A couple of months ago my book on Three.js was published by packt (which you can buy from here: Learning Three.js: The JavaScript 3D Library for WebGL). All the examples from that book were made for the version of Three.js which at that moment was the latest, which was r60. About a week ago Three.js r63 was released. No big changes this time, but there we’re still some minor changes which caused a couple of examples to stop working. So I created a seperate branch on the learning three.js github repo, where you can access all the examples updated and tested with version r63 of Three.js.   You can find the complete repo for r63 here: https://github.com/josdirksen/learning-threejs/tree/threejs_r63 If you’re still on r60 you can find that in the main branch: https://github.com/josdirksen/learning-threejs   If you don’t want to clone the repo you can also look at the examples online:   Chapter 1: Creating Your First 3D Scene with Three.js   Example 01.01 - Basic skeleton Example 01.02 - First Scene Example 01.03 - Materials and light Example 01.04 - Materials, light and animation Example 01.05 - Control gui Example 01.06 - Ascii renderer  Chapter 2: Working with the Basic Components That Make Up a Three.js Scene   Example 02.01 - Basic Scene Example 02.02 - Foggy Scene Example 02.03 - Override Material Example 02.04 - Geometries Example 02.05 - Custom geometry Example 02.06 - Mesh Properties Example 02.07 - Cameras Example 02.08 - Cameras  Chapter 3: Working with the Different Light Sources Available in Three.js   Example 03.01 - Ambient Light Example 03.02 - point Light Example 03.03 - Spot Light Example 03.04 - Directional Light Example 03.05 - Directional Light Example 03.06 - Area Light Example 03.07 - Lensflarest  Chapter 4: Working with the Three.js Materials   Example 04.01 - MeshBasicMaterial Example 04.02 - Depth Material Example 04.03 - Combined Material Example 04.04 - Mesh normal material Example 04.05 - Mesh face material Example 04.06 - Mesh Lambert material Example 04.07 - Mesh Phong material Example 04.08 - Shader material - http://glsl.heroku.com/ Example 04.09 - Linematerial Example 04.10 - Linematerial Dashed  Chapter 5: Learning to Work with Geometries   Example 05.01 - Basic 2D geometries - Plane Example 05.02 - Basic 2D geometries - Circle Example 05.03 - Basic 2D geometries - Shape Example 05.04 - Basic 2D geometries - Cube Example 05.05 - Basic 3D geometries - Sphere Example 05.07 - Basic 3D geometries - Cylinder Example 05.08 - Basic 3D geometries - Torus Example 05.08 - Basic 3D geometries - Torusknot Example 05.09 - Basic 3D geometries - Polyhedron  Chapter 6: Using Advanced Geometries and Binary Operations   Example 06.01 - Advanced 3D geometries - Convex Hull Example 06.02 - Advanced 3D geometries - Lathe Example 06.03 - Extrude Geometry Example 06.04 - Extrude TubeGeometry Example 06.05 - Extrude SVG Example 06.06 - Parametric geometries Example 06.07 - Text geometry Example 06.08 - Binary operations  Chapter 7: Particles and the Particle System   Example 07.01 - Particles - Only works in CanvasRenderer Example 07.02 - Particles - Only works in WebGLRenderer Example 07.03 - Particle Basic Material Example 07.04 - Particles - Canvas based texture Example 07.05 - Particles - Canvas based texture - WebGL Example 07.06 - Particles - Rainy scene Example 07.07 - Particles - Snowy scene Example 07.08 - Particles - Sprites Example 07.09 - Sprites in 3D Example 07.10 - 3D Torusknot  Chapter 8: Creating and Loading Advanced Meshes and Geometries   Example 08.01 - Grouping Example 08.02 - Merge objects Example 08.03 - Save &amp; Load Example 08.04 - Load and save scene Example 08.05 - Load blender model Example 08.06 - Load OBJ model Example 08.07 - Load OBJ and MTL Example 08.08 - Load collada model Example 08.09 - Load stl model Example 08.10 - Load pdb model Example 08.11 - Load pdb model Example 08.12 - Load pdb model Example 08.13 - Load ply model Example 07.13 - Load ply model  Chapter 9: Animations and Moving the Camera   Example 09.01 - Basic animations Example 09.02 - Selecting objects Example 09.03 - Animation tween Example 09.04 - Trackball controls Example 09.05 - Fly controls Example 09.06 - Roll controls Example 09.07 - first person camera Example 09.08 - Orbit controls Example 09.09 - Path Controls Example 09.10 - Working with morph targets Example 09.11 - Manual morph targets Example 09.12 - Load blender model Example 09.13 - Animation from blender Example 09.14 - Animation from collada Example 09.15 - animation from md2  Chapter 10: Loading and Working with Textures   Example 10.01 - Basic textures Example 10.02 - Bump maps Example 10.03 - Normal maps Example 10.04 - LightMap Example 10.05 - Env map Example 10.06 - Specular map Example 10.07 - UV mapping Example 10.08 - Repeat mapping Example 10.09 - Canvas texture Example 10.10 - Canvas texture bumpmap Example 10.11 - Video texture  Chapter 11: Custom Shaders and Render Post Processing   Example 11.01 - Effect composings Example 11.02 - Simple passes Example 11.03 - Post processing masks Example 11.04 - Shader Pass simple Example 11.04 - Shader Pass simple Example 11.06 - Advanced Example 11.07 - custom shaderpass  Chapter 12: Adding Physics to Your Scene with Physijs   Rigid body - Physijs Rigid body - Physijs Physijs shapes Physijs Constraints Physijs Constraints  ","categories": ["posts","webgl","javascript","three","three.js"],
        "tags": [],
        "url": "http://www.smartjava.org/content/all-109-examples-my-book-threejs-threejs-version-r63/",
        "teaser":null},{
        "title": "Capture Canvas and WebGL output as video using websockets",
        "excerpt":"About a month ago I got the request to create a video from one of the WebGL demos on this site (the rotating earth cube, showing population density). So, of course, I said yes, but creating high quality output of this animation wasn’t as easy as I thought. I looked at a couple of screen recording tools, but none were available for Mac that were free, offered a good resolution and high enough frame rate. To be honest, I didn’t really look that long since I thought it a good opportunity to see how I could get the content of just the single canvas/webgl element and export it to a movie.   So basically, after some looking around, I came across this link where someone used websockets to export the content of the canvas and save these captures individually. Finally at the server side we can use ffmpeg to convert these captures to a movie. This looked like an interesting approach, so I did pretty much the same thing. I, however, used a simple vert.x backend for the websocket server.   In this short article I’ll walk you through the required steps and code, so you can easily set this up yourself.   Creating the client javascript code   The first thing we’ll look at is the required code on the client side. The first thing we need to do here is set up a websocket:    var ws = new WebSocket(\"ws://localhost:8889/\");     var frame = 0;     var isOpen = false;     ws.onopen = function(e) {         console.log('opening');         isOpen = true;     };     ws.onclose = function(e) {         console.log('closing');         console.log(e);         isOpen = false;     };     ws.onerror = function(e) {         console.log('error');         console.log(e);     }   Note that not all the functions need to be defined, it’s just easier to spot issues when thing to wrong. Basically what this piece of code does, is that it opens up a websocket connection to “ws://localhost:8889” where our websocket server will be listening. If the connection is successful we set the isOpen to true. Note that we also define a “frame” variable here. This variable is used to keep track of the number of frame we sent and is used on the server side to ensure a correct sequence in the final movie. This piece of code goes at the top of our file and is executed before the rest of the javascript.   When you create animations, you usually have a render loop. The render loop for this example looked like this:      // render the scene     function render() {         var timer = Date.now() * 0.0001;            camera.position.x = (Math.cos( timer ) *  1800);         camera.position.z = (Math.sin( timer ) *  1800) ;         camera.lookAt( scene.position );            light.position = camera.position;         light.lookAt(scene.position);             renderer.render( scene, camera );        requestAnimationFrame( render );     }   Nothing to special. We just position the camera, move some lights around and render the scene. Furthermore we use requestAnimationFrame here to let the browser keep track of when to render the next frame. Even though we shouldn’t do too much in this rendering loop, I’ve added to code to send the data to the websocket server in this function. A better approach would have been by using webworkers to send the frame asynchronously, but this approach worked well enough.       function render() {                  renderer.render( scene, camera );         sendToServer();          requestAnimationFrame( render );      }     function sendToServer() {         var asString = renderer.domElement.toDataURL();           if (isOpen) {              frame++;               ws.send(str2ab(frame+asString));          }     }      function str2ab(str) {         var buf = new ArrayBuffer(str.length);         var bufView = new Uint8Array(buf);         for (var i=0, strLen=str.length; i&lt;strLen; i++) {             bufView[i] = str.charCodeAt(i);         }         return buf;     }   This piece of code uses the toDataURL() function to get the data from the canvas element, next it converts it to a bytearray so we can easily send it as a binary websockets message. Making it binary is a bit of overkill for this example I guess, but this was the approach I had the best result with. And this is it for the websockets client part. One last important thing to note here, is that you should make sure you cal the “sendToServer” after the canvas is rendered. This might seem obvious, but costed me a lot of time bug hunting. I assumed that it wouldn’t matter if I added it before or after, since there is always something rendered on the canvas, and thus available to be sent to the server. Well… this isn’t the case. The canvas is apparently cleared at the beginning of the render loop, which caused a lot of black screens to be sent to the server.   Setup the websocket server   You can use any websocket server you want and for this example I’ve use the vert.x approach, since I’ve been playing around more and more with this great asynchronous server framework. Lets look directly at the (very easy and straightforward) code:            HttpServer server2 = vertx.createHttpServer().websocketHandler(new Handler&lt;ServerWebSocket&gt;() {             @Override             public void handle(ServerWebSocket ws) {                 ws.dataHandler(new Handler&lt;Buffer&gt;() {                      @Override                     public void handle(Buffer event) {                         if (event.length() &gt; 100) {                             byte[] bytes = event.getBytes(0,event.length());                              String frame = new String(bytes);                             int frameNr = Integer.parseInt(frame.substring(0, frame.indexOf(\"data:\")));                             String frameData = frame.substring(frame.indexOf(\"base64,\")+7);                             BASE64Decoder decoder = new BASE64Decoder();                             try {                                 byte[] imageByte = decoder.decodeBuffer(frameData);                                 File f = new File(\"pngout/\" + String.format(\"%08d\", frameNr) + \"-frame.png\" );                                 FileOutputStream fOut = new FileOutputStream(f);                                 fOut.write(imageByte);                                 fOut.close();                             } catch (IOException e) {                                 e.printStackTrace();                               }                          }                     }                 });             }         });          server2.setMaxWebSocketFrameSize(512000);         server2.listen(8889);   Pretty easy to follow. The handle method is called on each event that is received. Since I’m not interested in handshake events I only check for event where we receive at least 100 bytes of data (there are other better ways, but I went for a quick solution :). In the handle method I splt the incoming data to get the frame I’m working with and the actual base64 encoded data. From this data I create a bytearray and store it in a file. This is done for all the frames that are received this way.   One thing to note here is the setMaxWebSockerFrameSize function (which is available since vert.x 2.1M2). This sets the maximum amount of data this server can receive in a single websocket message. Since we’re sending over rather large captures we need to increase this. If you don’t do this you get all kinds of strange error messages at the client side.   And the result   Now you can use an ffmpeg command like this:    \"ffmpeg -r 60 -i %08d-frame.png -vcodec libx264 -vpre lossless_slow -threads 0 output.mp4\"   To create the output movie from the individual frames. One of the first intermediate result (ignore the crappy compression from youtube) looks like this:     And that’s it.  ","categories": ["posts","three.js","webgl","html5","websockets","vert.x"],
        "tags": [],
        "url": "http://www.smartjava.org/content/capture-canvas-and-webgl-output-video-using-websockets/",
        "teaser":null},{
        "title": "Win a Free Copy of Packt's Learning Three.js: The JavaScript 3D Library for WebGL",
        "excerpt":"We are pleased to announce that we are organizing a giveaway especially for you. All you need to do is just comment below the post and win a free copy of Learning Three.js: The JavaScript 3D Library for WebGL. Two lucky winners stand a chance to win a copy of the book.         Keep reading to find out how you can be one of the Lucky One.   Overview of Learning Three.js: The JavaScript 3D Library for WebGL      Create and animate beautiful 3D graphics directly in the browser using JavaScript without the need to learn WebGL   Learn how to enhance your 3D graphics with light sources, shadows, and advanced materials and textures   Each subject is explained using extensive examples that you can directly use and adapt for your own purposes   How to Enter?   Simply post your expectations from this book in comments section below. You could be one of the 2 lucky participants to win the copy.   DeadLine: The contest will close on the 19th of February 2014. Winners will be contacted by email, so be sure to use your real email address when you comment! Please note: Winners residing only in the USA and Europe would get a chance to win print copies. Others would be provided with eBook copies.  ","categories": ["posts","webgl","three.js"],
        "tags": [],
        "url": "http://www.smartjava.org/content/win-free-copy-packts-learning-threejs-javascript-3d-library-webgl/",
        "teaser":null},{
        "title": "Browser to browser communication with Vert.x, websockets and HTML5",
        "excerpt":"I’ve been playing around with Vert.x for a couple of weeks and I’m really starting to like the simplicity of the framework. With very little configuration you can quickly get impressive results. In this article I’m diving a bit deeper in how to set up browser to browser communication using the Vert.x eventbus by using the javascript client side API. In short I’m going to show you the following things:      Use Vert.x as a simple web server   Configure vert.x on the serverside to act as bridge for the browser eventbus implementation   Create a simple example that shows how to use the eventbus from javascript   I’m going to skip the explanation on how to set up Vert.x. If you want more information on that look at this article where I explain the setup I’ve used in this example. So let’s get started with the first step.   Use Vert.x as a simple web server   This step isn’t really necessary, but I wanted to see how easy it was to host the example html and javascript files directly from vert.x. So instead of running them directly from the filesystem or from a local webserver, I created a very basic handler that serves simple files (with very little functionality). Doing this was surprisingly easy:           // very simple routematcher, that matches a single path, simulates minimal webserver         matcher.get(\"/web/:req\", new Handler&lt;HttpServerRequest&gt;() {              @Override             public void handle(HttpServerRequest req) {                 MultiMap params = req.params();                 if (params.contains(\"req\")) {                     File f = new File(\"src/main/resources/web/\" + params.get(\"req\"));                     try {                         // get the data from the filesystem and output to response                         String data = FileUtils.readFileToString(f);                         req.response().setStatusCode(200);                         req.response().putHeader(\"Content-Length\",Integer.toString(data.length()));                         req.response().write(data);                         req.response().end();                     } catch (IOException e) {                         // assume file not found, so send 404                         req.response().setStatusCode(404);                         req.response().end();                     }                 }             }         });   As you can see I added an additional matcher that matches everything inside the “/web” directory. When we receive a request we just get the file content as a string and return it. If something goes wrong, we just return a 404. Note that I’ve also could have just passed in the string to the req.response.end() method. With this small setup I just add the required HTML and JS files to the src/main/resources/web/ directory and I can access them through my browser. Easy? Yes. Very useful and functional? Hmm.. don’t know yet. The next step is to configure the server side part of the eventbus so the javascript client can talk to the Vert.x eventbus.   Configure vert.x on the serverside to act as bridge for the browser eventbus implementation   Before we can connect a javascript frontend to the Vert.x eventbus, we need one additional server side component. This component will act as a bridge between the javascript eventbus implementation and the serverside one. The javascript client uses Sock.js so you have to create the following:           // set in and outbound permitted addresses         JsonObject config = new JsonObject().putString(\"prefix\", \"/bridge\");         JsonArray inboundPermitted = new JsonArray();         inboundPermitted.add(new JsonObject().putString(\"address\", \"msg.client\"));          JsonArray outboundPermitted = new JsonArray();         outboundPermitted.add(new JsonObject().putString(\"address\", \"msg.server\"));         outboundPermitted.add(new JsonObject().putString(\"address\", \"msg.client\"));          vertx.createSockJSServer(server).bridge(config, inboundPermitted, outboundPermitted);   With the last line of this code fragment we create the bridge. This method takes three parameters. The first one defines the endpoint (url) the bridge will listen on. We need to remember this for when we write the client code. The next two parameters define on which eventbus addresses browser clients can send events, and the second one defines on which address the browser can receive events. So in this case:      msg.server: only outbound messages are allowed. So in other words, browser clients can't send events to this bus address, but can listen to them. We will use this address to send events, periodically, from the server to the client.   msg.client: both inbound and outbound message are allowed. So browser clients can receive and sent messages to this address. This address will be used for the browser clients to communicate with each other. Since all message travel through the central distributed event bus, we need to enable incoming and outgoing messages   Now all we need to do is connect the clients to the eventbus and create a simple HTML page we can use to communicate.   Create a simple example that shows how to use the eventbus from javascript   The first thing we need to do is include the correct javascript libraries.    &lt;script src=\"sockjs-0.3.4.min.js\"&gt;&lt;/script&gt; &lt;script src=\"vertxbus-2.1.js\"&gt;&lt;/script&gt;   No, once the page is loaded, we set up the eventbus listeners.       window.onload = init;      function init() {          // get the elements         var inputArea = document.getElementById('in');         var outputArea = document.getElementById('out');         var serverArea = document.getElementById('outserver');         var sendButton = document.getElementById('send');         var broadcastButton = document.getElementById('broadcast');          // get the eventbus         var eb = new vertx.EventBus('http://localhost:8888/bridge');          // when the eventbus is ready, register a listener         eb.onopen = function() {              // register to address             eb.registerHandler('msg.client', function(message) {                outputArea.value+=JSON.stringify(message)+'\\n';             });              // and register for server events             eb.registerHandler('msg.server', function(message) {                 serverArea.value+=JSON.stringify(message)+'\\n';             });         }          sendButton.onclick = function() {             eb.send(\"msg.client\",{send:inputArea.value});             outputArea.scrollTop = outputArea.scrollHeight         };          broadcastButton.onclick = function() {            eb.publish(\"msg.client\",{publish:inputArea.value})             outputArea.scrollTop = outputArea.scrollHeight         }     }   Not such complex code. What we do is we make a connection to the bridge by creating a new eventbus. We work with the bus in the same way as we do from the server side. I’ve created a simple UI so I can easily test whether stuff works:     &lt;div id=\"input\" class=\"main\"&gt;     &lt;form&gt; &lt;textarea id=\"in\" rows=\"5\" cols=\"80\"&gt;&lt;/textarea&gt;      &lt;div style=\"display:block\"&gt;          &lt;input type=\"button\" id=\"send\" value=\"send\" /&gt;          &lt;input type=\"button\" id=\"broadcast\" value=\"broadcast\" /&gt;      &lt;/div&gt;      &lt;/form&gt; &lt;/div&gt; &lt;div id=\"output\" class=\"main\"&gt; &lt;textarea id=\"out\" rows=\"5\" cols=\"80\" readonly&gt;&lt;/textarea&gt; &lt;/div&gt; &lt;div id=\"fromserver\" class=\"main\"&gt;     &lt;textarea id=\"outserver\" rows=\"5\" cols=\"80\" readonly&gt;&lt;/textarea&gt; &lt;/div&gt;   Which looks like this:      The bottom textarea displays events received from the server. We haven’t, however, set up the server to push messages at a regular interval. Luckily vert.x provides an easy to use mechanism for that.            long timerID = vertx.setPeriodic(1000, new Handler&lt;Long&gt;() {             public void handle(Long timerID) {                 vertx.eventBus().send(\"msg.server\",new JsonObject().putString(\"msg\", \"Timestamp at server: \" + System.currentTimeMillis()));             }         });   Easy right! The cool thing is that without much server side code you can easily create javascript applications that communicate with each other. The only thing you need to do is create the sockJS bridge and define the addresses that are allowed for inbound and outbound communication.  ","categories": ["posts","websockets","html5","vert.x"],
        "tags": [],
        "url": "http://www.smartjava.org/content/browser-browser-communication-vertx-websockets-and-html5/",
        "teaser":null},{
        "title": "Create global precipitation (rain) visualizations with HTML5, Canvas, and open data",
        "excerpt":"I’m currently working on my next book for Three.js and one of the chapters deals with visualizing open data. When looking around for data that I could use, I ran across a set of data from the NOAA. Through this site you can download a set of monthly precipiation reports for the whole world in a gridded format. So I downloaded them, and started playing around with the data to see how it looks and how it could be used. In this article I’m not going to show you the three.js based result, but I’ll give you a quick overview how to get to the format I initially used for debugging purposes:      In this image you can see the monthly precipitation for the whole world, on a logarithmic scale for July 2012. I’ve also created a simple site that shows this, and the animation , in action.   So, what do you need to do to convert the set you can download from the NOAA site to something visually.      Download and convert the NetCDF format.   Load in the resulting CSV file   Process the CSV data into a world grid   Animate the transitions between the months   As a bonus: also create a legend to show what color means what   First, though, we need to get the data.   Download and convert the NetCDF format   The first thing we need to do is get the data. I’ve used the following link:  where you can define the range of data you want to download. For this example I’ve used the range from January 2012 to December 2012 and selected the option to create a subset without making a plot.   The format in which it is downloaded, however, isn’t directly usable as input to our HTML5 canvas based visualization. You can use ncdump-json to create a JSON file, but you still need to be able to interpret it, so I choose the alternative way. I just wrote a simple java program to convert the NetCDF format to a simple CSV file.   I’ve used the following maven dependencies:       &lt;dependencies&gt;         &lt;dependency&gt;             &lt;groupId&gt;edu.ucar&lt;/groupId&gt;             &lt;artifactId&gt;netcdf&lt;/artifactId&gt;             &lt;version&gt;4.2.20&lt;/version&gt;         &lt;/dependency&gt;          &lt;dependency&gt;             &lt;groupId&gt;commons-io&lt;/groupId&gt;             &lt;artifactId&gt;commons-io&lt;/artifactId&gt;             &lt;version&gt;2.4&lt;/version&gt;         &lt;/dependency&gt;     &lt;/dependencies&gt;   And use the following piece of java code:   public class NetCDFDump {      public static void main(String[] args) throws IOException, InvalidRangeException {          String year = \"2012\";         NetcdfFile nc = NetcdfFile.open(\"src/main/resources/X84.31.143.145.44.1.47.49.nc\");          Variable precip = nc.findVariable(\"precip\");          // use the shapes to create an array         int[] shapes = precip.getShape();         // month, lat, lon         float[][][] data = new float[shapes[0]][shapes[1]][shapes[2]];          // iterate over 12 (or 11) months         int[] pos = new int[3];         int[] shape = {1,1,1};         for (int i = 0 ; i &lt; shapes[0] ; i++) {             pos[0]=i;             for (int lat = 0 ; lat &lt; shapes[1]; lat++) {                 pos[1] = lat;                 for (int lon = 0 ; lon &lt; shapes[2]; lon++) {                     pos[2] = lon;                     Array result = precip.read(pos, shape);                     data[pos[0]][pos[1]][pos[2]] = result.getFloat(0);                 }             }         }          // output data like this         // month, lat, lon, humidity         float[][] combined = new float[data[0].length][data[0][0].length];          for (int m = 0 ; m &lt; data.length ; m++) {             File outputM = new File(year + \"-out-\" + m + \".csv\");             for (int lat = 0 ; lat &lt; data[m].length ; lat++) {                 for (int lon = 0 ; lon &lt; data[m][lat].length; lon++) {                     float value = data[m][lat][lon];                     if (value &gt; -1000) {                         combined[lat][lon]+=value;                     } else {                         combined[lat][lon]+=-1000;                     }                      // write the string for outputfile                     StringBuffer bOut = new StringBuffer();                     bOut.append(m);                     bOut.append(',');                     bOut.append(lat);                     bOut.append(',');                     bOut.append(lon);                     bOut.append(',');                     bOut.append(value);                     bOut.append('\\n');                      // write to month file                     FileUtils.write(outputM,bOut,true);                 }             }         }          // now process the combined         File outputM = new File(year + \"-gem.csv\");         for (int i = 0; i &lt; combined.length; i++) {             for (int j = 0; j &lt; combined[0].length; j++) {                 StringBuffer bOut = new StringBuffer();                 bOut.append(i);                 bOut.append(',');                 bOut.append(j);                 bOut.append(',');                 bOut.append(combined[i][j]/data.length);                 bOut.append('\\n');                  FileUtils.write(outputM, bOut, true);             }         }     } }   I won’t go into too much detail what’s happening, but this piece of code results in a number of files, one for each month, and one containing the average.   Each month is shown in the following format   ... 0,65,78,32.65 0,65,79,35.09 0,65,80,31.14 0,65,81,42.7 0,65,82,49.57 ...   Respectively the values mean: the month, latitude, longitude and precipitation. For the average it looks almost the same, except the first entry is omitted.    ... 59,94,59.874165 59,95,65.954994 59,96,57.805836 ...   Now that we’ve got the data in an easy to use format, we can use it to create the visualizations.   Load in the resulting CSV file   To load the file we just use a simple XMLHttpRequest like this:          // create an XMLHttpRequest to get the data        var xmlhttp = new XMLHttpRequest();         xmlhttp.onreadystatechange = function() {             if (xmlhttp.readyState == 4 &amp;&amp; xmlhttp.status == 200) {                 var coords = CSVToArray(xmlhttp.responseText,\",\");                 // and process each of the coordinates                 ...             }         }          // make the call and use the callback to process the result         xmlhttp.open(\"GET\", \"location/of/the/file\", true);         xmlhttp.send();   The coords variable now contains all the coordinates, and for each coordinate the value to show. Converting this to a canvas is actually very easy.   Process the CSV data into a world grid   In the callback from the XMLHttpRequest we check whether we’ve received the data and convert it to a set of coordinates. The only thing we need to do is convert these coordinates to a visualization on the canvas.   var coords = CSVToArray(xmlhttp.responseText,\",\");                 coords.forEach(function(point) {                      var offset = 0;                     if (point.length &gt; 3) {                         offset = 1;                     }                      if (parseFloat(point[2+offset]) &gt;= 0) {                         var lat = parseInt(point[0+offset]);                         var lon = parseInt(point[1+offset]);                         var value = parseFloat(point[2+offset]);                          if (value &gt; max) max = value;                          // lat is from 0 to 180                         // lon is from 0 to 360                         var x = canvas.width/360*((lon)-180);                         if (x&lt;=0) {                             x=canvas.width-(x*-1);                         }                         var y = canvas.height/180*lat;                          if (value &gt;= 0) {                             context.beginPath();                             context.rect(x,y,4,4);                             context.fillStyle = scale(value).hex();                             context.fill();                          }                     }                 });   As you can see, very simple code where we just take the positions, convert them to an X and Y coordinate on the canvas, and create a small square with a specific color. For the generation of the color we use a Chroma.js scale.   var scale = chroma.scale(['red' , 'yellow', 'green', 'blue']).domain([1,1700], 100, 'log');   This call creates a color scale from red to yellow to green to blue. The values range from 1 to 1700, are divided into 100 steps, and use a logarithmic scale. This results in the following image (this time for the precipitation in January 2012:      Since we’ve got figures for all the months, we can now easily create a simple animation.   Animate the transitions between the months   For animations we’re going to create something like shown in the following movie, where we transition slowly between the various months:     Creating this animation can be done rather easy by just showing the images on top of each other, and changing the opacity. So first set up some css, which hides most of the images and puts them all one on top of each other.            #cf {             position:relative;             margin:0 auto;             height: 700px;         }          #cf img {             position:absolute;             left:0;             width: 1600px;         }   Now we can just add the images and using the ‘bottom’ class to only show the first image:    &lt;div id=\"cf\"&gt;     &lt;img id=\"img-1\" class=\"top\" src=\"./assets/images/2012-01-perc.png\" /&gt;     &lt;img id=\"img-2\" class=\"bottom\" src=\"./assets/images/2012-02-perc.png\" /&gt;     &lt;img id=\"img-3\" class=\"bottom\" src=\"./assets/images/2012-03-perc.png\" /&gt;     &lt;img id=\"img-4\" class=\"bottom\" src=\"./assets/images/2012-04-perc.png\" /&gt;     &lt;img id=\"img-5\" class=\"bottom\" src=\"./assets/images/2012-05-perc.png\" /&gt;     &lt;img id=\"img-6\" class=\"bottom\" src=\"./assets/images/2012-06-perc.png\" /&gt;     &lt;img id=\"img-7\" class=\"bottom\" src=\"./assets/images/2012-07-perc.png\" /&gt;     &lt;img id=\"img-8\" class=\"bottom\" src=\"./assets/images/2012-08-perc.png\" /&gt;     &lt;img id=\"img-9\" class=\"bottom\" src=\"./assets/images/2012-09-perc.png\" /&gt;     &lt;img id=\"img-10\" class=\"bottom\" src=\"./assets/images/2012-10-perc.png\" /&gt;     &lt;img id=\"img-11\" class=\"bottom\" src=\"./assets/images/2012-11-perc.png\" /&gt;     &lt;img id=\"img-12\" class=\"bottom\" src=\"./assets/images/2012-12-perc.png\" /&gt; &lt;/div&gt;   Now we just need some javascript to tie everything together:       var month=[];     month[0]=\"January\";     month[1]=\"February\";     month[2]=\"March\";     month[3]=\"April\";     month[4]=\"May\";     month[5]=\"June\";     month[6]=\"July\";     month[7]=\"August\";     month[8]=\"September\";     month[9]=\"October\";     month[10]=\"November\";     month[11]=\"December\";      var allTweens;      init();     animate();      function init() {         // create a chain of tweens         allTweens = setupTweens(12);         allTweens[0].start();     }      function setupTweens(imageCount) {         var tweens = [];         for (var i = 0 ; i &lt; imageCount ; i++) {             var tween = new TWEEN.Tween( { opac: 0, image: i, max: imageCount } )                     .to( { opac: 100 }, 2500 )                     .easing( TWEEN.Easing.Linear.None )                     .onUpdate( function () {                         // on update, lower the opacity of image i and update the opacity of                         // image i+1;                         var currentImage = document.getElementById('img-'+(this.image+1));                          if (this.image == imageCount -1) {                             var nextImage = document.getElementById('img-'+1);                         } else {                             var nextImage = document.getElementById('img-'+(this.image+2));                         }                          currentImage.style.opacity = 1- this.opac / 100;                         nextImage.style.opacity = this.opac / 100;                     } );              tween.onComplete(function() {                  document.getElementById('title-2012').textContent = \"Showing precipitation: \" + month[this.image] + \" \" + 2012;                 // Set the inner variable to 0.                 this.opac = 0;                 // we're done, restart                 if (this.max-1 == this.image) {                     allTweens[0].start();                 }              });             // connect to each another             if (i &gt; 0) {                 tweens[i-1].chain(tween);             }              tweens.push(tween);             tweens[0].repeat();         }          return tweens;     }      function animate() {         requestAnimationFrame(animate);         TWEEN.update();     }   Here we use tween.js to setup the transitions between the images.   As a bonus: also create a legend to show what color means what   In the animation you can see a legend at the bottom. This legend was created as a simple canvas, saved as a image. For completeness sake, the code to do this is shown here:       var canvas = document.createElement(\"canvas\");     canvas.width = 435;     canvas.height = 30;     var context = canvas.getContext('2d');      var domains = scale.domain();     document.body.appendChild(canvas);      // from 1 to 1700     for (var i = 0 ; i &lt; domains.length ; i++) {         context.beginPath();         context.rect(10+i*4,0,4,20);         console.log(domains[i]);         context.fillStyle = scale(domains[i]).hex();         context.fill();     }      context.fillStyle = 'black';     context.fillText(\"0 mm\", 0, 30);     context.fillText(Math.round(domains[25]) + \" mm\", 100, 30);     context.fillText(Math.round(domains[50]) + \" mm\", 200, 30);     context.fillText(Math.round(domains[75]) + \" mm\", 300, 30);     context.fillText(\"1700 mm\", 390, 30);    Here we just use the scale we’ve seen easier and walk through the domains to create a colored legend.  ","categories": ["posts","canvas","open data","html5"],
        "tags": [],
        "url": "http://www.smartjava.org/content/create-global-precipitation-rain-visualizations-html5-canvas-and-open-data/",
        "teaser":null},{
        "title": "Widgets and dashboard with Atlasboard, Node.js and d3.js",
        "excerpt":"So it’s been a while since I added something to this blog. I’ve been very busy with work and at the same time finishing up my second book on Three.js. For our company we’re looking for a new and flexible way to create dashboards. We want to use these kinds of dashboard to monitor the various development teams, provide a complete realtime overview for the IT manager and even try to monitor our complete devops process. A colleague of mine mentioned Atlasboard from atlassian. A really simple and straightforward dashboard with a very modular architecture.  In this article I’ll give a quick overview on how easy it is to create your own widgets and jobs.   Installing Atlasboard   Atlasboard uses node.js as a container. So to install Atlasboard, first make sure you’ve got node.js and npm installed. Once installed you can use npm to install Atlasboard:    npm install -g atlasboard   Note that when you do this on windows you might run into some issues starting Atlasboard. See the following thread here for the solution: https://bitbucket.org/atlassian/atlasboard/issue/61/cant-create-dashboard-on-windows   And that’s it. Now you can create a dashboard that runs on Atlasboard. Go to the directory where you want to create your dashboard and do the following:    atlasboard new mydashboard   This will create a directory called mydashboard. Move to this directory and start the atlasboard server:    cd mydashboard atlasboard start 3333   Now open your browser, point it to http://localhost:3333 and you’ll see your first dashboard:      Understanding Atlasboard   To understand atlasboard you need to understand it’s main components. The wiki from Atlasboard has some basic information on this, but its very easy to understand. The following figure from Atlasboard wiki explains pretty much everything:      Basically to work with Atlasboard you need to understand the following features:      Jobs: A job is a simple javascript file that is executed by node.js at a scheduled interval. With this job you can pull in information from various sources which can then be displayed by a widget. This can be any type of information. For instance github statistics, jenkins statistics, build results, sonar results, twitter feeds etc. Atlasboard doesn't use a database by default to keep track of the information retrieved by its jobs, but if you'd like you could easily ad mongoDB or something else.   Widgets: Consists out of a javascript file, an html template and a css file. When a job fires, it sends the information it retrieved to a widget, which can then display it. Atlasboard comes with a couple of standard widgets you can use, but, as you'll see in this article, creating custom widgets is very easy.    Dashboards: Defines the size of the individual widgets and their position on screen. In this file you also define which widget and job are tied together, the interval for each of the jobs, and provides you with a convenient way to configure widgets and jobs.     In the next section we’ll show you how to create your own custom widgets and jobs. The very simple goal is to create the following dashboard, which shows a gauge and a graph which both show the same information: the free memory on my system:      Setup the dashboard   First of, lets setup the dashboard file. In the directories created by Atlasboard, you’ll find a file called myfirst_dashboard.json. If you open this file you’ll see the configuration for all the example widgets and jobs for the demo dashboard. Open this file, and change it to this:   {   \"layout\": {       \"title\": false,       \"customJS\" : [\"jquery.peity.js\"],       \"widgets\" : [           {\"row\" : 1, \"col\" : 1, \"width\" : 4, \"height\" : 2, \"widget\" : \"freemem\",   \"job\" : \"freemem\",     \"config\": \"freemem-config\"},           {\"row\" : 2, \"col\" : 1, \"width\" : 4, \"height\" : 2, \"widget\" : \"gauge\",  \"job\" : \"freemem2\",     \"config\": \"gauge-config\"}       ]   },    \"config\" : {      \"gauge-config\" : {           \"interval\" : 1000     },      \"freemem-config\" : {         \"interval\" : 1000     }   } }   In the layout part of this file we define where the widgets are positioned on screen, which widget to use, widget job to use and which config to use. So when we look at the first line, you can see that we expect the freemem widget to use the freemem job and the freemem-config. This last part can directly be seen in the config part of this file (note there is config inheritance in atlasboard, but I’ll skipt that for now).   Create the jobs   So lets create the appropriate jobs and widgets. Atlasboard provides a command line for this. So do the following in your dashboard directory:    atlasboard generate widget freemem atlasboard generate widget gauge atlasboard generate job freemem atlasboard generate job freemem2   This will create the necessary files for the widgets and jobs in the packages/default directory. Lets start by looking at the job (freemem and freemem2 are the same). The following listing provides the complete content for the freemem.js job:   var os = require('os');  /**  * Job: freemem  *  * Expected configuration:  *   * { }  */   var data = []; var MAX_LENGTH = 100;  module.exports = function(config, dependencies, job_callback) {      // add the correct information to the data array     var length = data.push(os.freemem());     if (length &gt; MAX_LENGTH) {         data.shift();     }      job_callback(null, {         data: data,         total: os.totalmem()     }); };   This code uses the node.js os module to read the free and total memory of my system. By using the job_callback function we send two data elements to the widget at the front. The freemem2.js file is exactly the same. It seems that atlasboard can’t use the same job for the same widgets twice. Since I wanted to share the information I just created two jobs that look the same. Not the best solution, but the best I found so far :)   Create the widgets   Now all that is left to do is create the widgets. Lets first look at the graph. For the graph I used rickshaw which is included in atlasboard. Atlasboard also provides an easier to use interface to create graphs (https://bitbucket.org/atlassian/atlasboard-charts-package), but I like the direct Rickshaw approach better. The code for a widget is very simple:   freemem.css:   .content {     font-size: 35px;     color: #454545;     font-weight: bold;     text-align: center; }   freemem.html:   &lt;h2&gt;freemem&lt;/h2&gt; &lt;div class=\"content\"&gt;     &lt;div id=\"graph\"&gt;&lt;/div&gt;     &lt;div id=\"text\"&gt;&lt;/div&gt; &lt;/div&gt;   freemem.js:    widget = {     //runs when we receive data from the job     onData: function (el, data) {          //The parameters our job passed through are in the data object         //el is our widget element, so our actions should all be relative to that         if (data.title) {             $('h2', el).text(data.title);         }          var graphElement = document.querySelector(\"#graph\");         var textElement = document.querySelector(\"#text\");          while (graphElement.firstChild) {             graphElement.removeChild(graphElement.firstChild);         }          var dataArray = [];         var count = 0;         data.data.forEach(function(e){             dataArray.push({x: count++, y:e});          });          var graph = new Rickshaw.Graph({             element: document.querySelector(\"#graph\"),             height: 350,             renderer: 'area',             stroke: true,             series: [                 {                     data: dataArray,                     color: 'rgba(192,132,255,0.3)',                     stroke: 'rgba(0,0,0,0.15)'                 }             ]         });           graph.renderer.unstack = true;         graph.render();           $(textElement).html(\"\" + new Date());     } };   If you look through the code of freemem.js you can see that we don’t really do anything complex. We just parse the data we receive and use Rickshaw to draw the graph. Easy right?   If you look at the sourcecode for gauge it isn’t that much more complex. I’ve used the d3.js based gauge from here: http://tomerdoron.blogspot.co.il/2011/12/google-style-gauges-using-d3js.html   And changed to code so it’ll react to the updates from the job:   widget = {      gauges: [],      //runs when we receive data from the job     onData: function(el, data) {          //The parameters our job passed through are in the data object         //el is our widget element, so our actions should all be relative to that         if (data.title) {             $('h2', el).text(data.title);         }          var gaugeContainer = document.querySelector(\"#memoryGaugeContainer\");          // if no gauge is there yet, create one;         if (gaugeContainer.childNodes.length != 1) {             this.gauges['memory'] = createGauge(\"memory\", \"Memory\");         }          var freePercentage = 100-(data.data.pop()/(data.total/100));          var gauge = this.gauges['memory'];         gauge.redraw(freePercentage);     } };  function createGauge(name, label, min, max) ... }  function Gauge(placeholderName, configuration) {  ... }   the createGauge and Gauge object were taken from the previous link. I only implemented the widget code. Easy right :)   Conclusions   That’s it for this first article on Atlasboard. We’re seriously considering implementing this at work, so I’ll try to give an update in a couple of weeks. Overall I really like the approach Atlasboard takes and how easy it is to create new widgets and jobs.  ","categories": ["posts","node.js","d3.js"],
        "tags": [],
        "url": "http://www.smartjava.org/content/widgets-and-dashboard-atlasboard-nodejs-and-d3js/",
        "teaser":null},{
        "title": "Access information from GeoTIFF using Java",
        "excerpt":"I’m always very interested in combining information from various open data sources together. The last couple of weeks I’ve been working with GIS related information which is freely available within the Netherlands. To be more precise I’ve been experimenting with data from:      BAG: Is dutch for Basis Administratie Gebouwen, and contains information on all the building in the netherlands.   OSM: Openstreetmap provides user provisioned maps and additional information   I loaded this data into Postgis, so I could easily play around and experiment with it. Using this data its relatively easy to create maps like this:      My original goal was to use this information and create 3D models from them using Three.js. But, unfortunately, the height of buildings wasn’t provided by these data sources. Luckily though there was an additional source of information that could be used. Through the AHN (Actueel Hoogtebestand Nederland) you can download heightmaps from different parts of the Netherlands. One of the sources they provide is the raw unfiltered data as a raster file. This means that the building, bridges etc. are still in there and not filtered out. For instance, such a file looks like this:      This file contains coordinate information and for each coordinate the height is represented as a gray scale value. The information in this file is stored in a format called geoTIFF. So my idea was, to get the coordinate from the buildings in the database. Stored as normal GIS objects in postGIS, and correlate that with the height information from the geoTIFF file to determine the height. Probably won’t have the best accuracy, but should give a good enough value.   I first looked at directly loading this information in postGIS and using some queries to get the correct values, but apparently this only works from postGIS 2.0 and higher, and I didn’t feel like upgrading and reloading all the GIS data. So after some searching I ran into the org.geotools library which has a large amount of GIS related javatools. And one of them handles loading geoTIFF files for further processing. So I created a simple java program to load the data from postGIS, enrich it with information from the geoTIFF and update the database.   To get everything working I used the following maven pom.xml:   &lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\"          xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"          xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt;     &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;      &lt;groupId&gt;geotiff&lt;/groupId&gt;     &lt;artifactId&gt;geotiff&lt;/artifactId&gt;     &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;      &lt;dependencies&gt;         &lt;dependency&gt;             &lt;groupId&gt;org.geotools&lt;/groupId&gt;             &lt;artifactId&gt;gt-geotiff&lt;/artifactId&gt;             &lt;version&gt;12.0.1&lt;/version&gt;         &lt;/dependency&gt;         &lt;dependency&gt;             &lt;groupId&gt;org.geotools&lt;/groupId&gt;             &lt;artifactId&gt;gt-epsg-hsql&lt;/artifactId&gt;             &lt;version&gt;12.0.1&lt;/version&gt;         &lt;/dependency&gt;         &lt;dependency&gt;             &lt;groupId&gt;org.postgresql&lt;/groupId&gt;             &lt;artifactId&gt;postgresql&lt;/artifactId&gt;             &lt;version&gt;9.3-1102-jdbc41&lt;/version&gt;         &lt;/dependency&gt;         &lt;dependency&gt;             &lt;groupId&gt;commons-dbutils&lt;/groupId&gt;             &lt;artifactId&gt;commons-dbutils&lt;/artifactId&gt;             &lt;version&gt;1.6&lt;/version&gt;         &lt;/dependency&gt;     &lt;/dependencies&gt;      &lt;repositories&gt;         &lt;repository&gt;             &lt;id&gt;maven2-repository.dev.java.net&lt;/id&gt;             &lt;name&gt;Java.net repository&lt;/name&gt;             &lt;url&gt;http://download.java.net/maven/2&lt;/url&gt;         &lt;/repository&gt;         &lt;repository&gt;             &lt;id&gt;osgeo&lt;/id&gt;             &lt;name&gt;Open Source Geospatial Foundation Repository&lt;/name&gt;             &lt;url&gt;http://download.osgeo.org/webdav/geotools/&lt;/url&gt;         &lt;/repository&gt;         &lt;repository&gt;             &lt;snapshots&gt;                 &lt;enabled&gt;true&lt;/enabled&gt;             &lt;/snapshots&gt;             &lt;id&gt;boundless&lt;/id&gt;             &lt;name&gt;Boundless Maven Repository&lt;/name&gt;             &lt;url&gt;http://repo.boundlessgeo.com/main&lt;/url&gt;         &lt;/repository&gt;     &lt;/repositories&gt;      &lt;/project&gt;   With the libraries in place the actual code isn’t that complex. It takes some time experimenting with all the different objects and methods, since I couldn’t find any good docmentation. The following, though, is the complete java code which loads the geoTIFF and accesses the height information based on a location from postGIS. Note that in this case both projections are the same (SRID 28992), so I didn’t need to convert projections:   import org.apache.commons.dbutils.QueryRunner; import org.apache.commons.dbutils.ResultSetHandler; import org.geotools.coverage.grid.GridCoordinates2D; import org.geotools.coverage.grid.GridCoverage2D; import org.geotools.coverage.grid.GridGeometry2D; import org.geotools.gce.geotiff.GeoTiffReader; import org.geotools.geometry.DirectPosition2D;  import java.awt.image.Raster; import java.io.File; import java.sql.Connection; import java.sql.DriverManager; import java.sql.ResultSet; import java.sql.SQLException; import java.util.HashMap; import java.util.Map;  /**  * Created with IntelliJ IDEA.  * User: jos  * Date: 26/10/14  * Time: 19:26  * To change this template use File | Settings | File Templates.  */ public class GeoTIFFTest {      private final static String url = \"jdbc:postgresql://localhost/bag\";     private static GridCoverage2D grid;     private static Raster gridData;      public static void main(String[] args) throws Exception {         initTif();         loadData();     }      private static void initTif() throws Exception {         File tiffFile = new File(\"/Volumes/Iomega_HDD/mac/data/r44hn1.tif\");         GeoTiffReader reader = new GeoTiffReader(tiffFile);          grid =reader.read(null);         gridData = grid.getRenderedImage().getData();     }      private static double getValue(double x, double y) throws Exception {          GridGeometry2D gg = grid.getGridGeometry();          DirectPosition2D posWorld = new DirectPosition2D(x,y);         GridCoordinates2D posGrid = gg.worldToGrid(posWorld);          // envelope is the size in the target projection         double[] pixel=new double[1];         double[] data = gridData.getPixel(posGrid.x, posGrid.y, pixel);         return data[0];     }      private static void loadData() throws Exception {         Connection conn = DriverManager.getConnection(url);         QueryRunner runner = new QueryRunner();         final Map&lt;Long, Double&gt; map = new HashMap&lt;Long, Double&gt;();         ResultSetHandler handler = new ResultSetHandler() {              @Override             public Object handle(ResultSet resultSet) throws SQLException {                 while (resultSet.next()) {                     String point = resultSet.getString(\"point\");                     double x = Double.parseDouble(point.substring(                             point.indexOf('(') + 1,                             point.indexOf(' ')                     ));                      double y = Double.parseDouble(point.substring(                             point.indexOf(' ') + 1,                             point.indexOf(')')                     ));                      try {                         double hoogte = getValue(x, y);                         map.put(resultSet.getLong(\"gid\"),hoogte);                     } catch (Exception e) {                         e.printStackTrace();  //To change body of catch statement use File | Settings | File Templates.                     }                 }                 return null;  //To change body of implemented methods use File | Settings | File Templates.             }         };          runner.query(conn, \"SELECT gid, ST_AsText(ST_Centroid(geovlak)) as point \\n\" +                 \"FROM bag8mrt2014.pand\\n\" +                 \"WHERE geovlak &amp;&amp; ST_MakeEnvelope(130153, 408769,132896, 410774, 28992) ORDER by gid ;\", handler);          int count = 0;         for (Long key : map.keySet()) {              System.out.println(\"Inserting for key = \" + key + \" value: \" + map.get(key));             int col = runner.update(conn, \"UPDATE bag8mrt2014.pand SET hoogte= ? where gid = ?\",                     map.get(key), key);              count++;              if (count%100 == 0) {                 System.out.println(\"count = \" + count);             }         }     } }   So with the information updated I now also have height information in the database, so I can start looking at rendering this information in 3D. First experiments are pretty hopeful :)     ","categories": ["posts","open data","java","gis"],
        "tags": [],
        "url": "http://www.smartjava.org/content/access-information-geotiff-using-java/",
        "teaser":null},{
        "title": "First steps with REST, Spray and Scala",
        "excerpt":"On this site you can already find a couple of articles on how to do REST with a multiple of different frameworks. You can find an old one on Play, some on Scalatra and I even started an (as of yet unfinished) series on Express. So instead of finishing the series on Express, I’m going to look at Spray in this article.   Getting started  First thing we need to do is get the correct libraries set up, so we can start development (I use IntelliJ IDEA, but you can use whatever you want). The easiest way to get started is by using SBT. I’ve used the following minimal SBT file to get started.    organization  := \"org.smartjava\"  version       := \"0.1\"  scalaVersion  := \"2.11.2\"  scalacOptions := Seq(\"-unchecked\", \"-deprecation\", \"-encoding\", \"utf8\")  libraryDependencies ++= {   val akkaV = \"2.3.6\"   val sprayV = \"1.3.2\"   Seq(     \"io.spray\"            %%  \"spray-can\"     % sprayV withSources() withJavadoc(),     \"io.spray\"            %%  \"spray-routing\" % sprayV withSources() withJavadoc(),     \"io.spray\"            %%  \"spray-json\"    % \"1.3.1\",     \"io.spray\"            %%  \"spray-testkit\" % sprayV  % \"test\",     \"com.typesafe.akka\"   %%  \"akka-actor\"    % akkaV,     \"com.typesafe.akka\"   %%  \"akka-testkit\"  % akkaV   % \"test\",     \"org.specs2\"          %%  \"specs2-core\"   % \"2.3.11\" % \"test\",     \"org.scalaz\"          %%  \"scalaz-core\"   % \"7.1.0\"   ) }   After you’ve imported this file into your IDE of choice you should have the correct spray and akka libraries to get started.   Create a launcher   Next lets create a launcher which you can use to run our Spray server. For this we just an object, creativeally named Boot, which extends from the standard scala App trait.     package org.smartjava;  import akka.actor.{ActorSystem, Props} import akka.io.IO import spray.can.Http import akka.pattern.ask import akka.util.Timeout import scala.concurrent.duration._  object Boot extends App {    // create our actor system with the name smartjava   implicit val system = ActorSystem(\"smartjava\")   val service = system.actorOf(Props[SJServiceActor], \"sj-rest-service\")    // IO requires an implicit ActorSystem, and ? requires an implicit timeout   // Bind HTTP to the specified service.   implicit val timeout = Timeout(5.seconds)   IO(Http) ? Http.Bind(service, interface = \"localhost\", port = 8080) }   There isn’t happening that much in this object. What we do is we send a HTTP.Bind() message (better said we ‘ask’) to register a listener. If binding succeeds our service will receive messages whenever a request is received on the port.   Receiving actor   Now lets look at the actor where we’ll be sending the messages from the IO subsystem to.    package org.smartjava  import akka.actor.Actor import spray.routing._ import spray.http._ import MediaTypes._ import spray.httpx.SprayJsonSupport._ import MyJsonProtocol._  // simple actor that handles the routes. class SJServiceActor extends Actor with HttpService {    // required as implicit value for the HttpService   // included from SJService   def actorRefFactory = context    // we don't create a receive function ourselve, but use   // the runRoute function from the HttpService to create   // one for us, based on the supplied routes.   def receive = runRoute(aSimpleRoute ~ anotherRoute)    // some sample routes   val aSimpleRoute = {...}   val anotherRoute = {...}   So what happens here is that when we use the runRoute function, provided by HttpService, to create the receive function that handles the incoming messages.   creating routes  The final step we need to configure is create some route handling code. We’ll go into more detail for this part in one of the next articles, so for now we’ll show you how to create a route that based on the incoming media-type sends back some JSON. We’ll use the standard JSON support from Spray for this. As a JSON object we’ll use the following very basic case class which we extended with JSON support.    package org.smartjava  import spray.json.DefaultJsonProtocol  object MyJsonProtocol extends DefaultJsonProtocol {   implicit val personFormat = jsonFormat3(Person) }  case class Person(name: String, fistName: String, age: Long)   This way Spray will marshall this object to JSON when we set the correct response media-type. Now that we’ve got our response object lets look at the code for the routes:      // handles the api path, we could also define these in separate files   // this path respons to get queries, and make a selection on the   // media-type.   val aSimpleRoute = {     path(\"path1\") {       get {          // Get the value of the content-header. Spray         // provides multiple ways to do this.         headerValue({           case x@HttpHeaders.`Content-Type`(value) =&gt; Some(value)           case default =&gt; None         }) {           // the header is passed in containing the content type           // we match the header using a case statement, and depending           // on the content type we return a specific object           header =&gt; header match {              // if we have this contentype we create a custom response             case ContentType(MediaType(\"application/vnd.type.a\"), _) =&gt; {               respondWithMediaType(`application/json`) {                 complete {                   Person(\"Bob\", \"Type A\", System.currentTimeMillis());                 }               }             }              // if we habe another content-type we return a different type.             case ContentType(MediaType(\"application/vnd.type.b\"), _) =&gt; {               respondWithMediaType(`application/json`) {                 complete {                   Person(\"Bob\", \"Type B\", System.currentTimeMillis());                 }               }             }              // if content-types do not match, return an error code             case default =&gt; {               complete {                 HttpResponse(406);               }             }           }         }       }     }   }    // handles the other path, we could also define these in separate files   // This is just a simple route to explain the concept   val anotherRoute = {     path(\"path2\") {       get {         // respond with text/html.         respondWithMediaType(`text/html`) {           complete {             // respond with a set of HTML elements             &lt;html&gt;               &lt;body&gt;                 &lt;h1&gt;Path 2&lt;/h1&gt;               &lt;/body&gt;             &lt;/html&gt;           }         }       }     }   }   A lot of code is in there, so lets highlight a couple of elements in detail:     val aSimpleRoute = {     path(\"path1\") {       get {...}    } }   This starting point of the route first checks whether the request is made to the “localhost:8080/path1” path and then checks the HTTP method. In this case we’re only interested in GET methods. Once we’ve got a get method we do the following:            // Get the value of the content-header. Spray         // provides multiple ways to do this.         headerValue({           case x@HttpHeaders.`Content-Type`(value) =&gt; Some(value)           case default =&gt; None         }) {           // the header is passed in containing the content type           // we match the header using a case statement, and depending           // on the content type we return a specific object           header =&gt; header match {              // if we have this contentype we create a custom response             case ContentType(MediaType(\"application/vnd.type.a\"), _) =&gt; {               respondWithMediaType(`application/json`) {                 complete {                   Person(\"Bob\", \"Type A\", System.currentTimeMillis());                 }               }             }              // if we habe another content-type we return a different type.             case ContentType(MediaType(\"application/vnd.type.b\"), _) =&gt; {               respondWithMediaType(`application/json`) {                 complete {                   Person(\"Bob\", \"Type B\", System.currentTimeMillis());                 }               }             }              // if content-types do not match, return an error code             case default =&gt; {               complete {                 HttpResponse(406);               }             }           }         }       }   In this piece of code we extract the Content-Type header of the request and based on that determine the response. The response is automatically converted to JSON because the responseWithMediaType is set to application/json. If a mediatype is provided which we don’t understand we return an 406 message.   Lets test this   Now lets test whether this is working. Spray provides own libraries and classes for testing, but for now lets just use a simple basic rest client. For this I usually use the Chrome Advanced Rest Client. In the following two screenshots you can see three calls being made to http://localhost:8080/path1:   Call with media-type “application/vnd.type.a”:    Call with media-type “application/vnd.type.b”:    Call with media-type “application/vnd.type.c”:    As you can see, the responses exactly match the routes we defined.   What is next   In the following article we’ll connect Spray IO to a database, make testing a little bit easier and explore a number of other Spray.IO features.  ","categories": ["posts","spray","hateoas","rest","scala"],
        "tags": [],
        "url": "http://www.smartjava.org/content/first-steps-rest-spray-and-scala/",
        "teaser":null},{
        "title": "Use reactive streams API to combine akka-streams with rxJava",
        "excerpt":"Just a quick article this time, since I’m still experimenting with this stuff. There is a lot of talk around reactive programming. In Java 8 we’ve got the Stream API, we got rxJava we got ratpack and Akka has got akka-streams.   The main issue with these implementations is that they aren’t compatible. You can’t connect the subscriber of one implementation to the publisher of another. Luckily an initiative has started to provide a way that these different implementations can work together:   \"It is the intention of this specification to allow the creation of many conforming implementations, which by virtue of abiding by the rules will be able to interoperate smoothly, preserving the aforementioned benefits and characteristics across the whole processing graph of a stream application.\" From - http://www.reactive-streams.org/   How does this work   Now how do we do this? Lets look at a quick example based on the akka-stream provided examples (from here). In the following listing    package sample.stream  import akka.actor.ActorSystem import akka.stream.FlowMaterializer import akka.stream.scaladsl.{Flow, SubscriberSink, PublisherSource, Source} import rx.internal.reactivestreams.RxSubscriberToRsSubscriberAdapter import rx.{Subscriber, RxReactiveStreams, Observable} import scala.collection.JavaConverters._  object BasicTransformation {    def main(args: Array[String]): Unit = {      // define an implicit actorsystem and import the implicit dispatcher     implicit val system = ActorSystem(\"Sys\")      // flow materializer determines how the stream is realized.     // this time as a flow between actors.     implicit val materializer = FlowMaterializer()       // input text for the stream.     val text =       \"\"\"|Lorem Ipsum is simply dummy text of the printing and typesetting industry.          |Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,           |when an unknown printer took a galley of type and scrambled it to make a type           |specimen book.\"\"\".stripMargin      // create an observable from a simple list (this is in rxjava style)     val first = Observable.from(text.split(\"\\\\s\").toList.asJava);     // convert the rxJava observable to a publisher     val publisher = RxReactiveStreams.toPublisher(first);     // based on the publisher create an akka source     val source = PublisherSource(publisher);      // create a subscriber     val sub = new Subscriber[String]() {       override def onCompleted(): Unit = println(\"The observerd Akka-Stream is completed\")       override def onError(e: Throwable): Unit = println(\"The observered Akka-Stream caused on error\")       override def onNext(t: String): Unit = println(\"Message received:\" + t)     }      val subSink = SubscriberSink(new RxSubscriberToRsSubscriberAdapter(sub));      // we create a simple flow of three steps     val flow = Flow[String]       .map(_.toUpperCase).               // step       filter(_.length &gt; 3).              // step       take(5)                            // step      // we want to run this flow with our specified source and sink     // both the source and the sink are rxJava components converted     // used with 'reactive streams'     flow.runWith(source,subSink)   } }   The code comments in this example explain pretty much what is happening. What we do here is we create a rxJava based Observable. Convert this Observable to a “reactive streams” publisher and use this publisher to create an akka-streams source.  For the rest of the code we can use the akka-stream style flow API to model the stream. In this case we just do some filtering and print out the result.  ","categories": ["posts","rxjava","java","scala","reactive"],
        "tags": [],
        "url": "http://www.smartjava.org/content/use-reactive-streams-api-combine-akka-streams-rxjava/",
        "teaser":null},{
        "title": "ReactiveMongo with Akka, Scala and websockets",
        "excerpt":"I was looking for a simple websocket server for one of my projects to test some stuff with reactive mongo. When looking around, though, I couldn’t really find a simple basic implementation without including a complete framework. Finally I stumbled upon one of Typesage activtor projects: http://typesafe.com/activator/template/akka-spray-websocket. Even though the name implies that spray is required, it actually uses websocket stuff from here: https://github.com/TooTallNate/Java-WebSocket, which provides a very simple to use basic websocket implementation.   So in this article I’ll show you how you can setup a very simple websocket server (without requiring additional frameworks), together with Akka and ReactiveMongo. The following screenshots shows what we’re aiming for:  In this screenshot you can see a simple websocket client that talks to our server. Our server has the following functionality:      Anything the client sends is echo'd back.   Any input added to a specific (capped) collection in mongoDB is automatically pushed towards all the listeners.   You can cut and paste all the code from this article, but it is probably easier to just get the code from git. You can find it in github here: https://github.com/josdirksen/smartjava/tree/master/ws-akka   Getting started   The first thing we need to do is setup our workspace, so lets start by looking at the sbt configuration:    organization  := \"org.smartjava\"  version       := \"0.1\"  scalaVersion  := \"2.11.2\"  scalacOptions := Seq(\"-unchecked\", \"-deprecation\", \"-encoding\", \"utf8\")  libraryDependencies ++= {   val akkaV = \"2.3.6\"   Seq(     \"com.typesafe.akka\"   %%  \"akka-actor\"    % akkaV,     \"org.java-websocket\" % \"Java-WebSocket\" % \"1.3.1-SNAPSHOT\",     \"org.reactivemongo\" %% \"reactivemongo\" % \"0.10.5.0.akka23\"   ) }  resolvers ++= Seq(\"Code Envy\" at \"http://codenvycorp.com/repository/\"   ,\"Typesafe\" at \"http://repo.typesafe.com/typesafe/releases/\")   Nothing special here, we just specify our dependencies and add some resolvers so that sbt knows where to retrieve the dependencies from. Before we look at the code lets first look at the directory structure and the file of our project:    ├── build.sbt └── src     └── main         ├── resources         │   ├── application.conf         │   └── log4j2.xml         └── scala             ├── Boot.scala             ├── DB.scala             ├── WSActor.scala             └── WSServer.scala   In the src/main/resources directory we store our configuration files and in src/main/scala we store all our scala files. Let start by looking at the configuration files. For this project we use two:   The Application.conf file contains our project’s configuration and looks like this:   akka {   loglevel = \"DEBUG\" }  mongo {   db = \"scala\"   collection = \"rmongo\"   location = \"localhost\" }  ws-server {   port = 9999 }   As you can see we just define the log level, how to use mongo and on which port we want our websocket server to listen. And we also need a log4j2.xml file since the reactivemongo library uses that one for logging:   &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;Configuration status=\"INFO\"&gt;     &lt;Appenders&gt;         &lt;Console name=\"Console\" target=\"SYSTEM_OUT\"&gt;             &lt;PatternLayout pattern=\"%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n\"/&gt;         &lt;/Console&gt;     &lt;/Appenders&gt;     &lt;Loggers&gt;         &lt;Root level=\"INFO\"&gt;             &lt;AppenderRef ref=\"Console\"/&gt;         &lt;/Root&gt;     &lt;/Loggers&gt; &lt;/Configuration&gt;   So, with the boring stuff out of the way lets look at the scala files.   Starting the websocket server and registering the paths   The Boot.scala file looks like this:   package org.smartjava  import akka.actor.{Props, ActorSystem}  /**  * This class launches the system.  */ object Boot extends App {   // create the actor system   implicit lazy val system = ActorSystem(\"ws-system\")   // setup the mongoreactive connection   implicit lazy val db = new DB(Configuration.location, Configuration.dbname);    // we'll use a simple actor which echo's everything it finds back to the client.   val echo = system.actorOf(EchoActor.props(db, Configuration.collection), \"echo\")    // define the websocket routing and start a websocket listener   private val wsServer = new WSServer(Configuration.port)   wsServer.forResource(\"/echo\", Some(echo))   wsServer.start    // make sure the actor system and the websocket server are shutdown when the client is   // shutdown   sys.addShutdownHook({system.shutdown;wsServer.stop}) }  // load configuration from external file object Configuration {   import com.typesafe.config.ConfigFactory    private val config = ConfigFactory.load   config.checkValid(ConfigFactory.defaultReference)    val port = config.getInt(\"ws-server.port\")   val dbname = config.getString(\"mongo.db\")   val collection = config.getString(\"mongo.collection\")   val location = config.getString(\"mongo.location\") }   In this source file we see two objects. The Configuration object allows us to easily access the configuration elements from the application.conf file and the Boot object will start our server. The comments in the code should pretty much explain what is happening, but let me point out the main things:      We create an Akka actor system and a connection to our mongoDB instance.   We define an actor which we can register to a specific websocket path.   Then we create and start the websocketserver and register a path to the actor we just created.   Finally we register a shutdown hook, to clean everything up.   And that’s it. Now lets look at the interesting part of the code. Next up is the WSServer.scala file.   Setting up a websocket server   In the WSServer.scala file we define the websocket server.    package org.smartjava  import akka.actor.{ActorSystem, ActorRef} import java.net.InetSocketAddress import org.java_websocket.WebSocket import org.java_websocket.framing.CloseFrame import org.java_websocket.handshake.ClientHandshake import org.java_websocket.server.WebSocketServer import scala.collection.mutable.Map import akka.event.Logging  /**  * The WSserver companion objects defines a number of distinct messages sendable by this component  */ object WSServer {   sealed trait WSMessage   case class Message(ws : WebSocket, msg : String) extends WSMessage   case class Open(ws : WebSocket, hs : ClientHandshake) extends WSMessage   case class Close(ws : WebSocket, code : Int, reason : String, external : Boolean)                                                                                                           extends WSMessage   case class Error(ws : WebSocket, ex : Exception) extends WSMessage }  /**  * Create a websocket server that listens on a specific address.  *  * @param port  */ class WSServer(val port : Int)(implicit system : ActorSystem, db: DB )                               extends WebSocketServer(new InetSocketAddress(port)) {    // maps the path to a specific actor.   private val reactors = Map[String, ActorRef]()   // setup some logging based on the implicit passed in actorsystem   private val log = Logging.getLogger(system, this);    // Call this function to bind an actor to a specific path. All incoming   // connections to a specific path will be routed to that specific actor.   final def forResource(descriptor : String, reactor : Option[ActorRef]) {     log.debug(\"Registring actor:\" + reactor + \" to \" + descriptor);     reactor match {       case Some(actor) =&gt; reactors += ((descriptor, actor))       case None =&gt; reactors -= descriptor     }   }    // onMessage is called when a websocket message is recieved.   // in this method we check whether we can find a listening   // actor and forward the call to that.   final override def onMessage(ws : WebSocket, msg : String) {      if (null != ws) {       reactors.get(ws.getResourceDescriptor) match {         case Some(actor) =&gt; actor ! WSServer.Message(ws, msg)         case None =&gt; ws.close(CloseFrame.REFUSE)       }     }   }    final override def onOpen(ws : WebSocket, hs : ClientHandshake) {     log.debug(\"OnOpen called {} :: {}\", ws, hs);     if (null != ws) {       reactors.get(ws.getResourceDescriptor) match {         case Some(actor) =&gt; actor ! WSServer.Open(ws, hs)         case None =&gt; ws.close(CloseFrame.REFUSE)       }     }   }    final override def onClose(ws : WebSocket, code : Int, reason : String, external : Boolean) {     log.debug(\"Close called {} :: {} :: {} :: {}\", ws, code, reason, external);     if (null != ws) {       reactors.get(ws.getResourceDescriptor) match {         case Some(actor) =&gt; actor ! WSServer.Close(ws, code, reason, external)         case None =&gt; ws.close(CloseFrame.REFUSE)       }     }   }   final override def onError(ws : WebSocket, ex : Exception) {     log.debug(\"onError called {} :: {}\", ws, ex);     if (null != ws) {       reactors.get(ws.getResourceDescriptor) match {         case Some(actor) =&gt; actor ! WSServer.Error(ws, ex)         case None =&gt; ws.close(CloseFrame.REFUSE)       }     }   } }   A large source file, but not difficult to understand. Let me explain the core concepts:      We first define a number of messages as case classes. These are the messages that we sent to our actors. They reflect the messages our websocket server can receive from a client.   The WSServer itself extends from the WebSocketServer provided by the org.java_websocket library.   The WSServer defines one additional function called forResource. With this function we define which actor to call when we receive a message on our websocket server.   and finally we override the different on* methods which are called when a specific event happens on to our websocket server.   Now lets look at the echo functionality   The akka echo actor   The echo actor has two roles in this scenario. First it provides the functionality to respond to incoming messages by responding with the same message. Besides that it also creates a child actor (named ListenActor) that handles the documents received from mongoDB.    object EchoActor {    // Messages send specifically by this actor to another instance of this actor.   sealed trait EchoMessage    case class Unregister(ws : WebSocket) extends EchoMessage   case class Listen() extends EchoMessage;   case class StopListening() extends EchoMessage    def props(db: DB): Props = Props(new EchoActor(db)) }  /**  * Actor that handles the websocket request  */ class EchoActor(db: DB) extends Actor with ActorLogging {   import EchoActor._    val clients = mutable.ListBuffer[WebSocket]()   val socketActorMapping = mutable.Map[WebSocket, ActorRef]()    override def receive = {      // receive the open request     case Open(ws, hs) =&gt; {       log.debug(\"Received open request. Start listening for \", ws)       clients += ws        // create the child actor that handles the db listening       val targetActor = context.actorOf(ListenActor.props(ws, db));        socketActorMapping(ws) = targetActor;       targetActor ! Listen     }      // recieve the close request     case Close(ws, code, reason, ext) =&gt; {       log.debug(\"Received close request. Unregisting actor for url {}\", ws.getResourceDescriptor)        // send a message to self to unregister       self ! Unregister(ws)       socketActorMapping(ws) ! StopListening       socketActorMapping remove ws;     }      // recieves an error message     case Error(ws, ex) =&gt; self ! Unregister(ws)      // receives a text message     case Message(ws, msg) =&gt; {       log.debug(\"url {} received msg '{}'\", ws.getResourceDescriptor, msg)       ws.send(\"You send:\" + msg);     }      // unregister the websocket listener     case Unregister(ws) =&gt; {       if (null != ws) {         log.debug(\"unregister monitor\")         clients -= ws       }     }   } }   The code of this actor pretty much should explain itself. With this actor and the code so far we’ve got a simple websocket server that uses an actor to handle messages. Before we look at the ListenActor, which is started from the “Open” message received by the EchoHandler, lets quickly look at how we connect to mongoDB from our DB object:    package org.smartjava;  import play.api.libs.iteratee.{Concurrent, Enumeratee, Iteratee} import reactivemongo.api.collections.default.BSONCollection import reactivemongo.api._ import reactivemongo.bson.BSONDocument import scala.concurrent.ExecutionContext.Implicits.global  /**  * Contains DB related functions.  */ class DB(location:String, dbname:String)  {    // get connection to the database   val db: DefaultDB = createConnection(location, dbname)   // create a enumerator that we use to broadcast received documents   val (bcEnumerator, channel) = Concurrent.broadcast[BSONDocument]   // assign the channel to the mongodb cursor enumerator   val iteratee = createCursor(getCollection(Configuration.collection))                     .enumerate()                     .apply(Iteratee                         .foreach({doc: BSONDocument =&gt; channel.push(doc)}));    /**    * Return a simple collection    */   private def getCollection(collection: String): BSONCollection = {     db(collection)   }    /**    * Create the connection    */   private def createConnection(location: String, dbname: String)  : DefaultDB = {     // needed to connect to mongoDB.     import scala.concurrent.ExecutionContext      // gets an instance of the driver     // (creates an actor system)     val driver = new MongoDriver     val connection = driver.connection(List(location))      // Gets a reference to the database     connection(dbname)   }    /**    * Create the cursor    */   private def createCursor(collection: BSONCollection): Cursor[BSONDocument] = {     import reactivemongo.api._     import reactivemongo.bson._     import scala.concurrent.Future      import scala.concurrent.ExecutionContext.Implicits.global      val query = BSONDocument(       \"currentDate\" -&gt; BSONDocument(         \"$gte\" -&gt; BSONDateTime(System.currentTimeMillis())       ));      // we enumerate over a capped collection     val cursor  = collection.find(query)       .options(QueryOpts().tailable.awaitData)       .cursor[BSONDocument]      return cursor   }    /**    * Simple function that registers a callback and a predicate on the    * broadcasting enumerator    */   def listenToCollection(f: BSONDocument =&gt; Unit,                          p: BSONDocument =&gt; Boolean ) = {      val it = Iteratee.foreach(f)     val itTransformed = Enumeratee.takeWhile[BSONDocument](p).transform(it);     bcEnumerator.apply(itTransformed);   } }   Most of this code is fairly standard, but I’d like to point a couple of things out. At the beginning of this class we set up an iteratee like this:      val db: DefaultDB = createConnection(location, dbname)   val (bcEnumerator, channel) = Concurrent.broadcast[BSONDocument]   val iteratee = createCursor(getCollection(Configuration.collection))                     .enumerate()                     .apply(Iteratee                         .foreach({doc: BSONDocument =&gt; channel.push(doc)}));   What we do here is that we first create a broadcast enumerator using the Concurrent.broadcast function. This enumerator can push elements provided by the channel to multiple consumers (iteratees). Next we create an iteratee on the enumerator provided by our ReactiveMongo cursor, where we use the just created channel to pass the documents to any iteratee that is connected to the bcEnumerator.  We connect iteratees to the bcEnumerator in the listenToCollection function:      def listenToCollection(f: BSONDocument =&gt; Unit,                          p: BSONDocument =&gt; Boolean ) = {      val it = Iteratee.foreach(f)     val itTransformed = Enumeratee.takeWhile[BSONDocument](p).transform(it);     bcEnumerator.apply(itTransformed);   }   In this function we pass in a function and a predicate. The function is executed whenever a document is added to mongo and the predicate is used to determine when to stop sending messages to the iteratee.   The only missing part is the ListenActor   ListenActor which responds to messages from Mongo   The following code shows the actor responsible for responding to messages from mongoDB. When it receives a Listen message it registers itself using the listenToCollection function. Whenever a message is passed in from mongo it sends a message to itself, to further propogate it to the websocket.   object ListenActor {   case class ReceiveUpdate(msg: String);   def props(ws: WebSocket, db: DB): Props = Props(new ListenActor(ws, db)) } class ListenActor(ws: WebSocket, db: DB) extends Actor with ActorLogging {    var predicateResult = true;    override def receive = {     case Listen =&gt; {        log.info(\"{} , {} , {}\", ws, db)        // function to call when we receive a message from the reactive mongo       // we pass this to the DB cursor       val func = ( doc: BSONDocument) =&gt; {         self ! ReceiveUpdate(BSONDocument.pretty(doc));       }        // the predicate that determines how long we want to retrieve stuff       // we do this while the predicateResult is true.       val predicate = (d: BSONDocument) =&gt; {predicateResult} :Boolean       Some(db.listenToCollection(func, predicate))     }      // when we recieve an update we just send it over the websocket     case ReceiveUpdate(msg) =&gt; {       ws.send(msg);     }      case StopListening =&gt; {       predicateResult = false;        // and kill ourselves       self ! PoisonPill     }   } }   Now that we’ve done all that, we can run this example.  On startup you’ll see something like this:   [DEBUG] [11/22/2014 15:14:33.856] [main] [EventStream(akka://ws-system)] logger log1-Logging$DefaultLogger started [DEBUG] [11/22/2014 15:14:33.857] [main] [EventStream(akka://ws-system)] Default Loggers started [DEBUG] [11/22/2014 15:14:35.104] [main] [WSServer(akka://ws-system)] Registring actor:Some(Actor[akka://ws-system/user/echo#1509664759]) to /echo 15:14:35.211 [reactivemongo-akka.actor.default-dispatcher-5] INFO  reactivemongo.core.actors.MongoDBSystem - The node set is now available 15:14:35.214 [reactivemongo-akka.actor.default-dispatcher-5] INFO  reactivemongo.core.actors.MongoDBSystem - The primary is now available   Next when we connect a websocket we see the following:    [DEBUG] [11/22/2014 15:15:18.957] [WebSocketWorker-32] [WSServer(akka://ws-system)] OnOpen called org.java_websocket.WebSocketImpl@3161f479 :: org.java_websocket.handshake.HandshakeImpl1Client@6d9a6e19 [DEBUG] [11/22/2014 15:15:18.965] [ws-system-akka.actor.default-dispatcher-2] [akka://ws-system/user/echo] Received open request. Start listening for  WARNING arguments left: 1 [INFO] [11/22/2014 15:15:18.973] [ws-system-akka.actor.default-dispatcher-5] [akka://ws-system/user/echo/$a] org.java_websocket.WebSocketImpl@3161f479 , org.smartjava.DB@73fd64   Now lets insert a message into the mongo collection which we created with the following command:   db.createCollection( \"rmongo\", { capped: true, size: 100000 } )   And lets insert an message:   &gt; db.rmongo.insert({\"test\": 1234567, \"currentDate\": new Date()}) WriteResult({ \"nInserted\" : 1 })   Which results in this in our websocket client:    If you’re interested in the source files look at the following directory in GitHub: https://github.com/josdirksen/smartjava/tree/master/ws-akka  ","categories": ["posts","reactive","akka","mongodb","scala"],
        "tags": [],
        "url": "http://www.smartjava.org/content/reactivemongo-akka-scala-and-websockets/",
        "teaser":null},{
        "title": "Scala snippets 1:  Folding",
        "excerpt":"Coming from a Java background, Scala provides lots of nice features and libraries that allow you to create nice and concise code. Wrapping your head around these concepts though, can be hard. In this short series of articles I’ll walk through some of the concepts behind scala, and show you how you can use the various concepts. There is no strict structure to this series, I’ll just show concepts I found interesting.   The following snippets are available:      Scala snippets 1: Folding   Scala snippets 2: List symbol magic Scala snippets 3: Lists together with Map, flatmap, zip and reduce Scala snippets 4: Pimp my library pattern with type classes   Folding and unfolding  In this snippet, we’ll look at folding. According to wikipedia this is what folding does:   \"In functional programming, fold – also known variously as reduce, accumulate, aggregate, compress, or inject – refers to a family of higher-order functions that analyze a recursive data structure and recombine through use of a given combining operation the results of recursively processing its constituent parts, building up a return value.\"    http://en.wikipedia.org/wiki/Fold_(higher-order_function)   So basically, you input some data, apply a method and return a different value. Lets first look at the signature of the fold operation:   def fold[A1 &gt;: A](z: A1)(op: (A1, A1) =&gt; A1): A1   First lets look at the documentation (http://www.scala-lang.org/api/current/index.html#scala.collection.immutable.List):   Folds the elements of this traversable or iterator using the specified associative binary operator. The order in which operations are performed on elements is unspecified and may be nondeterministic.      A1: a type parameter for the binary operator, a supertype of A.   z: a neutral element for the fold operation; may be added to the result an arbitrary number of times, and must not change the result (e.g., Nil for list concatenation, 0 for addition, or 1 for multiplication.)   op: a binary operator that must be associative returns the result of applying fold operator op between all the elements and z   I always have difficulty reading these kind of descriptions to lets look at an example:   scala&gt; val list = \"Hello World this is a string\".split(\" \"); list: Array[String] = Array(Hello, World, this, is, a, string)  scala&gt; list.fold(\"&gt;&gt;\") {(z, i) =&gt; z + \":\" + i } res3: String = &gt;&gt;:Hello:World:this:is:a:string   Here we first create a List[String]. On this object we call the fold method. The first argument we provide is the starting value, and the second argument is the function we apply to each value of the list. In this case we just concatenate the values together. The output of this function is the used as input for the next one.   Lets look at a couple of other examples:   Here we sum all the values:  scala&gt; val list = List.range(0, 20) list: List[Int] = List(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19)  scala&gt; list.fold(0){(z, i) =&gt; z + i } res10: Int = 190   Or product them:  scala&gt; val list = List.range(1, 20) list: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19)  scala&gt; list.fold(1){(z, i) =&gt; i * z} res12: Int = 109641728   Easy right! There are a lot more advanced usages of fold you can use, but the basic premise is the same: http://oldfashionedsoftware.com/2009/07/30/lots-and-lots-of-foldleft-examples/  ","categories": ["posts","snippets","scala"],
        "tags": [],
        "url": "http://www.smartjava.org/content/scala-snippets-1-folding/",
        "teaser":null},{
        "title": "Scala snippets 2:  List symbol magic",
        "excerpt":"The following snippets are available:      Scala snippets 1: Folding   Scala snippets 2: List symbol magic Scala snippets 3: Lists together with Map, flatmap, zip and reduce Scala snippets 4: Pimp my library pattern with type classes   In scala every symbol can be a function, so overloading operators (which isn’t really overloading, since operators are already methods) is very easy and is something which you see in many libraries. In this snippet we’ll just explore a couple of the overloaded methods that make working with lists much easiers.   So lets get started and look at the ++ operator. First, like we always do, lets create a list.   scala&gt; val list = 0 until 10 toList list: List[Int] = List(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)  scala&gt; val list2 = 10 to 0 by -1 toList list2: List[Int] = List(10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0)   And just walk through the operators from here: http://www.scala-lang.org/api/2.11.1/index.html#scala.collection.immutable.List   The first operator we’ll look at is ++. With this operator we can add two lists together, and return a new one:   scala&gt; val list3 = list ++ list2 list3: List[Int] = List(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0)  scala&gt; val list3 = list2 ++ list list3: List[Int] = List(10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9)   Note that you don’t have to add the same types. Scala will automatically select the most relevant superclass.   scala&gt; val list1 = 0 to 10 toList list1: List[Int] = List(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)  scala&gt; val list2 = 10 to 0 by -1 toList list2: List[Int] = List(10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0)  scala&gt; val list3 = list1.asInstanceOf[List[Double]] list3: List[Double] = List(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)  scala&gt; list3 ++ list2 res4: List[AnyVal] = List(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0)   As you can see AnyVal is the most common supertype of both Int and Double so that one gets selected.   Now that we’ve seen the ++ operator lets look at one almost the same the ++: operator.With this operator we have the same semantics as ++ but this time the type of the result is determined by the right operand instead of the left one:   scala&gt; vector1 res14: Vector[Int] = Vector(10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)  scala&gt; list1 res15: List[Int] = List(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)  scala&gt; vector1 ++ list1 res16: scala.collection.immutable.Vector[Int] = Vector(10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)  scala&gt; vector1 ++: list1 res17: List[Int] = List(10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)   On to the next two operators :+ and +:. With these two operators we can append and prepend an element to a list:   scala&gt; 999 +: list1 res27: List[Int] = List(999, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)  scala&gt; list1 :+ 999 res28: List[Int] = List(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 999)   Simple right? Note that the : is always on the side of the list (target). The same goes for the ++: operator we saw earlier.   What more do we have? :: and :::. Both these operators add something to the beginning of a list. The :: operator adds a single element, and the ::: operator add a complete list. So basically they are the same as the +: and the ++ operator. The main change is that ++ and +: can be used with Traversable and ::: and :: can only be used with a list.   scala&gt; 11 +: list1 res38: List[Int] = List(11, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)  scala&gt; list1 res39: List[Int] = List(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)  scala&gt; 11 +: list1 res40: List[Int] = List(11, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)  scala&gt; list2 res41: List[Int] = List(10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0)  scala&gt; list1 ::: list2 res43: List[Int] = List(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0)   And then we’ve pretty much covered all except :\\ and :/. These functions allow you to fold (see here for more) an list. :\\ folds from right to left and :/ folds from left to right.  scala&gt; list1 res50: List[Int] = List(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)  scala&gt; (1 /: list1)((r,i) =&gt; {println(i);i+r}) 0 1 2 3 4 5 6 7 8 9 10 res51: Int = 56  scala&gt; (list1 :\\ 1)((i,r) =&gt; {println(i);i+r}) 10 9 8 7 6 5 4 3 2 1 0 res52: Int = 56   As you can see the folding direction determines whether the elements are processed from beginning to end or backwards.   And that’s it for this small snippet.  ","categories": ["posts","snippets","scala"],
        "tags": [],
        "url": "http://www.smartjava.org/content/scala-snippets-2-list-symbol-magic/",
        "teaser":null},{
        "title": "Scala snippets 3: Lists together with Map, flatmap, zip and reduce",
        "excerpt":"You can’t really talk about scala without going into the details of the Map, flatMap, zip and reduce functions. With these functions it is very easy to process the contents of lists and work with the Option object. Note that on this site you can find some more snippets:      Scala snippets 1: Folding   Scala snippets 2: List symbol magic Scala snippets 3: Lists together with Map, flatmap, zip and reduce Scala snippets 4: Pimp my library pattern with type classes   Lets just start with the map option. With a map option we apply a function to each element of the list and return that as a new list.   We can use this to multiply each value in the list:   scala&gt; list1 res3: List[Int] = List(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10)  scala&gt; list1.map(x=&gt;x*x) res4: List[Int] = List(0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100)  Some function you apply to a list might result in Option elements. Take for instance the following function:  scala&gt; val evenify = (x:Int) =&gt; if (x % 2 == 0) Some(x) else None evenify: Int =&gt; Option[Int] = &lt;function1&gt;  scala&gt; list1.map(evenify) res6: List[Option[Int]] = List(Some(0), None, Some(2), None, Some(4), None, Some(6), None, Some(8), None, Some(10))  The problem in this case is that we’re often not that interested in the None results in our list. But how do we easily get them out? For this we can use flatMap. With flatMap we can process lists of sequences. We apply the provided function on each element of each sequence in the list and return a list that contains the elements of each sequence of the original list. An example is much easier to understand:   scala&gt; val list3 = 10 to 20 toList list3: List[Int] = List(10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)  scala&gt; val list2 = 1 to 10 toList list2: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)  scala&gt; val list4 = List(list2, list3) list4: List[List[Int]] = List(List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), List(10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20))  scala&gt; list4.flatMap(x=&gt;x.map(y=&gt;y*2)) res2: List[Int] = List(2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40)   As you can see we have two lists. On this list we call the flatMap function. The flatMap processes each of the two entries individually. On each of te individual lists we call the map function to duplicate each entry. The final result is a single list that contains all entries flattened to a single list.   Now lets look back at the evenify function we saw earlier and the list of Option elements we had.  scala&gt; val list1 = 1 to 10 toList list1: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)  scala&gt; list1.map(evenify) res3: List[Option[Int]] = List(None, Some(2), None, Some(4), None, Some(6), None, Some(8), None, Some(10))  scala&gt; val list2 = list1.map(evenify) list2: List[Option[Int]] = List(None, Some(2), None, Some(4), None, Some(6), None, Some(8), None, Some(10))  scala&gt; list2.flatMap(x =&gt; x) res6: List[Int] = List(2, 4, 6, 8, 10)  Easy right. And ofcourse we can also write this in one line.  scala&gt; list1.flatMap(x=&gt;evenify(x)) res14: List[Int] = List(2, 4, 6, 8, 10)  As you can see, not that difficult. Now lets look at the other couple of functions you can use on lists. The first one is zip. And as the name implies with this function we can combine two lists together.  scala&gt; val list = \"Hello.World\".toCharArray list: Array[Char] = Array(H, e, l, l, o, ., W, o, r, l, d)  scala&gt; val list1 = 1 to 20 toList list1: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)  scala&gt; list.zip(list1) res30: Array[(Char, Int)] = Array((H,1), (e,2), (l,3), (l,4), (o,5), (.,6), (W,7), (o,8), (r,9), (l,10), (d,11))  scala&gt; list1.zip(list) res31: List[(Int, Char)] = List((1,H), (2,e), (3,l), (4,l), (5,o), (6,.), (7,W), (8,o), (9,r), (10,l), (11,d))  As soon as one list reaches the end the zip function stops. We’ve also got a zipAll function, which also processes the left over elements of the larger list:  scala&gt; list.zipAll(list1,'a','1') res33: Array[(Char, AnyVal)] = Array((H,1), (e,2), (l,3), (l,4), (o,5), (.,6), (W,7), (o,8), (r,9), (l,10), (d,11), (a,12), (a,13), (a,14), (a,15), (a,16), (a,17), (a,18), (a,19), (a,20))  If the list with characters is exhausted we’ll place the letter ‘a’ if the list of integers is exhausted, we’ll place a 1. We’ve got one final zip function to explore and that’s zipWithIndex. Once again, the name pretty much sums up what will happen. An index element will be added:  scala&gt; list.zipWithIndex res36: Array[(Char, Int)] = Array((H,0), (e,1), (l,2), (l,3), (o,4), (.,5), (W,6), (o,7), (r,8), (l,9), (d,10))  So on to the last of the functions to explore:reduce. With reduce we process all the elements in the list and return a single value. With reduceLeft and reduceRight we can force the direction in which the values are processed (with reduce this isn’t guaranteed):  scala&gt; list1 res51: List[Int] = List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20)  scala&gt; val sum = (x:Int, y:Int) =&gt; {println(x,y) ; x + y} sum: (Int, Int) =&gt; Int = &lt;function2&gt;  scala&gt; list1.reduce(sum) (1,2) (3,3) (6,4) (10,5) (15,6) (21,7) (28,8) (36,9) (45,10) (55,11) (66,12) (78,13) (91,14) (105,15) (120,16) (136,17) (153,18) (171,19) (190,20) res52: Int = 210  scala&gt; list1.reduceLeft(sum) (1,2) (3,3) (6,4) (10,5) (15,6) (21,7) (28,8) (36,9) (45,10) (55,11) (66,12) (78,13) (91,14) (105,15) (120,16) (136,17) (153,18) (171,19) (190,20) res53: Int = 210  scala&gt; list1.reduceRight(sum) (19,20) (18,39) (17,57) (16,74) (15,90) (14,105) (13,119) (12,132) (11,144) (10,155) (9,165) (8,174) (7,182) (6,189) (5,195) (4,200) (3,204) (2,207) (1,209) res54: Int = 210  Besides these functions we also have reduceOption (and also in the reduceLeftOption and reduceRightOption variants). These functions will return an Option instead of the value. This can be used to safely process empty lists, which will result None.  scala&gt; list1.reduceRightOption(sum) (19,20) (18,39) (17,57) (16,74) (15,90) (14,105) (13,119) (12,132) (11,144) (10,155) (9,165) (8,174) (7,182) (6,189) (5,195) (4,200) (3,204) (2,207) (1,209) res65: Option[Int] = Some(210)  scala&gt; val list3 = List() list3: List[Nothing] = List()  scala&gt; list3.reduceRightOption(sum) res67: Option[Int] = None  Enough for this snippet and for exploring the List/Collections API for now. In the next snippet we’ll look at some Scalaz stuff, since even though the library has a bit of a bad name of being complex, it provides some really nice features.  ","categories": ["posts","snippets","scala"],
        "tags": [],
        "url": "http://www.smartjava.org/content/scala-snippets-3-lists-together-map-flatmap-zip-and-reduce/",
        "teaser":null},{
        "title": "Scala snippets 4: Pimp my library pattern with type classes.",
        "excerpt":"I wanted to write an article on the fun parts of scalaz, but thought it would be best to first look a bit closer at the type classes system provided by scala. So in this snippet we’ll explore a small part of how type classes work and can help you in writing more generic code.   More snippets can be found here:      Scala snippets 1: Folding   Scala snippets 2: List symbol magic Scala snippets 3: Lists together with Map, flatmap, zip and reduce Scala snippets 4: Pimp my library pattern with type classes   Type classes   Looking at the type class definition from wikipedia might quickly scare you away:   \"In computer science, a type class is a type system construct that supports ad hoc polymorphism. This is achieved by adding constraints to type variables in parametrically polymorphic types. Such a constraint typically involves a type class 'T' and a type variable 'a', and means that 'a' can only be instantiated to a type whose members support the overloaded operations associated with 'T'.\"   Basically what type classes allow is to add functionality to existing classes without needing to touch the existing classes. We could for instance add standard “comparable” functionality to Strings without having to modify the existing classes. Note that you could also just use implicit functions to add custom behavior (e.g the “Pimp my library pattern”, https://coderwall.com/p/k_1jzw/scala-s-pimp-my-library-pattern-example), but using type classes is much more safe and flexible. A good discussion on this can be found here (http://stackoverflow.com/questions/8524878/implicit-conversion-vs-type-class).   So enough introducion, lets look at a very simple example of type classes. Creating a type class in scala takes a number of different steps.   The first step is to create a trait. This trait is the actual type class and defines the functionality that we want to provide. For this article we’ll create a very contrived example where we define a “Duplicate” trait. With this trait we duplicate a specific object. So when we get a string value of “hello”, we want to return “hellohello”, when we get an integer we return value*value, when we get a char ‘c’, we return “cc”. All this in a type safe manner.   Our typeclass is actually very simple:     trait Duplicate[A,B] {     def duplicate(value: A): B   }  Note that is look a lot like scala mix-in traits, but it is used completely different. Once we’ve got the typeclass definition, the next step is to create some default implementations. We do this in the trait’s companion object.    object Duplicate {      // implemented as a singleton object     implicit object DuplicateString extends Duplicate[String,String] {       def duplicate(value: String) = value.concat(value)     }      // or directly, which I like better.     implicit val duplicateInt = new Duplicate[Int, Int] {       def duplicate(value: Int) = value * value     }      implicit val duplicateChar = new Duplicate[Char, String] {       def duplicate(value: Char) = value.toString + value.toString     }   }  }   As you can see we can do this in a couple of different ways. The most important part here is the implicit keyword. Using this keyword we can make these members implicity available under certain circumstances. When you look at the implementation you’ll notice that they are all very straigthforward. We just implement the trait we defined for specific types. In this case for a string, an integer and a character.   Now we can start using the type classes.   object DuplicateWriter {    // import the conversions for use within this object   import conversions.Duplicate    // Generic method that takes a value, and looks for an implicit   // conversion of type Duplicate. If no implicit Duplicate is available   // an error will be thrown. Scala will first look in the local   // scope before looking for implicits in the companion object   // of the trait class.   def write[A,B](value: A)(implicit dup: Duplicate[A, B]) : B = {     dup.duplicate(value)   } }   // simple app that runs our conversions object Example extends App {   import snippets.conversions.Duplicate    implicit val anotherDuplicateInt = new Duplicate[Int, Int] {     def duplicate(value: Int) = value + value   }    println(DuplicateWriter.write(\"Hello\"))   println(DuplicateWriter.write('c'))   println(DuplicateWriter.write(0))   println(DuplicateWriter.write(0)(Duplicate.duplicateInt))  }   In this example we’ve create a DuplicateWriter which calls the duplicate function on the provide class by looking for a matching typecall implementation. In our Example object we also override the default duplicate function for the Int type with a custom one. In the last line we provide a specific Duplicate object to use by the DuplicateWriter. The output of this application is this:   20 100 HelloHello cc   If we run with an unsupported type (e.g a double):   println(DuplicateWriter.write(0d))   We get the following compile time messages (intellij IDE in this case).   Error:(56, 32) could not find implicit value for parameter dup: snippets.conversions.Duplicate[Double,B]   println(DuplicateWriter.write(0d))                                ^  Error:(56, 32) not enough arguments for method write: (implicit dup: snippets.conversions.Duplicate[Double,B])B. Unspecified value parameter dup.   println(DuplicateWriter.write(0d))                                ^  We can also customize the first of these messages by adding the following annotation to our trait/typeclass definition:      @implicitNotFound(\"No member of type class Duplicate in scope for ${T}\")   trait Duplicate[A,B] {     def duplicate(value: A): B   }   So that is a very quick introduction into type classes. As you can see, they provide a very easy way to add custom functionality to classes, even if you don’t control them. In the next snippet we’ll explore a couple of common, very useful type classes from the Scalaz library.  ","categories": ["posts","snippets","scala"],
        "tags": [],
        "url": "http://www.smartjava.org/content/scala-snippets-4-pimp-my-library-pattern-type-classes/",
        "teaser":null},{
        "title": "Building a REST service in Scala with Akka HTTP, Akka Streams and reactive mongo",
        "excerpt":"At the end of last year I wrote a couple of articles that showed how you can use Spray.io to create a Scala based REST service () and how to create a websocket server with Scala, Akka  and reactivemongo (). I wanted to explore the REST server part a bit more, but found out that at the end of 2013 Spray.io was acquired by typesafe, and would be integrated with the Akka stack. So in this article we’ll look at how you can use the Akka HTTP functionality to create a simple web server, and in a follow up we’ll look at how the routing from Spray.io was ported to Akka.   In this article we’ll take the following steps:     Get some dummy data into mongoDB for testing.   Create a server using Akka Http that uses a simple asynchronous handler to process requests.   Create a server which uses a custom flow graph to process incoming requests.   Test both of these servers with a http client also created with Akka-Http.   So lets start with some preparation work and get some data into mongoDB for us to work with.   Loading data into mongoDB   For this example we use some stock related information which you can download from here (http://jsonstudio.com/wp-content/uploads/2014/02/stocks.zip). You can easily do this by executing the following steps:   First get the data:      wget http://jsonstudio.com/wp-content/uploads/2014/02/stocks.zip  Start mongodb in a different terminal      mongod --dbpath ./data/  And finally use mongoimport to import the data       unzip -c stocks.zip | mongoimport --db akka --collection stocks  And as a quick check run a query to see if everything works:  jos@Joss-MacBook-Pro.local:~$ mongo akka       MongoDB shell version: 2.4.8 connecting to: akka &gt; db.stocks.findOne({},{Company: 1, Country: 1, Ticker:1 } ) {         \"_id\" : ObjectId(\"52853800bb1177ca391c17ff\"),         \"Ticker\" : \"A\",         \"Country\" : \"USA\",         \"Company\" : \"Agilent Technologies Inc.\" } &gt;    At this point we have our test data and can look at the code required to run a server.   Create a server which uses a simple asynchronous handler to process requests   To work with Akka Http and access the data in mongo we’re going to need some additional libraries. So before we do anything else, lets first look at the sbt build file we’ve used for this article:     When you look through the dependencies you’ll see the usual suspects:     akka-http-core-experimental contains all the http server and client stuff we're going to use. This library depends on akka-stream so we'll also get that library on our class path.   reactiemongo allows us to connect to mongo in a reactive way.   I've also included play2-reactivemongo and play-json which makes converting the BSON returned from mongo to JSON a lot easier.   Finally, for logging we add logback.   Now before we look at the code required to run the server, lets quickly look at how we’ll query mongo. For this we’ve created a simple helper object creatively named Database:     Not that much to explain. The most important thing to notice here is that both the find functions return a future, so calls to these functions won’t block.  Now that we’ve got the basics out of the way, lets look at the code for the first http server which uses an asynchronous handler.     In this piece of code we create a http server which listens on port 8091. We process each connection that is made with an asyncHandler. This handler should return a Future[HttpResponse].  Lets look at this handler next:     As you can see from this code the handler code is pretty straightforward. We use pattern matching to match a specific url and use the Database object we saw earlier to query mongo. Note the calls to convertToString. These are a couple of helper methods that convert BSON to JSON using the play libraries we included earlier:     When we start this server and open the adres in the browser, we’ll see something like this:      Easy right? Now lets look at a bit more advanced scenario.   Create a server which uses a custom flow graph to process incoming requests.   Akka-http internally uses akka-streams to handle http connections. This means that we can use akka-streams to easily handle http requests in a reactive manner. For a linear flow we can use the standard flow api provided by akka. For more advanced graphs akka-streams provides it’s own DSL, with which you can very easily create more complex graphs where stream events are processed in parallel.   Lets create a new serverbinding that listens on port 8090:     This serverbinding is created in the same manner as we did earlier. The main difference is that this this time we don’t pass the processing of the request onto a handler, but we specify an instance of a flow with the name broadCastMergeFlow. This broadcast merge flow looks like this:     The most important part are the last couple of lines in this code fragment. Here we draw a graph that defines how a message is handled when it is processed by the server. In this case we first broadcast the incoming http request to three parallel streams. In each stream we next make a call to our database to get a ticket. Next we merge the results back together (a merge takes the first available upstream even) and create a response. So depending which of the steps is the fastest we return a ticker either for GOOG, AAPL or MSFT. To see the result better we added a sleep to the getTickerHandler:     Neat right! Akka-streams provides a number of basic building blocks you can use to create these flows (for more info see their documentation: http://doc.akka.io/docs/akka-stream-and-http-experimental/1.0-M2/scala/stream-graphs.html). For instance if we want to zip the responses of the steps together we could create a flow like this:     I really like how this works and how easy it is to visualize data flowing through the different steps. If we use the merge approach you’ll see a result which looks something like this (when called 10 times):   {\"_id\":{\"$oid\":\"52853804bb1177ca391c2221\"},\"Ticker\":\"GOOG\",\"Profit Margin\":0.217 {\"_id\":{\"$oid\":\"52853804bb1177ca391c2221\"},\"Ticker\":\"GOOG\",\"Profit Margin\":0.217 {\"_id\":{\"$oid\":\"52853800bb1177ca391c1809\"},\"Ticker\":\"AAPL\",\"Profit Margin\":0.217 {\"_id\":{\"$oid\":\"52853807bb1177ca391c2781\"},\"Ticker\":\"MSFT\",\"Profit Margin\":0.282 {\"_id\":{\"$oid\":\"52853804bb1177ca391c2221\"},\"Ticker\":\"GOOG\",\"Profit Margin\":0.217 {\"_id\":{\"$oid\":\"52853800bb1177ca391c1809\"},\"Ticker\":\"AAPL\",\"Profit Margin\":0.217 {\"_id\":{\"$oid\":\"52853807bb1177ca391c2781\"},\"Ticker\":\"MSFT\",\"Profit Margin\":0.282 {\"_id\":{\"$oid\":\"52853804bb1177ca391c2221\"},\"Ticker\":\"GOOG\",\"Profit Margin\":0.217 {\"_id\":{\"$oid\":\"52853800bb1177ca391c1809\"},\"Ticker\":\"AAPL\",\"Profit Margin\":0.217 {\"_id\":{\"$oid\":\"52853807bb1177ca391c2781\"},\"Ticker\":\"MSFT\",\"Profit Margin\":0.282   The final part I’d want to show it how you can also use the same approach when you create a http client with akka-http.   Test both of these servers with a http client also created with Akka-Http   Akka-http also provides functionality to easy setup a http client that also uses a stream/flow based approach of message processing. The following listing shows the complete running client:     I won’t go into detail here, since the code follows the same process as for the HTTP server. That’s it for this article and an introduction into akka-stream and akka-http. I really like their approach to message processing and creating readable, reactive code. In a future article we’ll look at some other aspects of akka-http (routes for instance).  ","categories": ["posts","reactive","streams","akka","rest","scala"],
        "tags": [],
        "url": "http://www.smartjava.org/content/building-rest-service-scala-akka-http-akka-streams-and-reactive-mongo/",
        "teaser":null},{
        "title": "All 80+ recipes from Three.js Cookbook online",
        "excerpt":"About a month ago my third book on Three.js got released: Three.js Cookbook. This book has 80+ recipes on how to accomplish common tasks in Three.js. On this page you can find all the code examples from that book. If you like to know more about these recipes, click the link on the right side of this page:   Note that some of the examples might take some time to load. A number of examples use large textures, large models or other big files that need to be downloaded.   Chapter 1: Getting started   01.01 - WebGLRenderer - Skeleton 01.02 - Canvasrenderer - Skeleton 01.03 - CSS3DRenderer - Skeleton 01.04 - Detect WebGL Support 01.05 - Setup animation loop 01.06 - Determine framerate 01.07 - Control variables 01.08 - local webserver 01.09 - cross-origin 01.10 - Keyboard controls 01.11 - load resources asynchronously 01.12 - wait for resources to load 01.13 - Debug Three.js 01.14 - Drop file  Chapter 2: Geometries and Meshes   02.01 - Rotate around axis 02.02 - Rotate around point in space 02.04 - Update stuff 02.05 - Handle large number of objects 02.06 - Create terrain from heightmap 02.07 - Point object to another object 02.09 - Write text in 3D 02.10 - Parametric geometries 02.11 - Extend Three.js with a custom geometry object 02.12 - Spline Curve 02.13 - Load JSON Models with textures 02.14 - Use OBJMTL loader with multiple materials 02.15 - Apply Matrix Transformations  Chapter 3: Working with camera   03.01 - Make the camera follow an object 03.02 - Camera zoom in on object 03.03 - Use a perspective camera 03.04 - Use a orthographic camera 03.05 - Create a 2D overlay 03.06 - Change camera when screen resizes 03.07 - Convert world coordinates to screen coordinates 03.08 - Rotate the camera around the scene - y-axis 03.09 - Rotate the camera around the scene - x-axis 03.10 - Selecting objects  Chapter 4: Materials and textures   04.01 - add depth to mesh with bump maps 04.02 - add depth to mesh with normal maps 04.03 - HTML canvas as textures 04.04 - HTML video as texture 04.05 - Create a mesh with multiple materials 04.06 - Use separate materials for faces 04.07 - add transparency to a mesh 04.08 - Making part of an object transparent 04.10 - Reflections with cubemap 04.11 - Reflections with dynamic cubemap 04.12 - Setup repeating textures 04.13 - Configuring blend modes 04.14 - Create custom UV mappings 04.16 - Use a shadow map for fixed shadows  Chapter 5: Lights and shaders   05.01 - Using shadows with a spot light 05.02 - Using shadows with a directional light 05.03 - Soften lights 05.04 - Create natural lightin with a Hemisphere light 05.05 - Use a point light 05.06 - Move a light following a path 05.09 - Custom Vertex Shader - 1 05.10 - Custom vertex shader - 2 05.06 - Custom vertex shader 05.10 - Custom fragment shader  Chapter 6: particles and postprocessing   06.01 - Create a point cloud from an existing geometry 06.02 - Create a point cloud from scratch - 2 06.02 - Create a point cloud from scratch - 1 06.03 - Color individual particles 06.04 - Style the particles in a particle system 06.05 - Move individual particles - 2 06.05 - Move individual particles 06.06 - Explode geometry - 2 06.06 - Explode geometry - 1 06.07 - Setup basic post processing 06.08 - Create custom post-processing step 06.09 - Save WebGL output  Chapter 7: Animation and physics   07.01 - Animation with Tween.js 07.02 - Animation with morphing (manually) 07.03 - Animation with skeleton 07.04 - Morph animation from Blender 07.05 - Skeleton animation from Blender 07.06 - Add collision detection 07.07 - Save animation as movie 07.08 - Drag and drop object around scene 07.09 - Add a physics engine  ","categories": ["posts","examples","books","three.js"],
        "tags": [],
        "url": "http://www.smartjava.org/content/all-80-recipes-threejs-cookbook-online/",
        "teaser":null},{
        "title": "Using Go to build a REST service on top of mongoDB",
        "excerpt":"I’ve been following go (go-lang) for a while now and finally had some time to experiment with it a bit more.  In this post we’ll create a simple HTTP server that uses mongoDB as a backend and provides a very basic REST API.   In the rest of this article I assume you’ve got a go environment setup and working. If not, look at the go-lang website for instructions (https://golang.org/doc/install). Before we get started we need to get the mongo drivers for go. In a console just type the following:   go get gopkg.in/mgo.v2   This will install the necessary libraries so we can access mongoDB from our go code.   We also need some data to experiment with. We’ll use the same setup as we did in my previous article (http://www.smartjava.org/content/building-rest-service-scala-akka-http-akka-streams-and-reactive-mongo).   Loading data into MongoDB   We use some stock related information which you can download from here (http://jsonstudio.com/wp-content/uploads/2014/02/stocks.zip). You can easily do this by executing the following steps:   First get the data:      wget http://jsonstudio.com/wp-content/uploads/2014/02/stocks.zip  Start mongodb in a different terminal      mongod --dbpath ./data/  And finally use mongoimport to import the data       unzip -c stocks.zip | mongoimport --db akka --collection stocks --jsonArray  And as a quick check run a query to see if everything works:  jos@Joss-MacBook-Pro.local:~$ mongo akka       MongoDB shell version: 2.4.8 connecting to: akka &gt; db.stocks.findOne({},{Company: 1, Country: 1, Ticker:1 } ) {         \"_id\" : ObjectId(\"52853800bb1177ca391c17ff\"),         \"Ticker\" : \"A\",         \"Country\" : \"USA\",         \"Company\" : \"Agilent Technologies Inc.\" } &gt;    At this point we have our test data and can start creating our go based HTTP server. You can find the complete code in a Gist here: https://gist.github.com/josdirksen/071f26a736eca26d7ea4   In the following section we’ll look at various parts of this Gist to explain how to setup a go based HTTP server.   The main function   When you run a go application, go will look for the main function. For our server this main function looks like this:     This will configure a server to run on port 8000, and any request that comes in will be handled by the NewHandler() instance we create in line 64. We start the server by calling the server.listenAndServe() function.   Now lets look at our handler that will respond to requests.   The myHandler struct   Lets first look at what this handler looks like:     Lets split this up and look at the various parts. The first thing we do is define a constructor:     When we call this constructor this will instantiate a myHandler type and call the defineMappings() function. After that it will return the instance we created.   How does the type look we instantiate:     As you can we define a struct with a mux variable as a map. This map will contain our mapping between a request path and a function that can handle the request.   In the constructor we also called the defineMappings function. This funtion, which is defined on out myHandler struct, looks like this:     In this (badly named) function we define the mapping between incoming requests and a specific function that handles the request. And in this function we also create a session to mongoDB using the mgo.Dial function.  As you can see we define the requestHandlers in two different ways. The handler for “/hello” directly points to a function, an the handler for the “/get” path, points to a wrap function which is also defined on the myHandler struct:     This is a function, which returns a function. The reason we do this, is that we also want to pass our mongo session into the request handler. So we create a custom wrapper function, which has the correct signature, and just pass each call to a function where we also provide the mongo session. (Note that we also could have changed the ServeHTTP implementation we explain below)   Finally we define the ServeHTTP function on our struct. This function is called whenever we receive a request:     In this function we check whether we have a match in our mux variable. If we do, we call the configured handle function. If not, we just respond with a simple String.   The request handle functions   Lets start with the handle function which handles the “/hello” path:     Couldn’t be easier. We simply write a specific string as HTTP response. The “/get” path is more interesting:     What we do here is that we make use of the channel functionality of go to run three queries at the same time. We get the ticker information for AAPL, GOOG and MSFT and return a result to the specific channel. When we receive a response on one of the channels we return that response. So each time we call this service we either get the results for AAPL, for GOOG or for MSFT.   With that we conclude this first step into go-lang :)   ","categories": ["posts","rest","mongodb","go-lang","go"],
        "tags": [],
        "url": "http://www.smartjava.org/content/using-go-build-rest-service-top-mongodb/",
        "teaser":null},{
        "title": "Creating an Angular.js application without JavaScript: Scala.js",
        "excerpt":"In my day job I work a lot with Angular.js. And while this is a great framework, sometimes I do miss the type-safety and advanced features of languages such as Scala. Getting “undefined is not a function” is not the the most informative error message you get when developing JavaScript applications. A couple of weeks ago I ran across a twitter message showing that Scala.js 0.60 (http://www.scala-js.org/) was released. With Scala.js you can compile Scala directly to JavaScript. So you can use all the advanced library and language features of Scala, and create JavaScript applications in a typesafe, functional manner!   So for this article I’ll show you the steps I went through to create a minimal Angular.js application using nothing but Scala (and some HTML templates of course).   http://www.smartjava.org/examples/scalajs/html/index-dev.html#/home    This Angular / Scala.js app will show the following features:     Controllers written in Scala   Directives written in Scala   Filters written in Scala   Use the Angular.js route module for handling URL paths   Use foundation for templating   You can find the complete sources for this application in GitHub (https://github.com/josdirksen/smartjava/tree/master/scalajs).   Lets get started    The first thing we need to do is make sure we have node.js installed. This isn’t really necessary but will speed up development considerably. So install node.js from here (https://nodejs.org/download/) before you continue. To work with Scala.js we have to install an sbt plugin. In the file plugins.sbt in the project directory add the following line:  addSbtPlugin(\"org.scala-js\" % \"sbt-scalajs\" % \"0.6.0\")  Note that I’ve tested this with sbt 0.13.7, so make sure you’ve got a current version of sbt installed. Next lets look at the build.sbt file we’ve used for this project:     In this build.sbt you can see the following:      We enable the scalaJS Plugin   We include a couple of Scala.js dependencies like we normally do for scala   We also include a number of jsDependencies. These are the JavaScript libraries we want to use in our web application   And finally we configure how the application will be packaged. More on that later   Lets look a bit at the last couple of lines of this build file:     By setting the persistLauncher to true, we tell Scala.js to create the JavaScript that will automatically launch our application and by setting skip in packageJSDependencies to false, we tell Scala.js to package all our external JavaScript files into one big library for easy inclusion in your HTML page.   Before we look at the Scala code and the HTML templates lets first look at a couple of useful SBT commands. Go to the directory where your project is and run sbt to open the sbt console:   jos@Joss-MacBook-Pro.local:~/git/scalajs$ sbt [info] Set current project to scalajs (in build file:/Users/jos/git/scalajs/)   Now to generate the JavaScript code, we first have to enable node.js so that creating the JavaScript goes a whole lot quicker. In the sbt console set the following:  &gt; set scalaJSStage in Global := FastOptStage [info] Defining */*:scalaJSStage [info] The new value will be used by compile:jsEnv, compile:scalaJSExecClasspath and 1 others. [info]  Run `last` for details. [info] Reapplying settings... [info] Set current project to scalajs (in build file:/Users/jos/git/scalajs/) &gt;   Now you can compile the Scala to JavaScript by calling fastOptJS  &gt;  fastOptJS [warn] The global sbt directory is now versioned and is located at /Users/jos/.sbt/0.13. [warn]   You are seeing this warning because there is global configuration in /Users/jos/.sbt but not in /Users/jos/.sbt/0.13. [warn]   The global sbt directory may be changed via the sbt.global.base system property. [info] Fast optimizing /Users/jos/git/scalajs/target/scala-2.11/scalajs-fastopt.js [success] Total time: 7 s, completed Mar 8, 2015 8:01:31 AM   Enough about SBT, lets look at the Scala code   Angular App in Scala   The complete file can be found in Git (https://github.com/josdirksen/smartjava/tree/master/scalajs) so we won’t show that here. We’ll just look at the interesting parts. Before we look at the JavaScript though, lets look at the HTML templates:     This file is our main index file. As you can see, not that special. Just a single div where we have an ng-view. What is interesting are the three JavaScript files at the bottom of the page. These are generated by Scala.js and contain all our code:      scalajs-fastopt.js: our compiled application.   scalajs-jsdeps.js: all dependencies as a single file   scalajs-launcher.js: code that will launch our application when the page is loaded   We have two additional HTML files, one that is shown as the main page in the ng-view we just saw, and one that is used as the tiles you saw in the beginning of this article:      So now that we’ve seen the HTML, it is time for the actual code. The first part we’ll look at is the starting point of any Angular application, the module configuration:     Here we define our Angular module (named helloworld) and add two dependencies: ngRoute and mm.foundation. ngRoute provides us with a way to easily configure URL mapping to templates and controllers, and mm.foundation (https://github.com/pineconellc/angular-foundation/tree/gh-pages) provides us with directives that map to foundation (http://foundation.zurb.com/) widgets. Next we link to the controllers, filters, directives, config objects we want to use in this app. Note that these are all statically typed, so you can’t forget or register angular components incorrectly. In this example we have five additional components, lets look at each one. First lets look at the RoutingConfig:     As you can see, this is very easy and pretty much the same as you’d do it in JavaScript. The main difference is that instead of providing string values in JavaScript we now just reference a specific controller to use for a path.This controller, which we registered earlier, looks like this:     I won’t go into too much detail here, since the code is pretty self-explanatory. What you can see here is that we define the template that we want to show (when the url from the route is accessed), we setup some scope variables (all typed!) and start an interval using the injected Interval service.   If you’ve looked through the pages you can also see we use a number of filters. Defining a filter in Scala.js is really easy:     And finally we have a directive which we use in tile.html.     When you compile this application to JavaScript using sbt and open the HTML pages you’ll get what we showed in the beginning. A very basic Angular.js application with custom filters, directives and two Angular extensions.   ","categories": ["posts","scala.js","angularjs","angular.js","scala"],
        "tags": [],
        "url": "http://www.smartjava.org/content/creating-angularjs-application-without-javascript-scalajs/",
        "teaser":null},{
        "title": "Migrate sonar data from old to new sonar version",
        "excerpt":"At my current project we were setting up a new buildserver. One of the things we had to do was move an old sonar implementation running on top of H2 to a new version running on Oracle. The problem was that just migrating the data is nearly impossible to do when you start with H2, and when the versions were so far apart as in our case. During some research we stumbled upon the following migration Gist page showing a possible migration path: https://gist.github.com/aslakknutsen/2422117   What the example script on that page does, is that it retieves all the tags from a Git repository and just starts running maven and the sonar plugin to refill the new sonar. With a couple of additions and changes we managed to get this running for our scenario and were able to refill our new sonar with historical data:     The above code was run against with SonarQube 4.5.2, but should also work with SonarQube 5.x.   ","categories": ["posts","devops","sonar"],
        "tags": [],
        "url": "http://www.smartjava.org/content/migrate-sonar-data-old-new-sonar-version/",
        "teaser":null},{
        "title": "Visualizing back pressure and reactive streams with akka-streams, statsd, grafana and influxdb",
        "excerpt":"Currently reactive programming is getting more and more attentions. With reactive programming it is possible to easily create resilient, scalable and fault tolerant systems. In this article we’ll show some examples of reactive programming, and more specifically how backpressure works in practice. For this we’ll use akka-streams since I really like the Scala and Akka programming model, and they have a great DSL for defining flows of data. If you’re a bit new to reactive programming and Akka, a great introduction can be found at slideshare which I’ve included here:       Akka Streams and HTTP  from Roland Kuhn   In this article we’ll visualize a number of scenarios which will show how backpressure and certain flow constructs provided by akka-streams influence the producer of events (the source) and how it affects the subscriber (the sink) to these events. To visualize this data we’ll use a set of simple traits I created for some experiments with monitoring akka actors (sources can be found here: http://github.com/jos.dirksen/akka-mon), together with statsd, influxdb and grafana. For those of you now familiar with these technologies a quick explanation:      akka-mon (https://github.com/josdirksen/akka-mon): Just a very simple project I started to experiment some more with monitoring Akka systems. It allows you to add traits to your actors which enable some simple performance and count logging.   &lt;li statsd (https://github.com/etsy/statsd): Statsd is a simple event collector. It will collect events from any source, aggregate them, and every 30 seconds output a summary of what was collected.&lt;/li&gt;   influxdb (http://influxdb.com): Influxdb is a database specifically created for storing time series. We use influxdb to store the summarized metrics from statsd.   grafana (http://grafana.org/): Finally we need some way to visualize the data. Influxdb itself already contains a simple visualization, but nothing that can easily be used. So for this final component we use grafana. With grafana we can make selections from the influxdb database and visualize them in all kinds of graphs.   So basically with these components installed and configured we have the following flow:      akka-mon will send an event (using UDP) to statsd whenever a message is send or received by one of the configured actors. In our sceneria we send an event when the publisher sends a message, and when one of the subscribers receives a message.   These messages will be collected by statsd and every 30 seconds statsd sends an update of what it has received to influxdb. Influxdb will just store all this information.   To visualize this information we'll configure grafana to connect to the influxdb API so it can retrieve the stored information and visualize it on screen   As you can see there are a number of moving parts that need to be installed or configured to get all this working correctly. If you want to try this for yourself, I’ve added all the configuration files and the source code of the scenarios to github here (https://github.com/josdirksen/akka-back-pressure).   Getting started   I’m not going to explain all the configuration files and installation instructions of the various tools that are used. The default installations of influxdb, statsd and grafana should suffice, and the relevant configurations files can be found  (including the sbt file for this project). To start visualizing back pressure we first need a source (akka-streams term for a publisher) which produces messages and we of course need a sink (also called a subscriber) that can process the messages. Lets start looking at the sink first, since we create one which is a bit more complex, but allows us fine-grained control over its messaging rate:     In this method we use a flow graph to create a new source, which we can use in out other flows. This source combines two existing sources to create a new one. We use a simple source created from a range to produce a number of messages and we use a tick source to throttle the amount of messages that should be send per second. This works in the following manner:      The range source immediately starts producing events.   This event is passed on to the zip component.   The zip component now waits till it receives an event on both its inputs.   The tickSource produces events at the interval we specified.   Now when the tickSource produces an event, both inputs for the zip component will have received an element, and the zip component combines these into a tuple and sends them over the zip.out connector.   The unzip component (just a simple map function) receives this tuple ([Int, Tick]), and only passes on the event received from the rangeSource.   We have a final step called sendMap. This step is added so that we can receive metrics in statsd which we can eventually visualize in grafana.   Since this graph is a source we need to return an unconnected outlet. In this case we return sendMap.outlet and now we have a source whose message rate can be easily controlled.   First scenario: Fast source and fast subscriber   Now lets see what happens when we start processing messages for our first scenario. In this scenario we’ll use a source which produces around 30 messages per second and a subscriber that is faster. So in this scenario the rate of the subscriber should be equal to the rate of the publisher. Lets first look at the code that specifies the flow for this scenario:     In this scenario we connect the source (which we showed above) to an actor sink. An actor sink is an standard Akka actor which, in our case, requests events/messages as fast as they become available. In this example we use an Actor where we can configure a delay to simulate slow subscribers. This Actor looks like this:     To work with akka-streams this actor must extend the ActorSubscriber trait and define a requeststrategy (see documentation (link) for more information on this). As you can see this is a very simple actor, which just processes the message and waits for the specified delay. Note that the other traits are just monitoring traits which send metrics to statsd.   Now, lets see what happens when we run this example (if you do this yourself, make sure statsd and influxdb are also running). With the results in influxdb, we can use grafana to create a graph which shows the number of messages created by the source and processed by the subscriber.      As you can see, no suprises there. The rate of the subscriber is exactly equal to the rate of the publisher and hovers around the 30 messages per second.   Second scenario: Fast source and slowing down subscriber  So in the next scenario lets look and see how back pressure can affect the rate of our publisher. If you’ve viewed the presentation at the beginning of this article, you know that with back pressure a slow consumer can limit the rate a publisher sends event and vice versa. So in this scenario we’ll simulate a consumer which gets slower after each message it has processed:     For this scenario we use a different actor, the SlowDownActor. This actor will run slower each time it has processed a message:     In the receive message of this actor we do a Thread.sleep with an increasing delay. In this scenario, since we use backpressure, the rate of the publisher is controlled by the speed at which the subscriber can process messages. This can be seen in the resulting grafana graph:      In this graph both the subscriber and publisher rates are the same, and slowly go down as the delay in the subscriber increases.   Third scenario: Fast publisher, fast subscriber which gets slower, with drop buffer  In this scenario we introduce a specific buidling block provided by akka-streams, the buffer. If an upstream subscriber is to slow, the buffer will store a specific number of messages before either telling the publisher to slow down, or it will start dropping messages. We’ll use the following scenario:     We start producing 5000 messages at a rate of 50 per second. These messages get buffered in the buffer and finally consumed by the ‘slowingSink’, which is a SlowDownActor (see code above). We’ve configured the buffer with a size of 100 and an OverFlowStrategy of dropHead, which means we’ll drop the oldest message in the buffer when new ones arrive. When we run this scenario you’ll see the following:      You can see that the rate of the subscriber is effectively ignored. This is because we’ve set the OverflowStrategy to OverflowStrategy.dropHead. Interesting to note here is that the subscriber keeps processing messages some time after the publisher has stopped. This is because there are still messages in the buffer. If we set this to OverflowStrategy.backPressure, once the buffer is filled the producer will slow down:    val buffer = Flow[Int].buffer(2000, OverflowStrategy.BackPressure)   This results in the following grafana graph:      Here we see that the rate of the producer stays high, while that of the subscriber is low. This is the phase where the buffer is filling up. Once the buffer is filled up, the backpressure kicks into action and the the rate of the producer drops to the rate of the subscriber. When the publisher is done, the subscriber will happily keep on processing messages, until its buffer is empty.   Fourth scenario: Fast publisher, one fast consumers, one consumer which gets slower   So far we’ve only seen 1 publisher and 1 consumer, in this scenario we’ll look at the effect of two subscribers on the same publisher:     For this we use the Broadcast construct. With a broadcast we duplicate the message and send it to multiple subscribers. In this scenario we have one slow subscriber and one fast subscriber. The result of this can be seen in the following graph:      Here you can see that the rate of the publisher is defined by the rate of which the slow subscriber can process messages. This happens because the slow subscriber informs the publisher to slow down. This in term also affects the fast subscriber.   Fifth scenario: Fast publisher, one fast consumers and one slow consumer which gets slower but has buffer with drop   In the previous scenario we saw that the rate of a publisher drops to the slowest subscriber. An alternative approach is to use a buffer between the slow subscriber and the publisher. This way you can specify whether one slow subscriber is allowed to slow down the publisher. This is what we do in the following scenario:     In this scenario the source first sends messages to a broadcast component, the broadcast component duplicates the message and sends it to the downstream components. As you can see the broadcast is directly connected to a fast subscriber, and the other outlet of the broadcast is first connected to a buffer and finally to a sink which gets slower. This results in the following output:      Compare this graph to the previous one we saw. Here we see that the rate of the publisher and the fast subscriber stays at the same rate. The reason is that the messages are dropped by the buffer we added before the slow subscriber.   When we change the buffer to stop dropping messages, and use backpressure instead we get the following:      Here we see that while the buffer is filling up, both the fast subscriber and publisher can keep a high rate. Once the buffer is filled up, however, the rate drops down to the rate at which the slow subscriber can process messages.   Akka-streams offers a number of other graph components you can use to define a message flow. One interesting one to look at, and also the last one, is the balancer.    Sixth scenario: Fast publisher, fast subscriber and a slowing down subscriber together with a balancer   A balancer component sends the message to a subscriber which is available. So in our case more messages will be sent to the fast subscriber than to the slow subscriber. As the slow subscriber gets slower, more and more messages will be processed by the fast subscriber:     This results in the following graph:    This graph shows the expected behavior. One interesting part to note is that at a certain point the publisher also starts slowing down. The reason is that the fast subscriber gets more and more messages and at a certain point can’t keep up with the publisher. At this point backpressure kicks in slows down the publisher.    Conclusions   So what can we conclude from all this? What you can see is that with reactive-streams and backpressure it is really simple to make sure that the rate of a publisher and subscriber are nicely aligned. This allows for great control of the flow, avoids memory overflows, and with the additional flow constructs it is really easy to support more advanced scenarios.   There is much more to learn about reactive-streams, akka-streams and all the various ways you can use this. What really helped me a lot understanding this, is by looking at the graphs. I think they really nicely show how backpressure (and the other constructs) affect publisher and subscribers of event streams.  ","categories": ["posts","statsd","granafa","influxdb","reactive","akka-streams","akka"],
        "tags": [],
        "url": "http://www.smartjava.org/content/visualizing-back-pressure-and-reactive-streams-akka-streams-statsd-grafana-and-influxdb/",
        "teaser":null},{
        "title": "Create a reactive websocket server with akka-streams",
        "excerpt":"In the previous article we looked at how backpressure works when working with akka-streams. Akka also uses akka-streams as the base for akka-http, with which you can easily create a HTTP server and client, where each request is processed using a stream materialized using actors. In the latest release akka-http also supports websockets. In this article we’ll show you how you can create a websockets server using akka-http.   We’ll show and explain the following subjects:       Respond to messages using a simple flow created with the flow API.    Respond to messages with a flow graph, created with the flow graph DSL.    Proactively push messages to the client by introducing an additional source to the flow.    Create a custom publisher from an Akka Actor.   When writing this article, it became a bit longer than initially planned. I’ll write a follow-up on how you can see that with websockets backpressure and rate control is also working, so watch for that one in a couple of weeks. The source files for this article can be found in the following Github Gists:      build.sbt   WSServer.scala   VMStatPublisher.scala   So first lets look at how to set up the basic skeleton of the application.   Getting started   Lets start by looking at the dependencies we need. For all the examples we use the following simple sbt file     As you can see we use the RC2 version of akka-streams and akka-http, which was released at the end of April. Creating a websocket server with akka-http is very easy, and pretty much the same way as we also did with the http server:     In this example we bind a set of handlers, more on these later, to localhost port 9001. Whenever a request comes in, we try to match it using pattern matching. To detect a WebSocket request we need to check whether the value in a specific header is an UpgradeToWebSocket message. We do this in the WSRequest extractor. So when we have a WebSocket request and a specific Uri.Path we handle the request using the specified flow. If we can’t match the request, we just return a 400. Note that the binding itself is a future, so we just wait a second for the server to start, or assume something went wrong. In our pattern matching we match four different patterns with each its own flow. In the next sections we’ll look a bit closer at each of the flows to see how they work. Lets start simple with the “/echo” flow, which also allows us to introduce some test tools.   Echo flow   Before we look at the flow lets look a bit closer at what our handler functions require. The signature for “req.header[UpgradeToWebsocket].get.handleMessages” looks like this:   def handleMessages(handlerFlow: Flow[Message, Message, Any], subprotocol: Option[String] = None)(implicit mat: FlowMaterializer): HttpResponse   As you can see this function requires a Flow with an open input which accepts a Message and an open output which also expects a message. Akka-streams will attach the created websocket as a Source and pass any sent messages from the client into this flow. Akka-streams will also use the same websocket as a Sink and pass the resulting message from this flow to it. The result from this function is a HTTPResponse that will be sent to the client.   Now lets look at the echo flow. For this flow we defined the following case:     So whenever we receive a websocket message on “/echo”, we run it through the Flows.echoFlow:     Calling Flow[Message] like this, returns a minimal flow, which just returns the message it received as is input, directly to the output. So in our case any websocket message received, is passed back to the client. Now, lets see this in action. To test this we need a websocket client. We can, of course, write one ourselves, but for now we’ll just use a chrome plugin (Simple Web Socket Client). Connect this client to http://localhost:9001/echo and send a message:      The flow configured at this endpoint responds with the same text as was entered. We didn’t really do anything with this message. Lets add some custom functionality to the flow and see what happens.   SimpleFlow   When we connect a client to “/simple” the Flows.reverseFlow is used to handle incoming websocket messages:     This time we create a simple Flow using the standard streams API:     This time we once again create a flow using Flow[Message], and on the result we call map. In the provided map function we check whether we have a standard TextMessage (we’re ignoring streaming and binary messages in this article), and if we do, we reverse the text and return a new TextMessage. The result is pretty much like you’d expect:      Anything you enter here, is returned to the client but reversed.   So far we’ve only created very simple flows, using the flow API directly. I’ve you’ve already looked a bit closer at akka-streams you probably know that there is an alternative way of creating flows. You can also use the Graph DSL, as we’ll show in the next example:   The Graph flow   With a graph flow it is very easy to create more complex message processing graphs. In the following sample we’ll show you how you can use a couple of standard flow constructs to easily process and filter incoming messages. This sample will be run when we access the server on the following endpoint:     Lets first look at the source code:     In this graph we first filter out messages we don’t want by using a collect step. This step only passes on the incoming message, when there is a match in the provided partial function. Next we send the message to a broadcast step. The broadcast step allows you to duplicate the message and send it to multiple downstream steps. In our case we send it to three downstread computer steps. The compute step in itself is just a simple Map function where we concat a number to the message. With the zip node we create a single String from the three Strings created by the compute nodes. Finally, since our flow requires a Message, we map the string to a message in the mapToMessage step. To complete this flow, we need to return a tuple with the entry and exit points of this flow. At this point, we can run the sample again:      At this point we’ve only responded to messages from the client, but didn’t push anything to the client from the server proactively. In the following sample, we’ll introduce an additional source that can push messages to the client regardless whether the client requested it.   Pushing messages to the client   One of the patterns that are matched, use the Flow.graphFlowWithExtraSource flow:     This flow ignores incoming messages, and just sends out 2000 random strings to the connected client. The complete code for this flow is shown next:     In this graph we use a Merge step to connect the source we want to use to the rest of the flow. A merge step takes an element from an upstream source when it becomes available. To make sure that we only take elements from our source (the rangeSource), we add a filter on the messages we receive from the websocket client. Besides these steps we’ve got a couple of map steps we’ve already seen before, and that makes up our flow.   When we run a message to this flow, we should see 2000 messages being pushed to the client as fast as the client can process:      Which is exactly what happens. As soon as the connection is created, 2000 messages are pushed to the client. Any messages sent from the client are ignored, as you can see in the following screenshot:      We’ve also added a small logging step to the flow. This will just print out all numbers from 1 tot 2000, to give us an idea how everything is running. At this point we’ve only used the standard components provided by akka-streams. In the next section we’re going to create a custom publisher, that pushes VM information such as memory usage to a websocket client.   Pusing messages to the client with a custom publisher   We’ll need to take a couple of steps before we can get this to work correctly, and this will involve creating a couple of agents:     We'll need a actor that forms our stream. For this we'll use an agent that together with a scheduler sends a VMStat messages at a configured interval.   In akka-streams you can't connect a new subscriber to a running publisher. To work around this we'll have the actor from step 1, send its messages to a router. This router will then broadcast the messages further to an actor that can inject them into a flow.   Finally we need the actor that connects the messages to the akka flow. For this we create an actor for each websocket request, which acts like a publisher, and passes on messages received from the router into the flow.   Lets start with the first one.   The VMActor   The VMActor is a simple actor, which, when started, sends a message every period to the provided actorRef like this:     The code for this actor isn’t that special. It’s just a basic actor which collects some information and passes it on in a map to the provided actorRef. (I know, I know, should have made that a case class…). Now, lets look at the router.   The router   For the router, initially, I wanted to use the standard BroadcastGroup router. But this router is immutable and doesn’t really allow dynamically adding new routees. So for this usecase we create a very simple alternative router, which we create like this:    Here we create both actors, and pass in the actorRef of the router to the vmactor. The implementation of this router looks like this:     The router uses the default case classes the default routers of Akka also use. The vmactor will send an update 50 times per second, with an initial delay of 2 seconds. All that is left to do is create an actor that registers itself to the router as routee and can publish to the flow.   VMStatsPublisher   Akka-streams provides an ActorPublisher[T] trait which you must use on your actors, so that they can be used as publisher inside a flow. Before we look at the implementation of this actor, first lets look at the flow that uses this actor:     If you’ve looked at the other flows, this one shouldn’t come as a suprise. It looks a lot like the previous one, only this time we define the source like this:     This means that everytime a websocket connection is made, a new VMStatsPublisher actor is created and the router actorRef is passed into the constructor. So, finally, lets look at this publisher. We will first look at the complete code of this actor and then we’ll highlight a couple of small things in the discussion afterwards:     The comments inline should give you a fairly good idea what is happening here, but lets look at a couple of items. First lets look at how we register this actor with the router:     In the preStart we register, and we must also make sure to deregister before we’re stopped. In the receive method we can receive three types of objects. We can receive stats, which we send to an internal queue, we can receive a request from downstream for more message (the Request message), or we can get Cancel message when the subscriber closes in an orderly fashion. To deliver messages to our downstream subscriber we use the onNext call in the deliver function.     As long as there is a demand (totalDemand property which is managed by akka-streams), and we’ve got messages in our queue we’ll continue sending messages. This function also outputs a console message when there is no more demand from the subscriber to this publisher. When we connect to this flow using our client we see the following in our websocket client:      Cool right! There are a couple of other topics to explore regarding websockets and akka-streams, most importantly backpressure. I’ll create a separate article on that one in the next couple of weeks to show that slow websocket clients trigger backpressure with akka-streams.  ","categories": ["posts","websockets","scala","akka","akka-streams"],
        "tags": [],
        "url": "http://www.smartjava.org/content/create-reactive-websocket-server-akka-streams/",
        "teaser":null},{
        "title": "Backpressure in action with websockets and akka-streams",
        "excerpt":"So in the previous article  I showed how you could create a websocket server using akka-streams. In this follow up article we’ll look a bit closer on how backpressure works with websockets (and probably any TCP based protocol on top of akka). To show you this we’ll use the same setup as we did in the article on visualizing backpressure. There we used the following tools:      akka-mon: Some monitoring tools for actors to send events about metrics.   Statsd: Provides an UDP API to which akka-mon can send metrics. Statsd collects these metrics and can send them to systems such as graphite and influxdb.   Influxdb: We use this to collect the various metrics.   Grafana: Grafana can visualize the data stored in influxDB. We'll use this to create line graphs that show backpressure in effect.   In this article we won’t dive too much into the details, for more information look at the previous article (todo: link).   Where we left of   Let’s quickly look at (part of) the websocket server we’ve created in the previous article:     To test backpressure, we use the /stats route, which we slighlt changed for this scenario. This route provides a set of stats through the following route:     You can see that this time we also pass in the request parameter ‘id’. We do this so that we can more easily see which flow on the server corresponds to a specific websocket client.   What we would like to see is that the actorPublisher we use here, will slow down sending messages if the client can’t keep up. Without going into the details of the actorPublisher, it used the following function to deliver its messages:     Note that it prints out when it has no more demand from the connected source (the websocket client in our case). So what we’d expect is that at a certain point we would see these kind of messages, when we have a slow client. To show whether backpressure works we’ll look at two different kinds of slow clients. Scenario one which has a fast connection, but takes excessive time processing the message, and scenario two where the message is processed immediately, but which uses a very slow connection. As a last scenario, we’ll run the first scenario, but this last time we connect a dozen clients.   Slow client 1: Client takes a lot of time processing the message   For this first scenario we use a simple scala based websocket client created using the Java Websocket library (link-to-github), which looks like this:     As you can see from this code, with can create a number of cliens at once (10 in this sample), which all connect to the websocket server. Note that we also pass on the id of the client, so we can correlate the client to the server events more easily, once we start creating the graphs with grafana and looking at the server log messages.   For our first scenario, we’ll just use a single websocket client, and see whether backpressure kicks in at the serverside. Running this with a single websocket client which consume 20 messages per second and a server which pushes 40 messages per second results in the following graph:      As you can see in this graph we process 200 msg in 10 seconds with the client, and 400 msg in 10 seconds are sent by the server. You can also see that at a certain point the server stops sending messages. This is when the backpressure kicks in. We can also see this in the log file of the server:   Adding to router: Actor[akka://websockets/user/$a/flow-3718-7-publisherSource-stageFactory-stageFactory-bypassRouter-flexiRoute-stageFactory-stageFactory-Merge-actorPublisherSource#1915924352] No more demand for 1 No more demand for 1 No more demand for 1 No more demand for 1 No more demand for 1 No more demand for 1   So, even when the client doesn’t support the whole reactive streams itself, we can still profit from reative streams. The reason this works is because the TCP stack used by Akka-streams communicates back to akka-streams when it’s buffer is filling up. When that happens the TCP stack sends a msg to the publisher that it should stop sending messages. Once the TCP buffer is empty again, new messages are requested from the publisher. This isn’t really an exact science though, since the OS settings influence how much is bufferd. E.g for my laptop it is set to this:   net.inet.tcp.doautorcvbuf: 1 net.inet.tcp.autorcvbufincshift: 3 net.inet.tcp.autorcvbufmax: 1048576 net.inet.tcp.doautosndbuf: 1 net.inet.tcp.autosndbufinc: 8192 net.inet.tcp.autosndbufmax: 1048576   I don’t know the details about the BSD network stack, but assuming the buffer for this connection fills up to the max at both the receive and send buffer it will cache a large amount of messages. If we take 5KB per message and we have a total of 2MB of buffers to fill, there can be 400 messages buffered before backpressure kicks in. This is also something you see, when you look back to the previous image. In the beginning you see the publihser pushing out messages, without interruption. This is when the OS buffers are being filled up.   Slow client 2: Slow network connection, direct message processing   In the next scenario, let’s see what happens when we’ve got a client with limited bandwidth. To simulate this, we’ll use ip_relay (http://www.stewart.com.au/ip_relay/), which is a bit old, but provides a great and easy way to shape traffic and change the bandwith, while running the examples. Lets start ip_relay:   Joss-MacBook-Pro:ip_relay-0.71 jos$ ./ip_relay.pl 9002:localhost:9001   Resolving address (localhost).....   .... determined as: 127.0.0.1 Useing command line parameters:   local_port\t9002   remote_addrs\t127.0.0.1   remote_port\t9001   bandwidth\t0   forwarder 99 set.   ip_relay.pl Version: 0.71 Copyright (C) 1999,2000 Gavin Stewart  Passive socket setup on 0.0.0.0:9002 &gt;   Now we’ve got a local port 9002, which is forwarded to another localhost:9001 where our websocket server is listening. By default bandwidth isn’t throttled:   &gt; show bandwidth bandwidth\t0 &gt;   But we can set it using the following command:   &gt; set bandwidth 1000 bandwidth\t1000   This means we now have a throttled connection from port 9002 to port 9001 with a max bandwidth of 1kb. Now we change the ws-client to connect to localhost:9002. We can also use ip_relay to check whether it is working:   &gt; sh stat   Total connections: 1   Bandwidth set to: 1000 bytes / sec.   Forwarding connections for:     127.0.0.1:56771 -&gt; 127.0.0.1:9001 (CONN000001)         Connection Up: 1 mins, 18 secs. Idle: 0 secs.         Bytes transfered: 78000 in, 163 out.         Data rate       : 0.98 kB/s in, 0.00 kB/s out.             (5 sec avg.): 0.92 kB/s in, 0.00 kB/s out.             (1 min avg.): 0.71 kB/s in, 0.00 kB/s out. &gt;   At this point we have a consumer with very limited bandwidth. Lets look at grafana and see how many messages per second it can process, and what our sender is doing:      As expected we see a very slow consumer, and a publisher which, after filling the buffer, stops sending. Now what happens when we turn the bandwidth limiter off?   &gt; set bandwidth 0 bandwidth\t0 &gt; sh stat   Total connections: 1   Bandwidth is not set.   Forwarding connections for:     127.0.0.1:56771 -&gt; 127.0.0.1:9001 (CONN000001)         Connection Up: 5 mins, 38 secs. Idle: 0 secs.         Bytes transfered: 520000 in, 163 out.         Data rate       : 1.50 kB/s in, 0.00 kB/s out.             (5 sec avg.): 23.08 kB/s in, 0.00 kB/s out.             (1 min avg.): 3.31 kB/s in, 0.00 kB/s out.   The resulting graph looks like this:      You can see here, that the client starts processing many messages at once. These are the outgoing buffered message from the server. Once these are processed, the rates of the publisher and the client align. And when we turn the limiter back on, say to 3000 bytes per second:   &gt; set bandwidth 3000 bandwidth\t3000 &gt; sh stat   Total connections: 1   Bandwidth set to: 3000 bytes / sec.   Forwarding connections for:     127.0.0.1:56771 -&gt; 127.0.0.1:9001 (CONN000001)         Connection Up: 10 mins, 18 secs. Idle: 0 secs.         Bytes transfered: 3914379 in, 163 out.         Data rate       : 6.19 kB/s in, 0.00 kB/s out.             (5 sec avg.): 7.89 kB/s in, 0.00 kB/s out.             (1 min avg.): 10.61 kB/s in, 0.00 kB/s out.   Backpressure once again kicks in (after filling the buffer):      The very nice thing about this is, that we don’t have to worry about slow clients, and slow connections hogging up resources. All messages are sent non-blocking and asynchronous, so we should be able to serve a very high number of clients, with limited CPU and memory resources.   As the last scenario, lets run scenario 1 again, but this time with multiple clients, each with there own random delay.   Slow client 2: Slow network connection, direct message processing   Lets fire up 10 web socket clients and see how the server responds. What we hope to see is that one slow client doesn’t affect the speed at which a fast client can process messages.   We configure the client like this:     This means that we start 10 clients, that have a base delay of 50 ms and addd to that is a random delay of 1 to 100. Since we have a lot of lines and data points, lets first show the websocket clients message processing rates.      As you can see here we’ve got 10 listeners now, each receiving messages from our websocket server. Each also processes messages at a different speed. Now lets see what the server side is doing.      Here we see something very interesting. We see that there is a much closer correlation to the amount of messages sent and those processed (ignore the y-axis count), than we saw earlier. We can also see that, at a certain point, the demand stops completely for the two slowest subscribers.   So what can we conclude from all this. The main points, for me at least, are:      You can use akka-streams not just within a VM, but also for backpressuring TCP calls.   This works with websockets, but this would work as well with standard HTTP calls.   You, however, need to keep in mind that on OS level, both on the receiver and sender side, you have to deal with TCP buffering. Depending on your OS, this can have a very big effect on when backpressure kicks in.   And, on a closing note, I just have to say that working with reactive streams, and more specifically akka-streams, feels like a very big step forward in creating responsive, scalable systems  ","categories": ["posts","scala","akka-streams","reactive","akka"],
        "tags": [],
        "url": "http://www.smartjava.org/content/backpressure-action-websockets-and-akka-streams/",
        "teaser":null},{
        "title": "Exploring the HTML5 Web Audio API: Filters",
        "excerpt":" 03-07-2015 Update: Updated the example, it now works with the latest versions of Chrome and Firefox    With the Web Audio API you can have advanced audio processing directly inside your browser. This API specifies a whole bunch of nodes you can use to create your audio processing pipeline. In a previous article I showed a number of these components, and how they can be used together to play back sound and visualize the results.  In this second article in the series on the Web Audio API we’re going to look a bit closer at the following subjects:      Filter node: How do you use the various filter nodes that are available.   Oscillator node: Use a tone generator as audio input.   Microphone input: Record and visualize the input from the microphone.   For those who want to directly dive into the filters, I’ve created a web application that allows you to experiment with Web Audio filters (click the image):.        The controls of this application shouldn’t be too hard. At the top you’ve got three settings:      Select the inputs that should play: Music will loop a 30 second piece of classical music, Microphone will use the microphone as input and Triangle creates a \"triangle\"wave (very noisy). More than one can be selected, they will be automatically mixed.   Configure the filter to use: By changing the dropdown and using the sliders you can configure the filters. When you hover over \"info\" you're shown information on what slider does what.   Select what output to use: You can enable each of the outputs separately. The visual ones are shown at the bottom. When enabling \"speaker\" watch out when using \"Triangle\" input.   Whenever you change something, this change is immediately reflected in the audio configuration. So by sliding the controls of the filter you can immediately see (and hear) what happens.   Using filter nodes  We won’t dive too deep into the various nodes in this article. If you want to know the details of the “Web Audio API” look at the previous article I wrote on this subject. With a filter node you can, as the name implies, filter certain parts of your audio stream. You can for instance filter out all the lower frequencies, or make the bass more pronounced. This API offers the following set of filters:      LOWPASS:  A lowpass filter allows frequencies below the cutoff frequency to pass through and attenuates frequencies above the cutoff. LOWPASS implements a standard second-order resonant lowpass filter with 12dB/octave rolloff.   HIGHPASS:  A highpass filter is the opposite of a lowpass filter. Frequencies above the cutoff frequency are passed through, but frequencies below the cutoff are attenuated. HIGHPASS implements a standard second-order resonant highpass filter with 12dB/octave rolloff.   BANDPASS: A bandpass filter allows a range of frequencies to pass through and attenuates the frequencies below and above this frequency range. BANDPASS implements a second-order bandpass filter.   LOWSHELF:  The lowshelf filter allows all frequencies through, but adds a boost (or attenuation) to the lower frequencies. LOWSHELF implements a second-order lowshelf filter.   HIGHSHELF: The highshelf filter is the opposite of the lowshelf filter and allows all frequencies through, but adds a boost to the higher frequencies. HIGHSHELF implements a second-order highshelf filter   PEAKING: The peaking filter allows all frequencies through, but adds a boost (or attenuation) to a range of frequencies.   NOTCH: The notch filter (also known as a band-stop or band-rejection filter) is the opposite of a bandpass filter. It allows all frequencies through, except for a set of frequencies.   ALLPASS: An allpass filter allows all frequencies through, but changes the phase relationship between the various frequencies. ALLPASS implements a second-order allpass filter   You create a filter like this:    var filter = context.createBiquadFilter(); filter.type = 3;  // In this case it's a lowshelf filter filter.frequency.value = 440; filter.Q.value = 0; filter.gain.value = 0;   And just like all the other nodes, you use “connect” to add this to the audio processing pipeline. And basically that’s all there is to using filters. The following screenshot show the output of the spectrometer when using a LOWPASS filter. In this example I’ve used a “Triangle” source. A Triangle is build up of a number of frequencies, so nicely allows us to see what happens when we start playing around with filters. To get this effect I’ve moved the frequency slider up and down, and as you can see, certain highter frequencies are blocked by the filter.      If we look at the HIGHPASS filter we see exactly the opposite when sliding the frequency up and down.      Interesting stuff, right! If you want to know more about the filters look at the BiquadFilterNode section in the W3 API, or experiment with the filter playground I created.    Oscillator node   In the previous section we used a triangle as our waveform. We can create this waveform (and some others) directly from the Web Audio API.    var osc = context.createOscillator(); osc.frequency.value=600; osc.type=2;   Here frequency is the frequency of the wave the oscillator should produce, and type defines the waveform.   const unsigned short SINE = 0; const unsigned short SQUARE = 1; const unsigned short SAWTOOTH = 2; const unsigned short TRIANGLE = 3; const unsigned short CUSTOM = 4;   This results in the following wave forms:   Sine:    Square:    Sawtooth:    Triangle:    The final one, which I didn’t show, is the custom one. Here you can supply a Wavetable, which is a set of sine terms and cosine terms that together make up the waveform.   So, enough for the oscillator node. Final item on the list is how to use your microphone as input.   Microphone input   In a previous article on capturing audio in the browser, I already showed you how you can access the microphone. To use it in the Web Audio API we only need to take a couple of steps more. Before we look at how to do this, first we have to make sure chrome can access your microphone. Go to “chrome://flags” and make sure the following is enabled:      To use the microphone as input all we need to do now is the following:    navigator.webkitGetUserMedia({audio:true},function(stream) {             mediaStreamSource = context.createMediaStreamSource(stream);             mediaStreamSource.connect(filter);         });   We use the getUserMedia call (prefixed for chrome) to ask access to the microphone. Once we get it, we connect this source, just like we would any other source.   That’s it for this part on the Web Audio API. So start playing around with the filter playground and let me know if you’ve got any questions.  ","categories": ["posts","webrtc","chrome","html5","webaudio"],
        "tags": [],
        "url": "http://www.smartjava.org/content/exploring-html5-web-audio-api-filters/",
        "teaser":null},{
        "title": "Easy validation in Scala using Scalaz, Readers and ValidationNel",
        "excerpt":"I’m working on a new book for Packt which shows how you can use various Scala frameworks  to create REST services. For this book I’m looking at the following list of frameworks: Finch, Unfiltered, Scalatra, Spray and Akka-HTTP (reactive streams).  While writing the chapters on Finch and Unfiltered I really liked the way these two frameworks handled validation. They both use composable validators which allow you to create easy to use validation chains, that collect any validation errors.   In this article I’ll show you how you can use such an approach yourself using the Scalaz library, and more specifically on how the Reader monad and the Validation applicative can be combined to create flexible, compassable validators, that accumulate any validation errors.  As an example we’ll create a number of Readers that you can use to process an incoming HTTPRequest, get some parameters and convert it to a case class.   Getting started   The complete example can be found in the following Gist. All the code fragments in this article, are shown directly from that Gist. First, though, for this project we of course use SBT. Since we’ll only be using Scalaz, we have a very simple SBT build file:     To work with Scalaz, you can either import the required packages and functions individually, or, as we do in this case, be lazy and just import everything. So at the top of our example we’ll add the following:     Before we’ll look at the readers themselves, lets first define our case class and some type aliases to make things easier to understand:     The Reader monad   Before we start creating our own readers, lets look a bit closer at what a reader monad allows you to do. We’ll look at the example from here (great introduction into Scalaz): http://eed3si9n.com/learning-scalaz-day10   scala&gt; def myName(step: String): Reader[String, String] = Reader {step + \", I am \" + _} myName: (step: String)scalaz.Reader[String,String]   scala&gt; def localExample: Reader[String, (String, String, String)] = for {          a &lt;- myName(\"First\")          b &lt;- myName(\"Second\") &gt;=&gt; Reader { _ + \"dy\"}          c &lt;- myName(\"Third\")          } yield (a, b, c) localExample: scalaz.Reader[String,(String, String, String)]   scala&gt; localExample(\"Fred\") res0: (String, String, String) = (First, I am Fred,Second, I am Freddy,Third, I am Fred)   The point of a reader monad is to supply some configuration object (for instance a HTTPRequest, a DB, or anything else you can inject) without having to manually (or implicitly) pass it around all the functions. As you can see from this example we create three Readers (by calling myName) and pass in the request just once to the composed result. Note that for comprehension only works when all the readers are of the same type. In our case we have strings and ints so we use a somewhat different syntax to compose the readers as we’ll see later. The basic idea, however, is the same. We define a number of readers, and pass in the request to be processed.   Our Readers   We’ve defined our readers in a helper object, to make using them a bit easier.  First let’s look at the complete code:     What we do here is define a single as[T] method with an implicit to method, that returns a RequestReader of the specified type. Through the use of implicits scala will use one of the asString, AsInt etc. methods to determine how to convert the passed in key to a correct reader. Let’s look a bit closer at the asInt  and the keyFromMap function.     The asInt function creates a new Reader and uses the request that is passed in to call the keyFromMap function. This function tries to get the value from the Map, and if it is successful it return a Success, if not a Failure. Next we flatMap this result (only applies when the result is a Success) and try to convert the found value to an integer using the scalar provided parseInt function, which in turn also returns a Validation. The result from this function is passed on to the toMessage function which transforms a Validation[Throwable, S] to a Validation[String, s]. Finally, before returning, we use the toValidationNel function to convert the Validation[String, Int] to a ValidationNel[String, Int]. We do this so that we can more easily collect all the failures together.   Creating a new validation, just means creating a new reader that returns a ValidationNel[String, T].   Composing validations   Now lets look at how we can compose the validations together. To do this we can use the ApplicativeBuilder from Scalaz like this:                    Using the       @       symbol we combine our readers using the scalaz ApplicativeBuilder. Using the tupled function we return a list of tuples containing the individual results of our readers, when they’ve run. To run this reader, we need to supply it with a request:             The result of these calls look like this:   tuple3Invalid:  (Success(Amber),Failure(NonEmptyList(Key: last Not found)),Failure(NonEmptyList(Exception: class java.lang.NumberFormatException msg: For input string: \"20 Months\")))  tuple3valid:  (Success(Sophie),Success(Dirksen),Success(5))    Even though this approach already allows us to create and compose validations and return the individual successes and failures, it will still take some work to get either the failures or convert the values to our case class. Luckily though, we can also easily collect the success and failures, since we’re using the ValidationNel object:                    When this reader is run each individual validation will be applied, and passed into the provided apply function. In this function we collect the validations using the       @       constructor. This will return a Failure containing the collected errors, or an instantiated person if all the validations are successful:             Which results in this:   personInvalid:  Failure(NonEmptyList(Key: last Not found, Exception: class java.lang.NumberFormatException msg: For input string: \"20 Months\"))  personValid:  Success(Person(Sophie,Dirksen,5))    Cool right! This way we either get a success containing our domain object, or a single Failure object contain a list of errors.                  The final part I’d like to show is an alternative way of collecting the validations. In the previous example we used the       @       syntax,  you can also directly create an Applicative and use it to collect the validations:             And the output of this function is this:   applicativeInvalid:  Failure(NonEmptyList(Key: last Not found, Exception: class java.lang.NumberFormatException msg: For input string: \"20 Months\")) applicativeValid:  Success(Person(Sophie,Dirksen,5))   And that’s it. To summarise the main points:      Using the reader pattern it is very easy to pass some context into functions for processing.    With the applicative builder or the |@| symbol, you can easily compose readers that continue even if the first one should fail.   By using a validationNel we can easily collect various validation results, and either return the collected errors, or return a single success.  ","categories": ["posts","monad","scala","scalaz"],
        "tags": [],
        "url": "http://www.smartjava.org/content/easy-validation-scala-using-scalaz-readers-and-validationnel/",
        "teaser":null},{
        "title": "Scala typeclass explained: Implement a String.read function",
        "excerpt":"In this short article I’d like to explain how you can use the typeclass pattern in Scala to implement adhoc polymorphism. We won’t go into too much theoretical details in this article, the main thing I want to show you is how easy is really is to create a flexible extensible model without tying your domain model to specific implementation or traits. There are many resources regarding scala typeclasses available on the internet, but I couldn’t find a good reference, so that’s why I created this one. In this example we’ll look at a very pragmatic (and naive) implementation of the read typeclass from haskell. This type class allows you to convert a string to a specific type in a generic way.   What we want to accomplish with this typeclass is the following (The complete example can be found here: https://gist.github.com/josdirksen/9051baf09003dac37386)     With the Readable typeclass we provide a generic way to convert a String to a specific type. In the example above we used the type class to convert a String to some basic types, but also to different lists and a specific case class. The functionality for the basic types isn’t really that useful, since the scala String object already provides the toDouble, toString, etc. functions. This way, however, you don’t need to know the specific function to call, but you can just specify the target type you want :) It, however, gets much more interesting with the Task case class you see here. As you’ll see in the rest of the code, through the use of implicits we can simply add support for this case class, without having to change the implementation of either the String class or the Task class.   To implement a typeclass we first have to define the trait that defines the functions we need to implement. In this sample, we have only one function:     The read function should just transform the String to the specified type T. Now that we’ve defined our trait, lets look at the companion object, which contains some helper classes and simple implementations:     The code shouldn’t be that hard to understand. What we do here, is we create a number of Readable implementations. It’s important to note that we use the implicit parameter, so that we can pull them into scope later on. At this point we can already start using the typeclass. For that we import the implicits and use the Readable type class like this:     Easy right? Scala will look for an implicit that matches the specified type and convert the String to that type. This is nice, but doesn’t really look that nice. We need to instantiate a Readable (even though that isn’t too much code) and call read to convert the String. We can make it easier by extending the String class with a read function that does the conversion for us. For this add the following to the Readable companion object.     These operation define an implicit transformation which add the read function the String object. All that is left is to import the ops implicit functions and we can use the read directly:     This same approach can also be used to create a read function for case classes:     And that’s pretty much it. By just implementing the Readable[Task] trait our custom Task case class can be processed in the same manner as the other objects.   ","categories": ["posts","type class","example","typeclass","scala"],
        "tags": [],
        "url": "http://www.smartjava.org/content/scala-typeclass-explained-implement-stringread-function/",
        "teaser":null},{
        "title": "Listen to notifications from Postgresql with Scala",
        "excerpt":"In the past I’ve written a couple of articles (Building a REST service in Scala with Akka HTTP, Akka Streams and reactive mongo and ReactiveMongo with Akka, Scala and websockets) which used MongoDB to push updates directly from the database to a Scala application. This is a very nice feature if you just want to subscribe your application to a list of streaming events where it doesn’t really matter if you miss one when your application is down. While MongoDB is a great database, it isn’t a right fit for all purposes. Sometimes you want a relational database, with a well defined schema, or a database that can combine the SQL and noSQL worlds. Personally I’ve always really liked Postgresql. It’s one of the best relational databases, has great GIS support (which I really like a lot), and is getting more and more JSON/Schema-less support (which I need to dive into sometime).  One of the features I didn’t know about in Postgresql was that it provides a kind of subscribe mechanism. I learned about that when reading the “Listening to generic JSON notifications from PostgreSQL in Go” article which shows how to use this from Go. In this article we’ll try to see what you need to do, to get something similar working in Scala (approach for Java is pretty much the same).   How does this work in Postgresql   It is actually very easy to listen to notifications in Postgresql. All you have to do is the following:   LISTEN virtual; NOTIFY virtual; Asynchronous notification \"virtual\" received from server process with PID 8448. NOTIFY virtual, 'This is the payload'; Asynchronous notification \"virtual\" with payload \"This is the payload\" received from server process with PID 8448.   The connection that wants to listen to events calls LISTEN with the name of the channel it wants to listen on. And the sending connection just runs NOTIFY with the name of the channel, and a possible payload.   Preparing the database   The cool thing from the article on Go I mentioned in the introduction is that it provides a stored procedure which automatically sends a notification whenever a table row is INSERTed, UPDATEd, or DELETEd. The following, taken from Listening to generic JSON notifications from PostgreSQL in Go create a stored procedure which sends notifications when called.     The really cool thing about this stored procedure, is that the data is converted to JSON, so we can easily process it in our application. For this example I’ll use the same tables and data used in the Go article, so first create a table:   CREATE TABLE products (   id SERIAL,   name TEXT,   quantity FLOAT );   And create a trigger whenever something happens to the table.   CREATE TRIGGER products_notify_event AFTER INSERT OR UPDATE OR DELETE ON products     FOR EACH ROW EXECUTE PROCEDURE notify_event();   At this point, whenever a row is inserted, updated or deleted on the products table, a notify event is created. We can simply test this by using the pgsql command line:   triggers=# LISTEN events; LISTEN triggers=# INSERT INTO products(name, quantity) VALUES ('Something', 99999); INSERT 0 1 Asynchronous notification \"events\" with payload \"{\"table\" : \"products\", \"action\" : \"INSERT\", \"data\" : {\"id\":50,\"name\":\"Something\",\"quantity\":99999}}\" received from server process with PID 24131. triggers=#    As you can see, the INSERT resulted in an asynchronous event which contains the data. So, so far we’ve pretty much followed the steps also outlined in the Go article. Now lets look at how we can access the notifications from Scala.    Accessing notifications from Scala   First lets setup our project’s dependencies. As always we use SBT. The build.sbt for this project looks like this:     A quick summary of the depencies:     scalikeJDBC: This project provides an easy to use wrapper around JDBC, so we don't have to use the Java way of connection handling and stuff.   akka: We use the Akka framework to managed the connection with the database. Since the JDBC driver isn't asynchronous ar can push data, we need to set an interval.   json4s: This is just a simple Scala JSON library. We use this to quickly convert the incoming data into a simple case class.   We’ll first show you the complete source code for this example, and then explain the various parts:     If you’re familiar with Akka and with scalikeJDBC the code will look familiar. We start with some general setup stuff:     Here we define our case class to which we’ll transform the incoming JSON, setup a connection pool, define the Akka-System and start our Poller actor. Nothing too special here, the only thing special is on line 23. To add a listener from Scala we need access to the underlying JDBC Connection. Since scalikeJDBC uses connection pooling, we need to explicitly call setAccessToUnderlyingConnectionAllowed to make sure we’re allowed to access the actual connection when we call getInnerMostDelegate, and not just wrapped one from the connection pool. Interesting to note here, is that if we don’t set this, we don’t get an error message or anything, we just get a Null from this method call….   With this out of the way, and our Actor started, lets see what it does:     The first thing we do in our actor is set some properties needed by scalikeJDBC, and setup an timer which fires a message each 500 ms. Also note the preStart and postStop functions. In the preStart we execute a small piece of SQL, which tells postgres that this connection will be listening to notifications with the name “events”. We also set DB.autoClose to falls, to avoid the session pooling mechanism closing the session and connection. We want to keep these alive, so we can receive events. When the actor is terminated we make sure to clean up the timer and connection.   In the receive function we first get the real PGConnection and then get the notifications from the connection:     If there a no notification Null will be returned, so we wrap this in an Option, and just return an empty array in the case of Null. If there are any notification we just process them in a foreach loop and print out the result:     Here you can also see that we just get the “data” element from the notification, and convert it to our Product class for further processing. All you have to do now is start the application and from the same pgsql terminal add some events. If all went well, you’ll see output similar to this in your console:   Received for: events from process with PID: 24131 Received data: {\"table\" : \"products\", \"action\" : \"INSERT\", \"data\" : {\"id\":47,\"name\":\"pen\",\"quantity\":10200}}  Received as object: Product(47,pen,10200) Received for: events from process with PID: 24131 Received data: {\"table\" : \"products\", \"action\" : \"INSERT\", \"data\" : {\"id\":48,\"name\":\"pen\",\"quantity\":10200}}  Received as object: Product(48,pen,10200) Received for: events from process with PID: 24131 Received data: {\"table\" : \"products\", \"action\" : \"INSERT\", \"data\" : {\"id\":49,\"name\":\"pen\",\"quantity\":10200}}  Received as object: Product(49,pen,10200) Received for: events from process with PID: 24131 Received data: {\"table\" : \"products\", \"action\" : \"INSERT\", \"data\" : {\"id\":50,\"name\":\"Something\",\"quantity\":99999}}  Received as object: Product(50,Something,99999)   Now that you’ve got this basic construct working it’s trivial to use this, for instance, as a source for reactive streams, or just use websockets to further propagate these events.   ","categories": ["posts","akka","scala","postgresql"],
        "tags": [],
        "url": "http://www.smartjava.org/content/listen-notifications-postgresql-scala/",
        "teaser":null},{
        "title": "Akka Typed: First steps with typed actors in Scala",
        "excerpt":"With the release of Akka 2.4.0 a couple of weeks ago the experimental Akka Typed module was added. With Akka Typed it is possible to create and interact with Actors in a type safe manner. So instead of just sending messages to an untyped Actor, with Akka Typed we can add compile time type checking to our Actor interactions. Which of course is a great thing! In this first article on Akka Typed we’ll look at a couple of new concepts of Akka Typed and how you can create and communicate with Actors in this new way. The complete code used in this article can be found in this Gist.   As always lets quickly show the SBT build file:  name := \"akka-typed\"  version := \"1.0\"  scalaVersion := \"2.11.7\"  libraryDependencies += \"com.typesafe.akka\" %% \"akka-typed-experimental\" % \"2.4.0\"   This wil pull in the required main Akka libraries and the new Akka Typed way of creating Actors. Once added we can create our first actor.  One of the biggest differences is that we don’t explicitly create Actors anymore, but we define Behavior and from that behaviour we create an actor. To make it easy for you, Akka Typed comes with a lot of helper classes to make defining Behavior easier. In this article we’ll introduce a number of these.   Simple static Behavior   First lets look at the code for a simple static actor. Note that we show some additional case classes to make the examples a little bit more useful:     We first define some simple implicits and a set of case classes which we’ll send to our Actor. The actual Behavior  is defined using the Static case class. A Static Behavior, as the name implies, provides a non-changing actor which always executes in the same manner. In this case we just print out the message we received. To create an actor from this Behavior , we initiate a new ActorSystem. This ActorSystem can be seen as the root actor and we can send messages to it. Sending messages happens in the same manner, and the result from this piece of code is the following:   Step 1: Using static Actor Msg received:HelloCountry(Netherlands) Msg received:HelloWorld() Msg received:HelloCity(Waalwijk) Msg received:Hello(HelloHelloHello)   Note that with a Static actor we don’t handle any lifecycle signals, we just process the incoming message, and handle it in the same manner for each and every request.   Using the ask pattern   Another important pattern of Akka actors is the ability to use the ask pattern. With the ask pattern we send a message to an actor and get a Future[T] that contains the response.  In Akka Typed you use the following approach for this:     For this message we use an additional case class. This case class not just contains the message, but also an actorRef to which to respond. The reason this is done, is because in Akka Typed we can’t access the sender directly in the actor, so we need to pass it in, in the incoming message. Our actor Behavior is very straightforward. We just send a new message back to the passed in actor through its actorRef. The interesting thing here, that the request as well as the response are typed.  To ask something of an actor we use the familiar ‘?’ operation, and since we added a replyTo field to our case class, we pass in the anonymous actor that is created when we use the ? function. The ? operation returns a Future[HelloMsg], on which we just wait in this example.  When we run this example we get the following:   Step 2: Using reply Actor Response recevied: Hello(You said: Hello )   Switching out actor behavior   One of the cool things about actors is that they can switch behavior based on  processed messages. This works very nice for implementing state machines, protocol handlers etc. With Typed Actors we can of course also accomplish this. Lets first look at the code for this example:     In this code fragment we use the Total[T] case class to implement the switching behavior. With a Total case class we define the behavior that needs to be executed, when a message is processed. Besides that we also need to return the new Behavior that will execute when the next message is received. In this example we switch two different behaviors. The first one prints out everything in uppercase, and the other one in lowercase. So the first message will be printed in complete lowercase, the second in uppercase and so on.   This results in the following output:  in total function: hellocountry(netherlands) IN TOTAL FUNCTION: HELLOWORLD() in total function: hellocity(waalwijk) IN TOTAL FUNCTION: HELLO(HELLOHELLOHELLO)   Using the Full behavior   So far we’ve only looked at how to process messages. Besides processing messages, some behaviours might also need to respond to lifecycle events. In the traditional way this would mean overriding specific lifecycle functions. With Akka Typed, however, we don’t have access to these functions anymore. Lifecycle events are delivered to a behavior in the same way as normal messages. If you need direct access to these, you can use a different way to construct your behavior. One of the options is to use the Full[T] class for this:     As you can see by using the Full[T] class we get the message or the signal in a MessageOrSignal envelop, which we can process in the same way as we would do normally. Note that we’ve also added a decorator around this actor.  Using the ContextAware decorator we  can make the actor context available to the behavior (we don’t use it here any further though).   The output from these message looks like:   We can access the context: akka.typed.ActorContextAdapter@ee559a3 Recevied messageOrSignal: Sig(akka.typed.ActorContextAdapter@ee559a3,PreStart) Recevied messageOrSignal: Msg(akka.typed.ActorContextAdapter@ee559a3,HelloCountry(Netherlands)) Recevied messageOrSignal: Msg(akka.typed.ActorContextAdapter@ee559a3,HelloWorld()) Recevied messageOrSignal: Msg(akka.typed.ActorContextAdapter@ee559a3,HelloCity(Waalwijk)) Recevied messageOrSignal: Msg(akka.typed.ActorContextAdapter@ee559a3,Hello(HelloHelloHello))  // when the system is shutdown Recevied messageOrSignal: Sig(akka.typed.ActorContextAdapter@ee559a3,PostStop)   As you can see we receive either a message or a signal.   Combining behaviors   For the last example in this article, we’ll have a quick look on how to combine behaviors together to create new ones. For this Akka Typed introduces the following two operators:      &amp;&amp;: Sends the message to both behaviours.                                               : First sends the message to the left behavior, if that behavior returns an unhandled response, the right side is tried.                           In code this looks like this:                   The &amp;&amp; system is very straightforward, and we just reuse existing behaviors. For the               functionality we combine a new Behavior, which just always returns Unhanded, so should pass all messages to the fullBehavior behavior.           The result for the &amp;&amp; operator looks like this:  Msg received:HelloCountry(Netherlands) in total function: hellocountry(netherlands) Msg received:HelloWorld() IN TOTAL FUNCTION: HELLOWORLD() Msg received:HelloCity(Waalwijk) in total function: hellocity(waalwijk) Msg received:Hello(HelloHelloHello) IN TOTAL FUNCTION: HELLO(HELLOHELLOHELLO)  As you can see the message is processed by both behaviors. For the || operator the output looks like this:  We can access the context: akka.typed.ActorContextAdapter@4cba0952 Recevied messageOrSignal: Sig(akka.typed.ActorContextAdapter@4cba0952,PreStart) Can't handle it here! Recevied messageOrSignal: Msg(akka.typed.ActorContextAdapter@4cba0952,HelloCountry(Netherlands)) Can't handle it here! Recevied messageOrSignal: Msg(akka.typed.ActorContextAdapter@4cba0952,HelloWorld()) Can't handle it here! Recevied messageOrSignal: Msg(akka.typed.ActorContextAdapter@4cba0952,HelloCity(Waalwijk)) Can't handle it here! Recevied messageOrSignal: Msg(akka.typed.ActorContextAdapter@4cba0952,Hello(HelloHelloHello))   Here, we can see that after a “Can’t handle it here!” message the right side of the operator takes over.   first conclusions   This was just a quick first look at Typed Actors. For me it felt like a very nice way of creating actor systems so far. It feels very intuitive and the fact that the requests and responses both can  be types will most likely result in more secure code. In a future article we’ll get back to this subject.  ","categories": ["posts","akka","scala"],
        "tags": [],
        "url": "http://www.smartjava.org/content/akka-typed-first-steps-typed-actors-scala/",
        "teaser":null},{
        "title": "Akka Typed Actors: Exploring the receiver pattern",
        "excerpt":"In the previous article we looked at some of the basic features provided by Akka Typed. In this article and the next one we’ll look a bit closer at some more features and do that by looking at the two different patterns provided by Akka Typed: the Receiver and the Receptionist pattern. If you’re new to Akka Typed, it might be a good idea to first read the previous article, since that’ll give you a bit of an introduction into Akka Typed.  So for this article in our series on akka-typed we’ll look at the Receiver pattern.   As always, you can find the code for this example in a Github Gist: https://gist.github.com/josdirksen/77e59d236c637d46ab32   The receiver pattern   In the Akka Typed distribution there is a package call akka.typed.patterns. In this package there are two different patterns the Receiver pattern and the Receptionist pattern. Why these two patterns were important enough to add to the distribution I don’t really know to be honest, but they do provide a nice way to introduce some more concepts and ideas behind Akka Typed.   So let’s look into the Receiver pattern and we’ll do the Receptionist pattern in the next article. To understand what the Receiver pattern doen, lets just look at the messages that we can send to it:    /**    * Retrieve one message from the Receiver, waiting at most for the given duration.    */   final case class GetOne[T](timeout: FiniteDuration)(val replyTo: ActorRef[GetOneResult[T]]) extends Command[T]   /**    * Retrieve all messages from the Receiver that it has queued after the given    * duration has elapsed.    */   final case class GetAll[T](timeout: FiniteDuration)(val replyTo: ActorRef[GetAllResult[T]]) extends Command[T]   /**    * Retrieve the external address of this Receiver (i.e. the side at which it    * takes in the messages of type T.    */   final case class ExternalAddress[T](replyTo: ActorRef[ActorRef[T]]) extends Command[T]   As you can see from these messages what a Receiver does is that it queues messages of type T, and provides additional commands to either get one or more of those messages, while waiting a specific time. To use a receiver we need to get the ExternalAddress, so that we can send messages of type T to it.  And from an other actor we can send get GetOne and GetAll messages to see whether there are any messages waiting in the receiver.   For our example we’re going to create the following actors:      \tA producer which sends messages of type T to the receiver.   \tA consumer which can retrieve messages of type T from this receiver.   \tA root actor, which runs this scenario.   We’ll start with the producer, which looks like this:     In this object we define the messages that can be sent to the actor, and the behavior. The registerReceiverMsgIn message provides the actor with the destination it should send  messages to (more on this later), and the addHelloWorldMsg tells the behavior what message to send to the address provided by the registerReceiverMsgIn message. If you look at this behavior you can see that we use a Full[T] behavior. For this behavior we have to provide matchers for all the messages and signals, and as an added bonus we also get access to the actor ctx. In its initial state this behavior only responds to registerReceiverMsgIn messages. When it receives such a message it does two things:      \tIt defines a function which we can use to schedule a message, we we also directly call, to schedule a message being sent in half a second.   \tIt defines our new behavior. This new behavior can process the messages sent by the scheduleMessage function. When it receives that message, it sends the content to the provided messageConsumer (the Receiver), and calls the schedule message again. To keep sending messages every 500 ms.   So when we sent the initial registerReceiverMessage, it will result in an actor that sends a new message to the receiver every 500 ms. Now lets look at the other side: the consumer.   For the consumer we’ve also wrapped everything in an object, which looks like this:     In this object we defines a single behavior, which also switches its implementation after receiving the first message. The first message in this case is called registerReceiverCmdIn. With this message we get access to the actorRef (of the Receiver) that we need to send the GetAll and getOne messages to. After we’ve switched behavior, we process our own custom GetAllMessages message, which will trigger a GetAll message being sent to the Receiver. Since our own behavior isn’t typed for the kind of responses received from the Receiver, we use an adapter (ctx.spawnAdapter). This adapter will receive the response from the Receiver and print out the messages.   The final message part is an actor which initiates this behavior:     Nothing to special here. We create the various actors in this scenario and use the ctx.spawnAdapter to get the external address of the receiver, which we pass to the producerActor. Next we pass the address of the receiver actor to the consumer. Now we call the GetAllMessages on the consumer address which gets the messages from the receiver and prints them out.   So summarising the steps that will be executed in this example:      We create a root actor that will run this scenario.   From this root actor we create the three actors: receiver, consumer and producer.   Next we get the externalAddress from the receiver (the address to which we sent messages of type T) and using an adapter pass this to the producer.   The producer, on receiving this message, switches behavior and starts sending messages to the passed in address.   The root actor, in the meantime, passes the address of the Receiver to the consumer.   The consumer ,when it receives this messages, changes behavior and now waits for  messages of the type GetAllMessages.    The root actor will now send a GetAllMessages to the consumer.   When the consumer receives this messages it will use an adapter to send a GetAll message to the receiver. When the adapter receive a response it prints out the number of messages received, and handles further processing to the consumer by sending a PrintMessage for each received message from the receiver.   And the result of this scenario looks like this:   Scenario1: Started, now lets start up a number of child actors to do our stuff Scenario1: Get all the messages Consumer: Switching behavior Consumer: requesting all messages Producer: Switching behavior Producer: Adding new 'Hello(hello @ 1446277162929)' to receiver: Actor[akka://Root/user/receiver#1097367365] Producer: Adding new 'Hello(hello @ 1446277163454)' to receiver: Actor[akka://Root/user/receiver#1097367365] Producer: Adding new 'Hello(hello @ 1446277163969)' to receiver: Actor[akka://Root/user/receiver#1097367365] Consumer: Received 3 messages Consumer: Printing messages: Vector(Hello(hello @ 1446277162929), Hello(hello @ 1446277163454), Hello(hello @ 1446277163969))   Hello(hello @ 1446277162929)   Hello(hello @ 1446277163454)   Hello(hello @ 1446277163969) Producer: Adding new 'Hello(hello @ 1446277164488)' to receiver: Actor[akka://Root/user/receiver#1097367365] Producer: Adding new 'Hello(hello @ 1446277165008)' to receiver: Actor[akka://Root/user/receiver#1097367365] Consumer: requesting all messages Producer: Adding new 'Hello(hello @ 1446277165529)' to receiver: Actor[akka://Root/user/receiver#1097367365] Producer: Adding new 'Hello(hello @ 1446277166049)' to receiver: Actor[akka://Root/user/receiver#1097367365] Producer: Adding new 'Hello(hello @ 1446277166569)' to receiver: Actor[akka://Root/user/receiver#1097367365] Producer: Adding new 'Hello(hello @ 1446277167089)' to receiver: Actor[akka://Root/user/receiver#1097367365] Consumer: Received 6 messages Consumer: Printing messages: Vector(Hello(hello @ 1446277164488), Hello(hello @ 1446277165008), Hello(hello @ 1446277165529), Hello(hello @ 1446277166049), Hello(hello @ 1446277166569), Hello(hello @ 1446277167089))   Hello(hello @ 1446277164488)   Hello(hello @ 1446277165008)   Hello(hello @ 1446277165529)   Hello(hello @ 1446277166049)   Hello(hello @ 1446277166569)   Hello(hello @ 1446277167089) Producer: Adding new 'Hello(hello @ 1446277167607)' to receiver: Actor[akka://Root/user/receiver#1097367365] Producer: Adding new 'Hello(hello @ 1446277168129)' to receiver: Actor[akka://Root/user/receiver#1097367365] Producer: Adding new 'Hello(hello @ 1446277168650)' to receiver: Actor[akka://Root/user/receiver#1097367365] Producer: Adding new 'Hello(hello @ 1446277169169)' to receiver: Actor[akka://Root/user/receiver#1097367365] Producer: Adding new 'Hello(hello @ 1446277169690)' to receiver: Actor[akka://Root/user/receiver#1097367365] Producer: Adding new 'Hello(hello @ 1446277170210)' to receiver: Actor[akka://Root/user/receiver#1097367365] Consumer: requesting all messages Producer: Adding new 'Hello(hello @ 1446277170729)' to receiver: Actor[akka://Root/user/receiver#1097367365] Producer: Adding new 'Hello(hello @ 1446277171249)' to receiver: Actor[akka://Root/user/receiver#1097367365] Producer: Adding new 'Hello(hello @ 1446277171769)' to receiver: Actor[akka://Root/user/receiver#1097367365] Producer: Adding new 'Hello(hello @ 1446277172289)' to receiver: Actor[akka://Root/user/receiver#1097367365] Consumer: Received 10 messages Consumer: Printing messages: Vector(Hello(hello @ 1446277167607), Hello(hello @ 1446277168129), Hello(hello @ 1446277168650), Hello(hello @ 1446277169169), Hello(hello @ 1446277169690), Hello(hello @ 1446277170210), Hello(hello @ 1446277170729), Hello(hello @ 1446277171249), Hello(hello @ 1446277171769), Hello(hello @ 1446277172289))   Hello(hello @ 1446277167607)   Hello(hello @ 1446277168129)   Hello(hello @ 1446277168650)   Hello(hello @ 1446277169169)   Hello(hello @ 1446277169690)   Hello(hello @ 1446277170210)   Hello(hello @ 1446277170729)   Hello(hello @ 1446277171249)   Hello(hello @ 1446277171769)   Hello(hello @ 1446277172289) Producer: Adding new 'Hello(hello @ 1446277172808)' to receiver: Actor[akka://Root/user/receiver#1097367365] Producer: Adding new 'Hello(hello @ 1446277173328)' to receiver: Actor[akka://Root/user/receiver#1097367365] Producer: Adding new 'Hello(hello @ 1446277173849)' to receiver: Actor[akka://Root/user/receiver#1097367365] Producer: Adding new 'Hello(hello @ 1446277174369)' to receiver: Actor[akka://Root/user/receiver#1097367365]   Cool right! As you can see from the message sequence, our producer sends messages to the receiver which queues them up. Next we have a consumer which requests all the messages that have been received so far and prints them out.   That’s it for this article on Akka-Typed, in the next one we’ll look at the Receptionist pattern also present in Akka-Typed.  ","categories": ["posts","actors akka-typed","akka","scala"],
        "tags": [],
        "url": "http://www.smartjava.org/content/akka-typed-actors-exploring-receiver-pattern/",
        "teaser":null},{
        "title": "Two-way SSL (client certificates) with Scalatest",
        "excerpt":"At work we recently added the option to authenticate machine to machine communication using client certificates (two-way ssl). While this was relatively easy to set up and access programatically from different programming languages, we ran into some difficulties getting our integration test up and running. We wanted to have a couple of tests to make sure the information from the certificate was correctly parsed and mapped to an internal client id, and how the system reacts to invalid certificates and a couple of other edge cases.   Since we use Scalatest for all our integration testing we just wanted to add the private keystore and the trust store to scalatest and be done with it. However, the standard fluent API provided by Scalatest doesn’t offer it (or we couldn’t find it). After some looking around we came to the following setup which works for us in our integration test suite:     Note that some of these classes are actually deprecated, and could be replaced with other relevant classes from the apache commons library used here.  ","categories": ["posts","scalatest","scala","ssl"],
        "tags": [],
        "url": "http://www.smartjava.org/content/two-way-ssl-client-certificates-scalatest/",
        "teaser":null},{
        "title": "Akka Typed Actors: Exploring the receptionist pattern",
        "excerpt":"In this article we’ll explore another of Akka-Typed patterns. This time we’ll show you how you can use the receptionist patterns. This is the third and last article on a series on Akka-Typed. The other two articles can also be found on this site. If you don’t know anything about Akka-Typed yet, it’s a good idea to first read the “First steps with Akka Typed article”:      First steps with Akka Typed: http://www.smartjava.org/content/akka-typed-first-steps-typed-actors-scala   Exploring the receiver pattern: http://www.smartjava.org/content/akka-typed-actors-exploring-receiver-pattern   The idea behind the Receptionist pattern is very simple and is explained by the ScalaDoc:   /**  * A Receptionist is an entry point into an Actor hierarchy where select Actors  * publish their identity together with the protocols that they implement. Other  * Actors need only know the Receptionist’s identity in order to be able to use  * the services of the registered Actors.  */ object Receptionist { ...   So basically a client only needs to know how to access the receptionist and from there it can access any other actor which is registered at the Receptionist. So let’s first create a couple of actors that we register with the receptionist. For this we define two very simple actors:     Nothing special, just two services that print out each message that they receive. Besides these two services we also create a service which will act as a client to these two services:     As you can see, also a rather basic actor, which changes behavior once it receives a registerAddresses message. After it has changed its behavior it’ll act on sendMessage messages to call the registered services. Now how do we tie all this together?   For this we create another actor which kicks off this scenario for us:     That is a lot of code but it is really easy to see what is happening. The first thing we do is use ctx.spawn to create a set of child actors of the type we just discussed. Once we’ve defined the actors we need to register our FirstService and SecondService actors with the receptionist. For this we need to send a message like this:   /**    * Associate the given [[akka.typed.ActorRef]] with the given [[ServiceKey]]. Multiple    * registrations can be made for the same key. Unregistration is implied by    * the end of the referenced Actor’s lifecycle.    */   final case class Register[T](key: ServiceKey[T], address: ActorRef[T])(val replyTo: ActorRef[Registered[T]]) extends Command   In our example we do that in the following way:     As you can see we create an adapter to handle the replyTo parameter of the Register message (where we just ignore the response in this case). We then use these adapters to register our service actors with the receptionist. At this point we’ve registered our services with the receptionist and can now use the Find message:     /**    * Query the Receptionist for a list of all Actors implementing the given    * protocol.    */   final case class Find[T](key: ServiceKey[T])(val replyTo: ActorRef[Listing[T]]) extends Command   .. to get a list of registered actors for a specific type:     Important to note here is line 99:     Here we sent the information about the registered services to our actor which serves as a client.   Now all that is left to do is send a sendMessage message to the sender, and it should be sent to all the registered services:     When we now run this, the output looks like this:   First Service Receiver: FirstServiceMsg1(Hello1) First Service Receiver: FirstServiceMsg1(Hello1) First Service Receiver: FirstServiceMsg1(Hello1) Second Service Receiver: SecondServiceMsg1(Hello1) First Service Receiver: FirstServiceMsg1(Hello2) First Service Receiver: FirstServiceMsg1(Hello2) Second Service Receiver: SecondServiceMsg1(Hello1) First Service Receiver: FirstServiceMsg1(Hello2) Second Service Receiver: SecondServiceMsg1(Hello1) Second Service Receiver: SecondServiceMsg1(Hello2) Second Service Receiver: SecondServiceMsg1(Hello2) Second Service Receiver: SecondServiceMsg1(Hello2)   Easy right!  ","categories": ["posts","scala","akka"],
        "tags": [],
        "url": "http://www.smartjava.org/content/akka-typed-actors-exploring-receptionist-pattern/",
        "teaser":null},{
        "title": "10 Practical Docker Tips for Day to Day Docker usage",
        "excerpt":"I’ve had the opportunity to setup a complete new docker based microservice architecture at my current job, so since everyone is sharing their docker tips and trick, I thought I’d do the same thing.  So here are a list of tips, tricks or whatever you might call it, that you might find useful in your day to day dealing with Docker.   1. Multiple dockers at the same host.   If you want you can run multiple docker containers on the same host. This is especially useful if you want to set up different TLS settings, network settings, log settings or storage drivers for a specific container. For instance we currently run a standard set up of two docker daemons. One runs Consul which provides DNS resolution and serves as the cluster store for the other docker container.   For example:   # start a docker daemon and bind to a specific port docker daemon -H tcp://$IP:5000 --storage-opt dm.fs=xfs \\             -p \"/var/run/docker1.pid\" \\             -g \"/var/lib/docker1\" \\             --exec-root=\"/var/run/docker1  # and start another daemon docker daemon -H tcp://$IP:5001 --storage-opt dm.fs=xfs \\         -s devicemapper \\         --storage-opt dm.thinpooldev=/dev/mapper/docker--vg-docker--pool \\         -p \"/var/run/docker2.pid\" \\         -g \"/var/lib/docker2\" --exec-root=\"/var/run/docker2\"         --cluster-store=consul://$IP:8500 \\         --cluster-advertise=$IP:2376   2. Docker exec of course   This is probably one of the tips that everyone mentions. When you’re using docker not just for your staging, production or testing environments, but also on your local machine to run database, servers, keystores etc. it is very convenient to be able to run commands directly within the context of a running container.   We do a lot with cassandra, and checking whether the tables contain correct data, or if you just want to execute a quick CQL query docker exec works great:   $ docker ps --format \"table \\t \\t \" CONTAINER ID        NAMES               STATUS 682f47f97fce         cassandra           Up 2 minutes 4c45aea49180         consul              Up 2 minutes  $ docker exec -ti 682f47f97fce cqlsh --color Connected to Test Cluster at 127.0.0.1:9042. [cqlsh 5.0.1 | Cassandra 2.2.3 | CQL spec 3.3.1 | Native protocol v4] Use HELP for help. cqlsh&gt;   Or just access nodetool or any other tool available in the image:   $ docker exec -ti 682f47f97fce nodetool status Datacenter: datacenter1 ======================= Status=Up/Down |/ State=Normal/Leaving/Joining/Moving --  Address         Load       Tokens       Owns    Host ID                               Rack UN  192.168.99.100  443.34 KB  256          ?       8f9f4a9c-5c4d-4453-b64b-7e01676361ff  rack1  Note: Non-system keyspaces don't have the same replication settings, effective ownership information is meaningless   And this can of course be applied to any (client) tool bundled with an image. I personally find this much easier than installing all the client libraries locally and having to keep the versions up to date.   3. docker inspect and jq   This isn’t so much a docker tip, as it is a jq tip. If you haven’t heard of jq, it is a great tool for parsing JSON from the command line. This also makes it a great tool to see what is happening in a container instead of having to use the –format specifier which I can never remember how to use exactly:   # Get network information: $ docker inspect 4c45aea49180 | jq '.[].NetworkSettings.Networks' {   \"bridge\": {     \"EndpointID\": \"ba1b6efba16de99f260e0fa8892fd4685dbe2f79cba37ac0114195e9fad66075\",     \"Gateway\": \"172.17.0.1\",     \"IPAddress\": \"172.17.0.2\",     \"IPPrefixLen\": 16,     \"IPv6Gateway\": \"\",     \"GlobalIPv6Address\": \"\",     \"GlobalIPv6PrefixLen\": 0,     \"MacAddress\": \"02:42:ac:11:00:02\"   } }  # Get the arguments with which the container was started $ docker inspect 4c45aea49180 | jq '.[].Args' [   \"-server\",   \"-advertise\",   \"192.168.99.100\",   \"-bootstrap-expect\",   \"1\" ]  # Get all the mounted volumes 11:22 $ docker inspect 4c45aea49180 | jq '.[].Mounts' [   {     \"Name\": \"a8125ffdf6c4be1db4464345ba36b0417a18aaa3a025267596e292249ca4391f\",     \"Source\": \"/mnt/sda1/var/lib/docker/volumes/a8125ffdf6c4be1db4464345ba36b0417a18aaa3a025267596e292249ca4391f/_data\",     \"Destination\": \"/data\",     \"Driver\": \"local\",     \"Mode\": \"\",     \"RW\": true   } ]   And of course also works great for querying other kinds of (docker-esque) APIs that produce JSON (e.g Marathon, Mesos, Consul etc.). JQ provides a very extensive API for accessing and processing JSON. More information can be found here: https://stedolan.github.io/jq/   4. Extending an existing container and pushing it to a local registry.   On the central docker hub there are a great number of images available that will serve for many different use cases. What we noticed though, is that often we had to make some very small changes to the images. For instance for better health checks from consul, to better behave in our cluster setup or to add additional configuration that wasn’t easy to do through system variables or command line parameters. What we usually do if we run into this, is just create our own docker image and push it to our local registry.   For instance, we wanted to have JQ available on our consul image to make health checking our services easier:  FROM progrium/consul  USER root  ADD bin/jq /bin/jq ADD scripts/health-check.sh /bin/health-check.sh   With our health check scripts and JQ we do the health checks from our own consul image. We also have a local registry running so after image creation we just tag the resulting image and push it to our local registry:  $ docker build . ... $ docker tag a3157e9edc18 &lt;local-registry&gt;/consul-local:some-tag $ docker push &lt;local-registry&gt;/consul-local:some-tag   Now it is available to our developers, and can also be used in our different testing environments (we use a separate registry for production purposes).   5. Accessing dockers on remote hosts   The docker CLI is a very cool tool. One of the great features is that you can use it to easily access multiple docker daemons even if they are on different hosts. All you have to do is set the DOCKER_HOST environment variable to point to the listening address of the docker daemon, and, if the port is of course reachable, you can directly control docker on a remote host. This is pretty much the same principle that is used by docker-machine when you run a docker daemon and set up the environment through docker-machine env :    $ docker-machine env demo export DOCKER_TLS_VERIFY=\"1\" export DOCKER_HOST=\"tcp://192.168.99.100:2376\" export DOCKER_CERT_PATH=\"/Users/jos/.docker/machine/machines/demo\" export DOCKER_MACHINE_NAME=\"demo\"   But you don’t have to limit yourself to just docker daemons started through the docker-machine, if you’ve got a controlled and well secured network where your daemons are running, you can just as easily control all the from a single machine (or stepping stone).   6. The ease of mounting host directories   When you’re working with containers, you sometimes need to get some data inside the container (e.g. shared configuration). You can copy it in, or ssh it in, but most often it is easiest to just add a host directory to the container that is mounted inside the container. You can easily do this in the following manner:   $ mkdir /Users/jos/temp/samplevolume/ $ ls /Users/jos/temp/samplevolume/ $ docker run -v /Users/jos/temp/samplevolume/:/samplevolume  -it --rm busybox $ docker run -v /Users/jos/temp/samplevolume/:/samplevolume  -it --rm busybox / # ls samplevolume/ / # touch samplevolume/hello / # ls samplevolume/ hello / # exit $ ls /Users/jos/temp/samplevolume/ hello   As you can see the directory we specified is mounted inside the container, and any files we put there are visible on both the host and inside the container.  We can also use inspect to see what is mounted where:   $ docker inspect 76465cee5d49 | jq '.[].Mounts' [   {     \"Source\": \"/Users/jos/temp/samplevolume\",     \"Destination\": \"/samplevolume\",     \"Mode\": \"\",     \"RW\": true   } ]   There are a number of additional features which are very nicely explained on the docker site: https://docs.docker.com/engine/userguide/dockervolumes/   7. Add DNS resolving to your containers   I’ve already mentioned that we use consul for our containers. Consul is a distributed KV store which also provides service discovery and health checks. For service discovery Consul provides either a REST API or plain old DNS. The great part is that you can specify the DNS server for your containers when you run a specific image. So when you’ve got Consul running (or any other DNS server) you can add it to your docker daemon like this:   docker run -d --dns $IP_CONSUL --dns-search service.consul &lt;rest of confguration&gt;   Now we can resolve the ip address of any container registered with Consul by name. For instance in our environment we’ve got a cassandra cluster. Each cassandra instance registers itself with the name ‘cassandra’ to our Consul cluster. The cool thing is that we can now just resolve the address of cassandra based on host name (without having to use docker links).   $ docker exec -ti 00c22e9e7c4e bash daemon@00c22e9e7c4e:/opt/docker$ ping cassandra PING cassandra.service.consul (192.168.99.100): 56 data bytes 64 bytes from 192.168.99.100: icmp_seq=0 ttl=64 time=0.053 ms 64 bytes from 192.168.99.100: icmp_seq=1 ttl=64 time=0.077 ms ^C--- cassandra.service.consul ping statistics --- 2 packets transmitted, 2 packets received, 0% packet loss round-trip min/avg/max/stddev = 0.053/0.065/0.077/0.000 ms daemon@00c22e9e7c4e:/opt/docker$   8. Docker-ui is a great way to view and get insight into your containers   Managing docker using the Docker CLI isn’t that hard and provides great insides in what is happening. Often though you don’t need the full power of the Docker CLI but just want a quick overview of which containers are running and see what is happening. For this a great project is Docker ui (https://github.com/crosbymichael/dockerui):      With this tool, you can see the most important aspects of the containers and images of a specific docker daemon.   9. Container not starting up? overwrite the entry point and just run it from bash   Sometimes a container just doesn’t do what you want it to do. You’ve recreated the docker image a couple of times but somehow the application you run on startup doesn’t behave as you expect and the logging shows nothing useful. The easiest way to debug this is to just overwrite the entry point of the container and see what is going on inside the container. Are the file permissions right, did you copy the right files into the image or any of the other 1000 things that could go wrong.   Luckily docker has a simple solution for this. You can start a container with an entrypoint of your choosing:   $ docker run -ti --entrypoint=bash cassandra root@896757f0bfd4:/# ls bin   dev\t\t    etc   lib\t media\topt   root  sbin  sys  usr boot  docker-entrypoint.sh  home  lib64  mnt\tproc  run   srv   tmp  var root@896757f0bfd4:/#   10. Listening to events within a container   When you’re writing your own scripts or just want to learn what is happening with your running images you can use the docker event command. Writing scripts for this is very easy.      That’s it for now and we haven’t each touched upon docker compose and swarm yet, or the Docker 1.9 network overlay features! Docker is a fantastic tool, with a great set of additional tools surrounding it. In the future I’ll show some more stuff we’ve done with Docker so far.  ","categories": ["posts","docker","top"],
        "tags": [],
        "url": "http://www.smartjava.org/content/10-practical-docker-tips-day-day-docker-usage/",
        "teaser":null},{
        "title": "Create a Slack Docker proxy in Go - Part 1",
        "excerpt":"Last year I had the opportunity to work with lots of cool tools and technologies. A couple of those were go, slack and docker. We pretty much use slack for all our communication, and are slowly adding more and more integrations to slack to get information from various build and runtime tools.  In our environment we’ve got a number of different docker environments, and currently either log into the host machine to check on the docker status, or use docker-ui (which we hacked to work with multiple docker environments).  Even though docker-ui works great, and provides a great way to get an overview of what is happening within a specific docker daemon, it isn’t really lightweight and in some scenarios, requires a lot of clicks to get the information a developer is looking for. An alternative would be for developers to just use the docker API, but that would involve opening a lot of firewall ports. So we started looking into using  (or creating) a Slack integration which we could use to query our docker environments directly from slack. That way developers wouldn’t need to use a new tool, they already have slack open most of the time, and can quickly get the status of a specific docker container (and if time permits we’ll also add Jenkins integration for deployments to the various dev environments from slack).   If your team doesn’t use slack yet, you can start out for free, and use all the cool features of slack. I’ve worked in teams of 30+ people all using the same free slack version, so there is really no use not to use slack.  So enough slack promotion, lets get started.   Sending commands from slack   Slack provides a number of different ways to integrate with external systems. In this scenario we’ll use a “Slash Command”. With slash commands we can just type in our command in Slack And slack will send a POST message with content-type application/x-www-form-urlencoded, to the configured end point which contains data like this:    token=gIkuvaNzQIHg97ATvDxqgjtO team_id=T0001 team_domain=example channel_id=C2147483705 channel_name=test user_id=U2147483697 user_name=Steve command=/weather text=94070 response_url=https://hooks.slack.com/commands/1234/5678    Now we can either response directly, or send a response to the provided response_url. In this article we’ll just respond directly (which needs to be done within 3000 ms). So first off, lets configure slack to send a specific command:      In this case whenever we type /docker, the command will be sent to our configured endpoint. For instance if we type in “/docker CI ps”, we’ll get an overview of all the images running in the CI environment:      In the next couple of section we’ll show you how to implement such a proxy with go-lang.   Creating a go-lang proxy   All the code from here can be found on my github (https://github.com/josdirksen/slack-proxy) and if you just want to run it just follow these simple steps:    $ git clone https://github.com/josdirksen/slack-proxy $ cd slack-proxy $ export GOPATH=`pwd` $ go get github.com/fsouza/go-dockerclient $ go build src/github.com/ $ go build src/github.com/josdirksen/slackproxy/slack-proxy.go # Make sure to edit the config.json to point to the correct docker location and keys/certificates $ ./slack-proxy -config resources/config.json {5cLHiZjpWaRDb0fP6ka02XCR {[{local tcp://192.168.99.100:2376 true /Users/jos/.docker/machine/machines/eris ca.pem cert.pem key.pem}]}}    Lets look at the code. The first file we’ll look at is the main slack-proxy.go file:     As you can see here, we don’t really do that much. The main thing we do is parse the configuration, and setup a HTTP listener on port 9000.  So whenever we get a request on port 9000, the specified handler, returned by GetConfigListener, is called :     The handler itself also doesn’t really do that much. It just passes on the command that needs to be executed to the handler identified by “docker”. Note that at this step we only support “docker” commands. But we could easily use this setup to also handle other commands. More on that in a future article.   Handle commands  So lets look a bit closer at the handler.HandleCommand function (which you can find in the dockerCommandHandlers.go file:     Lots of code, but the main thing to look at is the HandleCommand function. Here we just create a new docker client (see code above), and use a simple switch to determine which function to call.     For instance if you look a bit closer to the handlePsCommand you can see that we just convert the response from the client.ListContainers function and write that to the http.ResponseWriter.     #Running and testing the code   Now we can build and run the code:    $ git clone https://github.com/josdirksen/slack-proxy $ cd slack-proxy $ export GOPATH=`pwd` $ go get github.com/fsouza/go-dockerclient $ go build src/github.com/ $ go build src/github.com/josdirksen/slackproxy/slack-proxy.go # Make sure to edit the config.json to point to the correct docker location and keys/certificates $ ./slack-proxy -config resources/config.json {5cLHiZjpWaRDb0fP6ka02XCR {[{local tcp://192.168.99.100:2376 true /Users/jos/.docker/machine/machines/eris ca.pem cert.pem key.pem}]}}    And to test it we use curl to send a request. The request we test with looks like this:    $ curl 'http://localhost:9000/handleSlackCommand' -H 'Content-Type: application/x-www-form-urlencoded' --data 'token=5cLHiZjpWaRDb0fP6ka02XCR&amp;team_id=T0001&amp;team_domain=example&amp;channel_id=C2147483705&amp;channel_name=test&amp;user_id=U2147483697&amp;user_name=Steve&amp;command=/docker&amp;text=local+ps'    Which returns information like this:    ID: 669265a1343601468e71fbe65dec0251595f6c043ceeb5748f6736b28fd5da39 Command: /bin/start -server -advertise 192.168.99.100 -bootstrap-expect 1 Created: 2016-01-06 08:05:53 +0100 CET Image: docker-register.equeris.com/consul:eris-local Status: Up 3 minutes Names: [/consul] Ports:           type: tcp IP: 192.168.99.100 private: 8400 public: 8400          type: tcp IP: 192.168.99.100 private: 8302 public: 8302          type: tcp IP: 192.168.99.100 private: 8301 public: 8301          type: udp IP: 192.168.99.100 private: 53 public: 53          type: tcp IP:  private: 53 public: 0          type: tcp IP: 192.168.99.100 private: 8500 public: 8500          type: udp IP: 192.168.99.100 private: 8301 public: 8301          type: tcp IP: 192.168.99.100 private: 8300 public: 8300          type: udp IP: 192.168.99.100 private: 8302 public: 8302  ID: 682f47f97fced8725d228a68a4bdf5486bf0d3e247f6864331e4f6b413b92f65 Command: /find-ip-add-to-consul.sh -Dcassandra.load_ring_state=false Created: 2015-11-24 13:29:38 +0100 CET Image: docker-register.equeris.com/cassandra:eris Status: Up 3 minutes Names: [/cassandra-eris] Ports:           type: tcp IP: 192.168.99.100 private: 9160 public: 9160          type: tcp IP: 192.168.99.100 private: 9042 public: 9042          type: tcp IP:  private: 7199 public: 0          type: tcp IP:  private: 7001 public: 0          type: tcp IP: 192.168.99.100 private: 7000 public: 7000    That’s it for this article. Note that we’ve left a lot of things out, we only implemented the ps command, don’t do much validation or any error handling. In follow up articles we’ll look closer into adding more features to this proxy.  ","categories": ["posts","docker","slack","go-lang","go"],
        "tags": [],
        "url": "http://www.smartjava.org/content/create-slack-docker-proxy-go-part-1/",
        "teaser":null},{
        "title": "Create a Slack Docker proxy in Go - Part 2",
        "excerpt":"In the previous article we had a look at how you could easily create a slack-docker-proxy using go and a couple of small libraries. In this second article we&#39;ll show you how easy it is add additional commands, and we&#39;ll look at how to dockerize this component to you can easily run it inside a docker daemon. Note that the full sources for the various go files can of course be found at the slack-proxy github repository  Running go applications in docker.  Let&#39;s first look at how we can deploy and run this proxy. We saw that we could run it directly from the command line like this:    $ export GOPATH=directory/where/you/cloned/the/repo $ go get github.com/fsouza/go-dockerclient $ go build src/github.com/josdirksen/slackproxy/slack-proxy.go $ ./slackproxy -config ./resources/config.json    But that isn&#39;t the most practical way to do this. And since we&#39;re already using this to monitor docker, why not just run this proxy inside docker as well.  To run a go-lang application in docker we can use two different approaches. We can compile the application from the command line and just COPY it into the docker image, or we can add the sources to an image that contains all the go stuff we need, and compile it as a step when creating our docker image. We&#39;ll go for this second approach.  We&#39;ve shown the docker file we&#39;ll use for that here:     The steps should be pretty self-explanatory. What we do is we make sure the sources are in the location the docker image expects (the /go/src folder), install any external dependencies and just run &#39;docker build&#39;. Which results in the following output:    $ docker build . Sending build context to Docker daemon 203.8 kB Step 1 : FROM golang:latest  ---&gt; bc422006801e Step 2 : COPY src/ /go/src/  ---&gt; Using cache  ---&gt; a7c813facaf8 Step 3 : COPY resources/config.json /  ---&gt; Using cache  ---&gt; b6fd55de16f6 Step 4 : EXPOSE 9000  ---&gt; Using cache  ---&gt; d30199bcc59a Step 5 : WORKDIR /go/src  ---&gt; Using cache  ---&gt; e861b557571d Step 6 : RUN go get github.com/fsouza/go-dockerclient &amp;&amp; go install github.com/fsouza/go-dockerclient  ---&gt; Running in 6f712526bb99  ---&gt; 76cecb4c7a19 Removing intermediate container 6f712526bb99 Step 7 : RUN go build -o /app/main github.com/josdirksen/slackproxy/slack-proxy.go  ---&gt; Running in 77c092d1bcf2  ---&gt; f7ca02665f94 Removing intermediate container 77c092d1bcf2 Step 8 : CMD /app/main --config /config.json  ---&gt; Running in 0bbeb60ca608  ---&gt; ebd9d3fe8ce7 Removing intermediate container 0bbeb60ca608 Successfully built ebd9d3fe8ce7    The resulting image can now be run like this:    $ docker run 94abcb31af14 {5cLHiZjpWaRDb0fP6ka02XCR {[{local tcp://192.168.99.100:2376 true /Users/jos/.docker/machine/machines/eris ca.pem cert.pem key.pem}]}}    Or uploaded to your local registry by tagging it:    docker tag 94abcb31af14 my.own.registry/slack-proxy:latest    Extending with new functionality  Extending this proxy with new functionality is really easy. A common scenario we have is that after we&#39;ve deployed something and stuff stops working we can quickly check the log files. Now logging in to the machine and using docker log isn&#39;t that hard, but it takes a couple of steps. So we&#39;ll add support to view log files directly from slack. We&#39;ll first define the function in the dockerCommandHandlers.go file.     Note that we once again don&#39;t really do any input validation. We just assume the correct number of arguments are passed in. Now that we&#39;ve got the function, we update the HandleCommand function in the same file to this:     And we&#39;re done. Now when a call is made like this:    $ curl 'http://localhost:9000/handleSlackCommand' -H 'Content-Type: application/x-www-form-urlencoded' --data     'token=5cLHiZjpWaRDb0fP6ka02XCR&amp;team_id=T0001&amp;team_domain=example&amp;channel_id=C2147483705&amp;channel_name=test&amp;user_id=jos&amp;command=docker&amp;text=local+logs+669265a13436+10'  &gt; 2016/02/07 18:48:04 [WARN] agent: Check 'service:wiremock' is now warning &gt; 2016/02/07 18:48:09 [WARN] agent: Check 'service:dag-storage' is now warning &gt; 2016/02/07 18:48:17 [WARN] agent: Check 'service:wiremock' is now warning &gt; 2016/02/07 18:48:19 [WARN] agent: Check 'service:dag-storage' is now warning &gt; 2016/02/07 18:48:29 [WARN] agent: Check 'service:dag-storage' is now warning &gt; 2016/02/07 18:48:30 [WARN] agent: Check 'service:wiremock' is now warning &gt; 2016/02/07 18:48:39 [WARN] agent: Check 'service:dag-storage' is now warning &gt; 2016/02/07 18:48:43 [WARN] agent: Check 'service:wiremock' is now warning &gt; 2016/02/07 18:48:49 [WARN] agent: Check 'service:dag-storage' is now warning &gt; 2016/02/07 18:48:56 [WARN] agent: Check 'service:wiremock' is now warning    We get back the latest 10 log entries for the container with id 669265a13436.  Or if you&#39;re more inclined to use Postman.      Extending besides docker   So far we&#39;ve only looked at exposing docker through this minimal proxy. Exposing other services, components and information sources through this setup is actually really easy to do. As an example lets write a couple of commands that expose information of the host the slack-proxy is running on. For this we first add a new CommandHandler:     If you&#39;ve looked at the way we&#39;ve setup the docker handler, this one, shouldn&#39;t come as a suprise. We use a simple switch to match the command we need to execute, and in this case use the gopsutil library to return file system information. Note that if you want to run this without docker, you first have to do a \"go get github.com/shirou/gopsutil\". Since we added an additional command handler we also need to make a small change to the GetHandler function from the handlerFactory:     With this change, when the command is system we&#39;ll return the handler we just created, and if the command is docker, we&#39;ll return the one that can handle docker commands. To see this in action, you can use the following curl command for the memory information:    $ curl 'http://localhost:9000/handleSlackCommand' -H 'Content-Type: application/x-www-form-urlencoded' --data 'token=5cLHiZjpWaRDb0fP6ka02XCR&amp;team_id=T0001&amp;team_domain=example&amp;channel_id=C2147483705&amp;channel_name=test&amp;user_id=jos&amp;command=system&amp;text=local+mem'  &gt; Total: 17179869184, Free:41549824, UsedPercent:65.031576    And to test the host info use this:    $ curl 'http://localhost:9000/handleSlackCommand' -H 'Content-Type: application/x-www-form-urlencoded' --data 'token=5cLHiZjpWaRDb0fP6ka02XCR&amp;team_id=T0001&amp;team_domain=example&amp;channel_id=C2147483705&amp;channel_name=test&amp;user_id=jos&amp;command=system&amp;text=local+host'  &gt; Boottime: 1453982905Hostname: Joss-MacBook-Pro.localUptime: 889269%    And with that we&#39;re at the end of this article on slack / docker (and other sources) with go-lang. You can find the complete sources for this article here. And if you&#39;ve got suggestions please let me know.  ","categories": ["posts","slack","docker","go-lang","go"],
        "tags": [],
        "url": "http://www.smartjava.org/content/create-slack-docker-proxy-go-part-2/",
        "teaser":null},{
        "title": "Website scraping using Selenium, Docker and Chrome with Extensions.",
        "excerpt":"For a specific project we needed a quick way to get the content of a specific URL and check whether a word was present in the text there. If all we had to  scrape were static websites, this wouldn’t be that difficult. We would just get the sources, parse them with jsoup and extract the readable content. However,  a large percentage of the target sites, are single page apps or angular applications, which only show the content after some javascript processing. So we  started looking at an alternative way to do this.    First try, PhantomJS   First, we looked at whether we could simply use PhantomJS to get the website and return the content. With PhantomJS this is really simple, and the whole code  looks something like this:     var args = system.args;   var webPage = require('webpage');  var page = webPage.create();   page.open(args[1], function (status) {      var content = page.plainText;      console.log('Content: ' + content);      phantom.exit();  });    And we can use this minimal script like this:    $ phantomjs test.js https://news.ycombinator.com/ Content: Hacker News new | comments | show | ask | jobs | submit login 1. Outlook 2016’s New POP3 Bug Deletes Your Emails (josh.com) 40 points by luu 2 hours ago | 3 comments 2. 52-hertz whale (wikipedia.org) 148 points by iso-8859-1 5 hours ago | 46 comments 3. What My PhD Was Like (jxyzabc.blogspot.com) 30 points by ingve 2 hours ago | 6 comments ...    Well that was a couple of minutes work, and with a quick minimal API surrounding it, it perfectly matched our requirements, and would be easily embedabble. We can  just use PhantomJS to render the page, handle the JavaScript and no more worries. But, unfortunately, after testing a couple of site, we ran into an issue with  a number of European sites. The problem was this: the EU Cookie Law (http://ec.europa.eu/ipg/basics/legal/cookies/index_en.htm). This guideline pretty much states this:   What is the cookie law.    \"The ePrivacy directive – more specifically Article 5(3) – requires prior informed consent for storage ofor access to information stored on a user's terminal equipment. In other words, you must ask users if they agreeto most cookies and similar technologies (e.g. web beacons, Flash cookies, etc.) before the site starts to use them.\"   Or in other words, before you can use the site, you have to consent that cookies may be used. Most sites implement this through the use of a modal popup, or an overlay at either the top  or the bottom. Depending on the country you’re in, this law is more or less enforced (see https://cookiepedia.co.uk/cookie-laws-across-europe for country specific details). In the Netherlands, we’re I live and work and most of our customers are, it’s been pretty much mandated by the government for websites to strictly follow this rule.   So we ran into the problem that instead of getting the real content of the site, we’d sometimes get only the cookie message, or our content contained information that wasn’t relevant. For example see the following site, which provides such a popup:      While this was probably created with the best intentions in daily use it is very annoying, and when trying to automate and scrape stuff it makes stuff unnecesary complex.   Luckily though, there is a way we can circumvent most of these popups. By installing the “I don’t care about cookies (http://www.kiboke-studio.hr/i-dont-care-about-cookies/)” extension in your browser, most of these popups and consent forms will just be ignored. So what we’d want to accomplish is running this extension so that we can scrape websites without having to worry about invalid content based on this cookie law. Running Chrome extensions in PhantomJS, however isn’t possible, so we need an alternative approach.   Selenium and Chrome.   Basically what we want is, instead of scripting PhantomJS, we want to script Chrome with this extension installed. Actually doing this is suprisingly easy. The first thing we need is to be able to run and control Chrome. For this we can use ChromeDriver(https://sites.google.com/a/chromium.org/chromedriver/) which is an implementation of the WebDriver specification (https://w3c.github.io/webdriver/webdriver-spec.html) which allows remote control of a specific Chrome instance. When we’ve got Chrome running with the webdriver enabled, we can connect using one of the WebDriver client libraries (http://www.seleniumhq.org/download/) and start scripting our Chrome browser. To start a Chrome enabled webdriver (especially on a headless server, which is where we’d be running this) is a bit cumbersome, so we’ll just use a ready to use Docker image instead.    $ docker run -d -v /dev/shm:/dev/shm -p 4444:4444 selenium/standalone-chrome cbca5a2daf290c72f5771bbc1655e61a7700053ff1e2b95ece5301286692885e ➜  tmp  docker ps CONTAINER ID        IMAGE                        COMMAND                  CREATED             STATUS              PORTS                    NAMES cbca5a2daf29        selenium/standalone-chrome   \"/opt/bin/entry_point\"   4 seconds ago       Up 1 seconds        0.0.0.0:4444-&gt;4444/tcp   high_ritchie    This will start up a single node with Selenium and Chrome configured. We can see this is working by opening up a browser and pointing it to the hub url on the exposed port. Note the /dev/shm:/dev/shm part, this is needed to circumvent a bug when running Chrome in Docker (https://github.com/elgalu/docker-selenium/issues/20#issuecomment-133011186).   The Hub url on my local machine is: http://192.168.99.100:4444/wd/hub:      Not that exciting right? Lets run a simple script to see if everthing is working. You can use Selenium with a number of different tools and languages, for this article I’ll use Scala, but libraries are available for a lot of different languages. Let’s look at the code you can use to get a page, and output all it’s text content to the console:    import java.net.URL  import org.openqa.selenium.WebDriver import org.openqa.selenium.chrome.{ChromeOptions} import org.openqa.selenium.remote.{DesiredCapabilities, RemoteWebDriver} import org.scalatest.selenium.WebBrowser  object SeleniumScraper extends App with WebBrowser {    val capability = DesiredCapabilities.chrome()   implicit val webDriver: WebDriver = new RemoteWebDriver(new URL(\"http://192.168.99.100:4444/wd/hub\"), capability)    try {     go to (\"http://bbc.co.uk/\")     find(tagName(\"body\")) match {       case Some(el) =&gt; println(el.text)       case None =&gt; println(\"No body element found\")     }   } finally {     close()     quit()   } }    When we run this, we use the Selenium Chrome instance running in our docker container, to fetch the page. The first lines of the output look something like this:    Cookies on the BBC website The BBC has updated its cookie policy. We use cookies to ensure that we give you the best experience on our website. This includes cookies from third party social media websites if you visit a page which contains embedded content from social media. Such third party cookies may track your use of the BBC website. We and our partners also use cookies to ensure we show you advertising that is relevant to you. If you continue without changing your settings, we'll assume that you are happy to receive all cookies on the BBC website. However, you can change your cookie settings at any time. Continue Change settings Find out more Accessibility links Accessibility Help Sign in BBC navigation News Sport Weather Shop Earth Travel More Search the BBC SATURDAY, 27 FEBRUARY Hillary Clinton secures a big win over Bernie Sanders in the South Carolina primary, the latest battleground for the Democratic presidential nomination. ...    While this is already pretty good, we can see the cookie law in action here. While on the BBC site it doesn’t hide any content or prevent you from continuing, it still shows us a popup and could give us false positives when we want to search for words occuring on a specific page.   Adding extensions to chrome using selenium   I’ve mentioned the i-dont-care-about-cookies plugins earlier. If we could install this plugin in our automated browser, we could use the plugin’s filtering to remove and ignore these cookie messages. To install a plugin automatically in chrome we could follow the procedure from here: https://developer.chrome.com/extensions/external_extensions#preferences. But that would mean creating a new docker image with the correct settings. An easier approach, however, is offered by Selenium. First, though, we need the plugin as a crx archive. For this you can just use http://chrome-extension-downloader.com/ to download the plugin as a file from the filestore. Once you have this file, just add the following to your selenium script and the plugin will be automatically loaded whenever your script starts.    val capability = DesiredCapabilities.chrome() val options = new ChromeOptions(); options.addExtensions(new File(\"/Users/jos/Downloads/idontcareaboutcookies-2.5.3.crx\")) capability.setCapability(ChromeOptions.CAPABILITY, options)    Now when we run this same simple script again, we see the following:    Accessibility links Accessibility Help Sign in BBC navigation News Sport Weather Shop Earth Travel More Search the BBC SATURDAY, 27 FEBRUARY Huge win for Clinton in South Carolina Hillary Clinton secures a big win over Bernie Sanders in the South Carolina primary, the latest battleground for the Democratic presidential nomination.    As you can see, this time, the specific cookie message is ignored.   So with just a couple of simple steps we can scrape websites, and make sure we only get the core content by using an extension. We can of course also use this to run other plugins such as adblockers.  ","categories": ["posts","selenium","chrome","scala","docker"],
        "tags": [],
        "url": "http://www.smartjava.org/content/website-scraping-using-selenium-docker-and-chrome-extensions/",
        "teaser":null},{
        "title": "Developing Scala in Atom with Ensime",
        "excerpt":"When I began Java software development way back in the time of Java 1.3 I remember using JDeveloper for a time, before switching to the early versions of Eclipse. I stayed with Eclipse for a very long time (about 8 years) before switching to IntelliJ about 4 years ago. I’ve always been a big fan of IntelliJ, not just for programming Java, but I also kept using it when I started with Scala a couple of years ago. Last months, though, I started noticing that I actually only use a small amount of all the IntelliJ functionality and was slowly getting a bit annoyed with the way IntelliJ slowed down to a crawl for certain Scala projects. Combined with the fact that IntelliJ tripped over some Shapeless, Spray, Scalaz stuff, made me start looking for an alternative leightweight Scala editor.   Since I already am using Atom for taking notes (nvAtom in combination with a synced dropbox folder works great!), and I tried Ensime a couple of months ago, I thought I’d gave that combination another try. In this article you can read the steps I’ve taken, the plugins I’ve installed to get to a fairly good working Ensime / Atom combination.   Getting and installing Ensime   I’ve waited with this article until Scala 2.11.8 came out, since it solves a number of issues with imports and implicits when using Ensime. So for the best experience of using Ensime, upgrade your projects to Scala 2.11.8. So with that out of the way lets get started with installing Ensime.      To install Ensime, just follow the instructions on the site (http://ensime.github.io/editors/atom/installation/), and you’ll get a atom which you can use to run Ensime.   Preparing and importing a project   Once installed, make sure you point to your local sbt installation, and you’re more or less ready to start using ensime. The last thing we need to do, is install an sbt plugin which is needed to generate some information ensime needs. Follow the installation instructions from here (http://ensime.github.io/build_tools/sbt/) and once done open a console and navigate to the project you want to import. If you haben’t already done so, update the Scala version to 2.11.8. In that directory call sbt gen-ensime.    ➜  graphql-sangria  sbt gen-ensime [info] Set current project to graphql-sangria (in build file:/Users/jos/dev/git/graphql-sangria/) [info] ENSIME update. Please vote for https://github.com/sbt/sbt/issues/2266 [info] ENSIME updateClassifiers. Please vote for https://github.com/sbt/sbt/issues/1930 [warn] Multiple dependencies with the same organization/name but different versions. To avoid conflict, pick one version: [info] Resolving org.scala-lang#scalap;2.11.7 ... [info] ENSIME processing graphql-sangria (graphql-sangria) ➜  graphql-sangria  ls -lat total 32 -rw-r--r--   1 jos  staff  9327 Mar 19 09:54 .ensime drwxr-xr-x  57 jos  staff  1938 Feb 22 09:28 .. drwxr-xr-x   6 jos  staff   204 Jan 20 07:15 .ensime_cache drwxr-xr-x   9 jos  staff   306 Jan 19 20:22 . drwxr-xr-x  12 jos  staff   408 Jan  5 14:17 .idea -rw-r--r--   1 jos  staff   410 Jan  2 14:45 build.sbt drwxr-xr-x   4 jos  staff   136 Jan  2 13:24 src drwxr-xr-x   6 jos  staff   204 Jan  2 13:24 target drwxr-xr-x   6 jos  staff   204 Jan  2 13:23 project ➜  graphql-sangria    Note that I use the following sbt wrapper (https://github.com/paulp/sbt-extras), which drastically reduces the output of SBT only to the necessary, and provides a lot of additional feature that make working with SBT a little easier. So when you run this, you might see some more information. The import thing to notice here, is that we’ve got a .ensime and a .ensime_cache directory now, which contains Ensime specific files.   In Atom open up a new window, add that folder and you can use ctrl-cmd-p Ensime: start to start ensime.      The first time will take some time while dependencies are being downloaded, but eventually Ensime will finish downloading and start the server. At this point you can use Ensime to check your Scala code (either while your typing or whenever you save). At the bottom it will show you the issues with the current file or with the complete project. For instance in the sample project, I’ve got a number of unused imports:      It also provides import suggestions:      And shows specific errors.      So for a lot of projects, you can already use Atom for your Scala development. There are, however, also a couple of issues with Ensime.   What doesn't work that great yet   I’m an avid IntelliJ user, and one of the features I noticed I used a lot when I started using Atom, was the auto-import function of IntelliJ. Ensime/Atom provides something similar, but it is by far not as useful as the IntelliJ one. With Atom/Ensime you can import a suggestion, but (at the moment) it just selects one, instead of allowing you to chose. More often than not the wrong one gets imported.   Even though the import feature is an annoying one, it’s just a matter of time before it is added, since Ensime does provide multiple hints, it’s just that the Atom/Ensime plugin doesn’t support it yet.   There are a number of other feature, where IntelliJ provides more helpful information, but nothing that really stands in the way of working, you’ll just be browsing the source/docuemntation a little more, which is easy to do (Ensime: go to doc):      Or just ctrl-click to go to the declaration.   Some stuff that was blocking   While Ensime/Atom is quickly getting better, at the moment, for some projects, there could be some really blocking issues. If you use a project that uses macros or some compiler plugins, you might get false errors. I was going to show a project where false errors where reported, but with the latest version the errors apparently don’t occur anymore in my spray/shapeless project. So I took one of my other projects (which uses a lot of Scalaz stuff), where IntelliJ has issues with as a test to see how Ensime/Atom handles that. While it takes some time for Ensime to compile everything and get to a stable state, it handles everything correctly!   Regardless of this recent suprising succes, there is still a big change, you’ll run into false errors. This is an issue where the Ensime team is aware of (see issue: https://github.com/ensime/ensime-server/issues/673), and which apparently is very difficult to solve. People however are working on this by providing a plugin which returns the correct signatures for generated code, to make the editors happy. More information on this can be found here: https://github.com/fommil/imaginary-friend   Additional plugins which make working with Ensime easier  .   With the Ensime plugin you can probably already get a lot of work done using the standard functionality in Atom. However, I’ve installed the two suggested plugins from the Ensime/Atom site and they really make working with multiple projects and navigating through your code a lot easier. So if you intend to start using Atom and Ensime more. These two are very useful to install:      Project manager: Allows you to manage multiple projects   Last cursor position: Navigate back to where you where   Lastly, I suggest you install the term3 package, with this package you can open a terminal window directly in your Atom, which is nice when you’ve got some quick sbt commands to execute, or do some git actions (which is pretty much the way I also work in IntelliJ)   Conclusion   Development of the Ensime server goes quickly, as I’ve just noticed myself. A month ago there were a whole lot of issues with spray/shapeless which seems to have been solved now (whether this is through Ensime improvements or scala 2.11.8 I don’t know). The Atom plugin has some rough edges, but is already really useful. One of the biggest advantages is how leightweight Atom is in comparison with IntelliJ. So I’ll start working with Atom more and more, but will probably come back to IntelliJ for certain type of projects.  ","categories": ["posts","ensime","atom","scala"],
        "tags": [],
        "url": "http://www.smartjava.org/content/developing-scala-atom-ensime/",
        "teaser":null},{
        "title": "Presentation: Service discovery in a microservice architecture",
        "excerpt":"I recently gave a presentation on how you can use Consul (http://consul.io) for all your service discovery needs in a microservice architecture.  The presentation contains a couple of extensive demos whose sources and explanations can be found here: https://github.com/josdirksen/next-build-consul   There are also a couple of articles on this site that further explain the concepts shown in this presentation:     Service Discovery with Docker and Consul: part 1   Service Discovery with Docker and Consul: part 2       Service discovery in a microservice architecture using consul  from Jos Dirksen   Feel free to use anything from this presentation! Should you have any questions just let me know.   Jos  ","categories": ["posts","microservices","consul"],
        "tags": [],
        "url": "http://www.smartjava.org/content/presentation-service-discovery-microservice-architecture/",
        "teaser":null},{
        "title": "Service Discovery with Docker and Consul: part 1",
        "excerpt":"During the last year I've become a big fan of using Consul for all things related to service discovery. If you're doing microservices you've probably ran into the issue that when the number of services you create increases it becomes more and more difficult to manage the communication between all these services. Consul provides a perfect fit for this problem. It provides an easy to use, open standards based (opinionated) approach to service discovery (and besides that provides a large set of other functions). I recently gave a presentation on how to do Service discovery in a microservices architecture using Consul, and got a couple of requests to explain a bit more about it. So in this article, and a couple of follow ups, I'll explain a bit more about how you can use Consul. I won't just focus on purely the service discovery part provided by Consul, but also show you a couple of other features provided by either Consul or one of the tools surrounding it.   For the other articles in this series you can look here:     Presentation on Service discovery with consul   Service discover with Docker and Consul: Part 2   Note that all the samples, docker files, etc. can be found in the following repo: https://github.com/josdirksen/next-build-consul. So instead of \"copy &amp; pasting\" from this article just clone the repository.  Getting started  In this first article we'll create a simple docker based architecture with a number of services that will communicate with one another using simple HTTP calls, and that will discover each other using Consul. Our initial target architecture looks a bit like this:      To accomplish all this we first need to take the following steps to set up an environment we can run the services in:  Note: I'm using docker-machine on my Mac to do all this. If you run Windows or Linux the commands might vary slightly. Let's hope that Docker for Mac (and Windows) quickly comes out of beta (https://blog.docker.com/2016/03/docker-for-mac-windows-beta), so we don't need this anymore...     Create four docker-machines: One in which we'll run a Consul server, and three in which we'll run our individual services and a Consul agent.   Start main consul server: We'll use a single Consul server (and multiple Consul agents, more on that later) to keep track of the running services and some docker related stuff.   Setup docker swarm: To avoid having to deploy our services individually we'll use a Docker Swarm to manager the three nodes on which we'll run our services. In the rest of this article we'll use docker-compose to start and stop the individual services.   Setup docker overlay network: If we want to have our services communicate with each other in a simple manner, we will create an overlay network. This will allow the components we deploy to docker to easily communicate with one other (since they'll share the same subnet)   Start the Consul agents: Each node will have its own consul agent, which will monitor the health of the services on that node and communicate with the consul server.  Creating the docker-machines  So the first thing we'll do is create some docker-machines. First we'll create the docker-machine that'll hold our consul server. The reason we run this one first, is so that we can point the other docker-machines to consul running inside this container and use it for managing docker-swarm and the overlay network we want to use.    docker-machine create nb-consul --driver virtualbox    Before we start the Consul server, lets quickly look at the architecture behind Consul.      In this image you can see the two modes Consul can run in. It can run in Server mode or Agent mode. All the Servers talk to each other and decide who is the leader. An agent just talks to one of the servers and normally runs on the node that is also running the services. Note that the state between all the Servers and Agents within a cluster is shared. So when a service registers itself with one of the agents, that information is available to all the Servers and the Agents that are connected to one another.  For this set of articles we won't setup a cluster of Servers, but just use one. Now that we've got our docker-machine running, we can start the consul server. Before we start let me first show you a simple script that make switching between different docker-machines easier and the alias we use to avoid typing \"docker-machine\".    # quickly switch environments e.g: . dm-env nb-consul $ cat ~/bin/dm-env eval `docker-machine env $2 $1`  # avoid typing too much $ alias dm dm=docker-machine    So with these aliases in place, first we do a \"dm-env nb-consul\" to select the correct docker-machine.  Start the main consul machine  Next we get the ip address of this server and then we can start our Consul server like this.    # get the ip address $ 192.168.99.106  # use this ip address in the advertise docker run -d --restart always -p 8300:8300 -p 8301:8301 -p 8301:8301/udp -p 8302:8302/udp \\             -p 8302:8302 -p 8400:8400 -p 8500:8500 -p 53:53/udp -h server1 progrium/consul \\            -server -bootstrap -ui-dir /ui -advertise $(dm ip nb-consul)    At this point we have our docker consul server running. Now lets create the other three servers on which we'll run our services.  Setup docker swarm  As you can see in the following commands, we're also creating a docker swarm cluster at the same time, and the \"nb1\" node is the swarm master.    docker-machine create -d virtualbox --swarm --swarm-master \\             --swarm-discovery=\"consul://$(docker-machine ip nb-consul):8500\" \\            --engine-opt=\"cluster-store=consul://$(docker-machine ip nb-consul):8500\" \\            --engine-opt=\"cluster-advertise=eth1:2376\" nb1  docker-machine create -d virtualbox --swarm  \\             --swarm-discovery=\"consul://$(docker-machine ip nb-consul):8500\" \\             --engine-opt=\"cluster-store=consul://$(docker-machine ip nb-consul):8500\" \\            --engine-opt=\"cluster-advertise=eth1:2376\" nb2  docker-machine create -d virtualbox --swarm \\               --swarm-discovery=\"consul://$(docker-machine ip nb-consul):8500\"  \\               --engine-opt=\"cluster-store=consul://$(docker-machine ip nb-consul):8500\" \\               --engine-opt=\"cluster-advertise=eth1:2376\" nb3    At this point we've got four docker-machines up and running. One is running a Consul master, and the other ones aren't doing much yet.   $ dm ls NAME        ACTIVE   DRIVER       STATE     URL                         SWARM nb1         -        virtualbox   Running   tcp://192.168.99.110:2376   nb1 (master) nb2         -        virtualbox   Running   tcp://192.168.99.111:2376   nb1 nb3         -        virtualbox   Running   tcp://192.168.99.112:2376   nb1 nb-consul   *        virtualbox   Running   tcp://192.168.99.106:2376    Before we continue with configuring the slaves, there is one more utility script that might come in handy:   $ cat addToHost #!/usr/bin/env bash  $ cat addToHost #!/usr/bin/env bash  update-docker-host(){ \t# clear existing docker.local entry from /etc/hosts \tsudo sed -i \"/[[:space:]]\"${1}\"\\.local$/d\" /etc/hosts  \t# get ip of running machine \texport DOCKER_IP=\"$(docker-machine ip $1)\"  \t# update /etc/hosts with docker machine ip \t[[ -n $DOCKER_IP ]] &amp;&amp; sudo /bin/bash -c \"echo \\\"${DOCKER_IP} $1.local\\\" &gt;&gt; /etc/hosts\" }  update-docker-host nb1 update-docker-host nb2 update-docker-host nb3 update-docker-host nb-consul    This script adds the ip addresses of the docker-machines to your local \"hosts\" file. This means that we can simply access the docker hosts by just going to \"http://nb-consul.local:8500\" for instance.   Setup the docker network   In our scenario we want all our services to be able to communicate with one another. We have multiple docker hosts so we need to find an easy way to have services running in node \"nb1\" to be able to communicate with \"nb2\". The easiest way to accomplish this is to create a single network that is used by all the services running in the docker containers. To do this we create a simple \"overlay\" network like this:   # select the swarm master $ dm-env nb1 --swarm # create an overlay network the the name my-net $ docker network create --driver overlay --subnet=10.0.9.0/24 my-net    And since we created this on our swarm master, this network will be available in all the members of our swarm. When we create our services later on, we'll connect those to this network, so that they all share the same subnet.  Start the consul agents  To start the consul agents, we're going to use docker-compose. The docker-compose file is very straightforward, and is just a simple way to avoid typing in all the launch commands (especially when you're doing live demos)   version: '2'  services:   agent-1:     image: progrium/consul     container_name: consul_agent_1     ports:       - 8300:8300       - 8301:8301       - 8301:8301/udp       - 8302:8302       - 8302:8302/udp       - 8400:8400       - 8500:8500       - 53:53/udp     environment:       - \"constraint:node==nb1\"     command: -ui-dir /ui -join 192.168.99.106 -advertise 192.168.99.110     networks:       default:         aliases:           - agent-1    agent-2:     image: progrium/consul     container_name: consul_agent_2     ports:       - 8300:8300       - 8301:8301       - 8301:8301/udp       - 8302:8302       - 8302:8302/udp       - 8400:8400       - 8500:8500       - 53:53/udp     environment:       - \"constraint:node==nb2\"     command: -ui-dir /ui -join 192.168.99.106 -advertise 192.168.99.111     networks:       default:         aliases:           - agent-2    agent-3:     image: progrium/consul     container_name: consul_agent_3     ports:       - 8300:8300       - 8301:8301       - 8301:8301/udp       - 8302:8302       - 8302:8302/udp       - 8400:8400       - 8500:8500       - 53:53/udp     environment:       - \"constraint:node==nb3\"     command: -ui-dir /ui -join 192.168.99.106 -advertise 192.168.99.112     networks:       default:         aliases:           - agent-3  networks:   default:     external:       name: my-net    Nothing to special in this file. The only thing you might notice it that we use explicit IP addresses in the commands to start the Consul agents. We could, easily, just use an environment variable for this, which is set through a simple bash script. But for this article we just specify the IP addresses of the relevant docker-machines. Make sure your \"DOCKER_HOST\" points to the docker swarm master and start the agents like this:   # start the agents $ docker-compose -f docker-compose-agents.yml up -d Creating consul_agent_3 Creating consul_agent_2 Creating consul_agent_1  # check what is running $ docker ps  --format '\\t\\t\\t'  bf2000882dcc\tprogrium/consul\t\"/bin/start -ui-dir /\"\tnb1/consul_agent_1 a1bc26eef516\tprogrium/consul\t\"/bin/start -ui-dir /\"\tnb2/consul_agent_2 eb0d1c0cc075\tprogrium/consul\t\"/bin/start -ui-dir /\"\tnb3/consul_agent_3    At this point we have a Consul server running in docker-machine \"nb-consul\" and we've got three agents running on our nodes. To validate our setup, let's open up the Consul server's interface: http://nb-consul.local:8500      And, as you can see, we've got 1 server running (our Consul Server), and the three agents. So at this point we can start adding our services, to get to this architecture:      Adding the services  The services in this case are just simple golang applications. I created a simple application that can run in frontend or in backend mode. In frontend mode it provides a minimal UI with a button to call a backend service, and in backend mode it provides a simple API that returns some information to the calling party, and it provides a simple UI showing some statistics. For convenience I've pushed this image to the docker hub (https://hub.docker.com/r/josdirksen/demo-service/) so you can easily use it without having to build from the source github repository.  As you can see in the previous architecture overview we want to start a frontend and a backend service on each of the nodes. We could do this manually, but since we've got docker-swarm we can easily do this through a single docker-compose file. If you want to see what this file looks like you can check the sources here (https://github.com/josdirksen/next-build-consul).  Lets first launch the services, and then we'll look at how they register themselves with Consul:   # make sure you select the swarm master $ . dm-env nb1 --swarm  # now use docker-compose to run the backend services $ docker-compose -f docker-compose-backend.yml up -d Creating Backend2 Creating Backend3 Creating Backend1  # and use docker-compose to run the frontend services $ docker-compose -f docker-compose-frontend.yml up -d Creating Frontend1 Creating Frontend3 Creating Frontend2  # check in docker if everything is running $ docker ps --format '\\t\\t\\t'  65846be2e367    josdirksen/demo-service \"/entrypoint.sh --typ\"  nb2/Frontend2 aedd80ab0889    josdirksen/demo-service \"/entrypoint.sh --typ\"  nb3/Frontend3 d9c3b1d83b5e    josdirksen/demo-service \"/entrypoint.sh --typ\"  nb1/Frontend1 7c860403b257    josdirksen/demo-service \"/entrypoint.sh --typ\"  nb1/Backend1 80632e910d33    josdirksen/demo-service \"/entrypoint.sh --typ\"  nb3/Backend3 534da0670e13    josdirksen/demo-service \"/entrypoint.sh --typ\"  nb2/Backend2 bf2000882dcc    progrium/consul \"/bin/start -ui-dir /\"  nb1/consul_agent_1 a1bc26eef516    progrium/consul \"/bin/start -ui-dir /\"  nb2/consul_agent_2 eb0d1c0cc075    progrium/consul \"/bin/start -ui-dir /\"  nb3/consul_agent_3    As you can see in the last output of \"docker ps\" we have three frontends, three backends, and three consul agents running. This is pretty much the architecute we're aiming for. We can also see this when we open up Consul:      As you can see we've got three frontend services and three backend services registered in Consul. If we open one of the backends we'll see some general information:      And we can use the frontend UI, to call one of our backends:      There are however a couple of questions we need to answer:     Service registration: When we start a backend or frontend service, we see it appearing in Consul. How do we do this?   Service discovery: And when we click the button on the frontend service a call is made to one of the backend services. How does the frontend know which service to call?  In the next sections we'll look a bit closer at these questions.  Service registration  First off, service registration. To register a service with Consul, we have to make a very simple REST call to our local consul-agent, which looks something like this:   {   \"Name\": \"service1\",   \"address\": \"10.0.0.12\",   \"port\": 8080,   \"Check\": {      \"http\": \"http://10.0.0.12:8080/health\",      \"interval\": \"5s\"   } }    As you can see, we specify the name, address and port where the service can be found, and we add an additional health check. When the healtcheck returns something in the 200 range the service is marked as healthy and can be discoverd by other services. So how do we do this for our services. If you look at the sources for this example you can find the \"script/entrypoint.sh\" file, which looks like this:   #!/usr/bin/env bash  IP=`ip addr | grep -E 'eth0.*state UP' -A2 | tail -n 1 | awk '{print $2}' | cut -f1 -d '/'` NAME=\"$2-service\"  read -r -d '' MSG &amp;lt;&amp;lt; EOM {   \"Name\": \"$NAME\",   \"address\": \"$IP\",   \"port\": $PORT,   \"Check\": {      \"http\": \"http://$IP:$PORT\",      \"interval\": \"5s\"   } } EOM  curl -v -XPUT -d \"$MSG\" http://consul_agent_$SERVER_ID:8500/v1/agent/service/register &amp;amp;&amp;amp; /app/main \"$@\"    What this script does, it that it creates the JSON to be sent to the consul-agent and before starting the main application it uses \"curl\" to send it. So when a service is started, it automatically registers itself to the local consul agent (note that you can also do this more automatically for instance using Consul Registrator. This works since we can just reference the local agent by its name, since it is in the same contaner. If you look closely you might see that we use a couple of environment variables here. These are passed in through the docker-compose file we use:   ... frontend-1:     image: josdirksen/demo-service     container_name: Frontend1     ports:       - 8090:8090     environment:       - \"constraint:node==nb1\"       - SERVER_ID=1       - SERVERNAME=Server1       - PORT=8090     command: /entrypoint.sh --type frontend     dns: 192.168.99.106     dns_search: service.consul ...    The interesting part here are the DNS entries. As you might remember the 192.168.99.106 is the address of our consul server. This means that we do DNS lookups against Consul (we could also have pointed to a consul agent).  Service discovery  With this setup we can just reference a service by name, and use DNS to resolve it. The following shows how this works.   # check which IPs are registered for the backend-service # called from outside the container $ dig @nb-consul.local backend-service.service.consul +short 10.0.9.7 10.0.9.8 10.0.9.6  # If we do this from a container, we can do just this docker exec -ti nb2/Frontend2 ping backend-service PING backend-service.service.consul (10.0.9.8): 56 data bytes 64 bytes from 10.0.9.8: icmp_seq=0 ttl=64 time=0.809 ms 64 bytes from 10.0.9.8: icmp_seq=1 ttl=64 time=0.636 ms    Cool right? We can discover service, by just using DNS. That also means that integrating this in our existing applications is really easy, since we can just rely on basic DNS resolving. For example, in the frontend service we call the backend using this code:   resp, err := http.Get(\"http://backend-service:8081\") if err != nil { \t// handle error \tfmt.Println(err) } else { \tdefer resp.Body.Close() \tbody, _ := ioutil.ReadAll(resp.Body) \tw.Header().Set(\"Content-Type\",resp.Header.Get(\"Content-Type\")) \tw.Write(body) }    This calls one of the backend services using DNS. We now also have some simple failover, since the DNS timetolive of Consul is set to 0. Applications might still do some caching, but it means that we already have some basic failover:   $ curl -s backend-service:8081          {\"result\" : {           \"servername\" : \"Server1\",           \"querycount\" : 778           }         }  # shutdown server 1 and do again, curl has a DNS cache of # 1 minute, so you might need to wait a bit $ curl -s backend-service:8081          {\"result\" : {           \"servername\" : \"Server2\",           \"querycount\" : 770           }         }  $ curl -s backend-service:8081          {\"result\" : {           \"servername\" : \"Server2\",           \"querycount\" : 771           }         }    Which of course also works for our frontend/golang application:      In the follow up to this article, we'll also show some more advanced failover, by introducing HAProxy as an intermediate for more advanced failover techniques.  Conclusions  That pretty much wraps it up for this first article. So in summary what have we done:     We've setup a simple architecture using 4 docker nodes. 1 for the consul server, and three for our services.   The services register themselves with Consul on service startup.   We don't need to explicitly do something to enable service discovery. We can use standard DNS to lookup a service.   Consul uses a TTL of 0 for DNS, and returns the available services using roundrobin. As you've seen, you can already use this for basic failover, when a DNS lookup fails.  Keep tuned for the follow up article somewhere in the coming weeks.  ","categories": ["posts","microservices","docker","consul"],
        "tags": [],
        "url": "http://www.smartjava.org/content/service-discovery-docker-and-consul-part-1/",
        "teaser":null},{
        "title": "Scalaz features for everyday usage part 1: Typeclasses and Scala extensions",
        "excerpt":"Most of you have probably heard of the great Javascript book: Javascript the good parts. In the same light I'd like to show some stuff from Scalaz which are really great to use in everyday projects, without having to dive into the (at least for me) scary inner workings of Scalaz. For this first part we'll dive into a number of useful typeclasses. In future parts we'll look at stuff like Monad Transformers, Free monads, Validation etc.  The following articles are currently available in this series:      Scalaz features for everyday usage part 1: Typeclasses and Scala extensions    Scalaz features for everyday usage part 2: Monad Transformers and the Reader Monad&lt;/p&gt;   Scalaz features for everyday usage part 3: State Monad, Writer Monad and lenses&lt;/p&gt;  For these examples we'll use the scala REPL. So if you want to play along start scala and load the scala library:    scala&gt; :require /Users/jos/.ivy2/cache/org.scalaz/scalaz-core_2.11/bundles/scalaz-core_2.11-7.2.1.jar Added '/Users/jos/.ivy2/cache/org.scalaz/scalaz-core_2.11/bundles/scalaz-core_2.11-7.2.1.jar' to classpath.  scala&gt; import scalaz._ import scalaz._  scala&gt; import Scalaz._ import Scalaz._    In this first article we'll look at the following typeclasses from the Scalaz library:     Equals typeclass: for typesafe equals operators.   Order typeclass: for somewhat more typesafe ordering   Enum typeclass: to create feature rich enumerations  Besides that we'll also look at a couple of simple extensions Scalaz adds to some of the types provided by the standard Scala library. We won't look at everything Scalaz adds, but just at a couple of extensions to Option and Boolean.  Useful typeclasses  With typeclasses you can easily add functionality to existing classes (see Pimp my library pattern). Scalaz comes with a couple on useful typeclasses out of the box you can immediately use.  Typesafe Equals operator  Scalaz provides a typesafe equals operator, which will throw a compile error when comparing invalid types. So while == and != in Scala will allow you to compare a String and an Int using the Scalaz === and =/= operators will result in a compile time error:    scala&gt; 1 == 1 res6: Boolean = true scala&gt; 1 === 1 res7: Boolean = true scala&gt; 1 == \"1\" res8: Boolean = false scala&gt; 1 === \"1\" &lt;console&gt;:14: error: type mismatch;  found   : String(\"1\")  required: Int               1 === \"1\"    Scalaz, provides the following set of operators, which behavior can be easily seen from the function implementation.   final def ===(other: F): Boolean = F.equal(self, other) final def /==(other: F): Boolean = !F.equal(self, other) final def =/=(other: F): Boolean = /==(other) final def ≟(other: F): Boolean = F.equal(self, other) final def ≠(other: F): Boolean = !F.equal(self, other)    Order typeclass  This is a very simple typeclass which provides more typesafe ordering. Just like with the Equals operators we can now catch comparison of two different types at compile time:    scala&gt; 1 &lt; 4d res25: Boolean = true  scala&gt; 1 lte 4d &lt;console&gt;:14: error: type mismatch;  found   : Double(4.0)  required: Int               1 lte 4d  scala&gt; 1 ?|? 1 res31: scalaz.Ordering = EQ  scala&gt; 1 ?|? 2 res32: scalaz.Ordering = LT  scala&gt; 1 ?|? 2d &lt;console&gt;:14: error: type mismatch;  found   : Double(2.0)  required: Int               1 ?|? 2d    Scalaz provides the following set of operators for this:    final def &lt;(other: F): Boolean = F.lessThan(self, other) final def &lt;=(other: F): Boolean = F.lessThanOrEqual(self, other) final def &gt;(other: F): Boolean = F.greaterThan(self, other) final def &gt;=(other: F): Boolean = F.greaterThanOrEqual(self, other) final def max(other: F): F = F.max(self, other) final def min(other: F): F = F.min(self, other) final def cmp(other: F): Ordering = F.order(self, other) final def ?|?(other: F): Ordering = F.order(self, other) final def lte(other: F): Boolean = F.lessThanOrEqual(self, other) final def gte(other: F): Boolean = F.greaterThanOrEqual(self, other) final def lt(other: F): Boolean = F.lessThan(self, other) final def gt(other: F): Boolean = F.greaterThan(self, other)    Enum typeclass  With the Scalaz Enum type it is very easy to create enumerators, which have more functionality than the ones in the standard Scala or Java libraries. It provides a number of functions to traverse through an Enum, and helps in creating new ones, of subset of ones. Scalaz provides the following set of functions:    final def succ: F = F succ self final def -+-(n: Int): F = F.succn(n, self) final def succx: Option[F] = F.succx.apply(self) final def pred: F = F pred self final def ---(n: Int): F = F.predn(n, self) final def predx: Option[F] = F.predx.apply(self) final def from: EphemeralStream[F] = F.from(self) final def fromStep(step: Int): EphemeralStream[F] = F.fromStep(step, self) final def |=&gt;(to: F): EphemeralStream[F] = F.fromTo(self, to) final def |-&gt;(to: F): List[F] = F.fromToL(self, to) final def |==&gt;(step: Int, to: F): EphemeralStream[F] = F.fromStepTo(step, self, to) final def |--&gt;(step: Int, to: F): List[F] = F.fromStepToL(step, self, to)    A very nice example can be found here at stackoverflow: http://stackoverflow.com/questions/28589022/enumeration-concept-in-scala-which-option-to-take, which however requires a couple of small changes to get all the Scalaz goodies. The following code shows how to use this enumeration:    scala&gt; import scalaz.Ordering._ import scalaz.Ordering._  scala&gt; :paste // Entering paste mode (ctrl-D to finish)    case class Coloring(val toInt: Int, val name: String)    object Coloring extends ColoringInstances {      val RED = Coloring(1, \"RED\")     val BLUE = Coloring(1, \"BLUE\")     val GREEN = Coloring(1, \"GREEN\")   }    sealed abstract class ColoringInstances {      import Coloring._      implicit val coloringInstance: Enum[Coloring] with Show[Coloring] = new Enum[Coloring] with Show[Coloring] {        def order(a1: Coloring, a2: Coloring): Ordering = (a1, a2) match {         case (RED, RED) =&gt; EQ         case (RED, BLUE | GREEN) =&gt; LT         case (BLUE, BLUE) =&gt; EQ         case (BLUE, GREEN) =&gt; LT         case (BLUE, RED) =&gt; GT         case (GREEN, RED) =&gt; GT         case (GREEN, BLUE) =&gt; GT         case (GREEN, GREEN) =&gt; EQ       }        def append(c1: Coloring, c2: =&gt; Coloring): Coloring = c1 match {         case Coloring.RED =&gt; c2         case o =&gt; o       }        override def shows(c: Coloring) = c.name        def zero: Coloring = Coloring.RED        def succ(c: Coloring) = c match {         case Coloring.RED =&gt; Coloring.BLUE         case Coloring.BLUE =&gt; Coloring.GREEN         case Coloring.GREEN =&gt; Coloring.RED       }        def pred(c: Coloring) = c match {         case Coloring.GREEN =&gt; Coloring.BLUE         case Coloring.BLUE =&gt; Coloring.RED         case Coloring.RED =&gt; Coloring.GREEN       }        override def max = Some(GREEN)        override def min = Some(RED)      }   }  // Exiting paste mode, now interpreting.  defined class Coloring defined object Coloring defined class ColoringInstances    Now we can use all the functions defined in the Scalaz enum ops:    scala&gt; import Coloring._ import Coloring._  scala&gt; RED res0: Coloring = Coloring(1,RED)  scala&gt; GREEN res1: Coloring = Coloring(1,GREEN)  scala&gt; RED |-&gt; GREEN res2: List[Coloring] = List(Coloring(1,RED), Coloring(1,BLUE), Coloring(1,GREEN))  scala&gt; RED succ warning: there was one feature warning; re-run with -feature for details res3: Coloring = Coloring(1,BLUE)  scala&gt; RED -+- 1 res4: Coloring = Coloring(1,BLUE)  scala&gt; RED -+- 2 res5: Coloring = Coloring(1,GREEN)    Nice right? This is a really great way of creating flexible and feature rich enumerations.  Standard classes extensions  Like we said in the beginning of this article, we'd look at how Scalaz make the standard Scala library more feature rich by adding functionality to some of its standard classes.  More fun with Options  With the Optional typeclass Scalaz makes working with Scala Options easier. For instance it provides function to make construction easier:    scala&gt; Some(10) res11: Some[Int] = Some(10)  scala&gt; None res12: None.type = None  scala&gt; some(10) res13: Option[Int] = Some(10)  scala&gt; none[Int] res14: Option[Int] = None    What you'll see is that the resulting type of these function is an Option[T] instead of Some or None. You might wonder why this is useful, but look at the following: Say we have a list of Options, over which we want to fold:    scala&gt; val l = List(Some(10), Some(20), None, Some(30)) l: List[Option[Int]] = List(Some(10), Some(20), None, Some(30))  scala&gt; l.foldLeft(None) { (el, z) =&gt; el.orElse(z)  } &lt;console&gt;:22: error: type mismatch;  found   : Option[Int]  required: None.type               l.foldLeft(None) { (el, z) =&gt; el.orElse(z)  }    This will fail, because our fold expects the result to be a None.type and not an Option. When we use the Scalaz versions, it works as expected:    scala&gt; l.foldLeft(none[Int]) { (el, z) =&gt; el.orElse(z)  } res19: Option[Int] = Some(10)    And Scalaz wouldn't be Scalaz without introducing some new operators.    // Alternative for getOrElse scala&gt; Some(10) | 20 res29: Int = 10  scala&gt; none | 10 res30: Int = 10  // Ternary operator scala&gt; Some(10) ? 5 | 4 res31: Int = 5  // ~ operator: Returns the item contained in the Option if it is defined, otherwise, the zero element for the type A scala&gt; some(List()) res32: Option[List[Nothing]] = Some(List())  scala&gt; ~res32 res33: List[Nothing] = List()  scala&gt; some(List(10)) res34: Option[List[Int]] = Some(List(10))  scala&gt; ~res34 res35: List[Int] = List(10)    Nothing too complex, just some helper functions. There is a lot of other stuff surrounding Options in the Scalaz library, but that is a bit out of scope for this article.  More Boolean functionality  Scalaz also adds some functionality to the Boolean type.    # Ternary operations are back! scala&gt; true ? \"This is true\" | \"This is false\" res45: String = This is true  scala&gt; false ? \"This is true\" | \"This is false\" res46: String = This is false  # Returns the given argument if this is `true`, otherwise, the zero element for the type of the given argument. scala&gt; false ?? List(120,20321) res55: List[Int] = List()  scala&gt; true ?? List(120,20321) res56: List[Int] = List(120, 20321)    And a whole list of additional operators for binary arithmetics:   // Conjunction. (AND) final def ∧(q: =&gt; Boolean) = b.conjunction(self, q) // Conjunction. (AND) final def /\\(q: =&gt; Boolean) = ∧(q) // Disjunction. (OR) final def ∨(q: =&gt; Boolean): Boolean = b.disjunction(self, q) // Disjunction. (OR) final def \\/(q: =&gt; Boolean): Boolean = ∨(q) // Negation of Disjunction. (NOR) final def !||(q: =&gt; Boolean) = b.nor(self, q) // Negation of Conjunction. (NAND) final def !&amp;&amp;(q: =&gt; Boolean) = b.nand(self, q) // Conditional. final def --&gt;(q: =&gt; Boolean) = b.conditional(self, q) // Inverse Conditional. final def &lt;--(q: =&gt; Boolean) = b.inverseConditional(self, q) // Bi-Conditional. final def &lt;--&gt;(q: =&gt; Boolean) = b.conditional(self, q) &amp;&amp; b.inverseConditional(self, q) // Inverse Conditional. final def ⇐(q: =&gt; Boolean) = b.inverseConditional(self, q) // Negation of Conditional. final def ⇏(q: =&gt; Boolean) = b.negConditional(self, q) // Negation of Conditional. final def -/&gt;(q: =&gt; Boolean) = b.negConditional(self, q) // Negation of Inverse Conditional. final def ⇍(q: =&gt; Boolean) = b.negInverseConditional(self, q) // Negation of Inverse Conditional. final def &lt;\\-(q: =&gt; Boolean) = b.negInverseConditional(self, q)    For instance:    scala&gt; true /\\ true res57: Boolean = true  scala&gt; true /\\ false res58: Boolean = false  scala&gt; true !&amp;&amp; false res59: Boolean = true    More information on additional functions  In this short article we've only shown you a couple of the additonal functions provided by Scalaz. If you want more information, the easiest is to just lookup the sources, which in this case, provide quite helpful information e.g:     scalaz.syntax.std.BooleanOps   scalaz.syntax.std.ListOps   scalaz.syntax.std.MapOps   scalaz.syntax.std.OptionOps   scalaz.syntax.std.StringOps  Some examples:  List fun    # get the tail as an option scala&gt; List(10,20,30) res60: List[Int] = List(10, 20, 30)  scala&gt; res60.tailOption res61: Option[List[Int]] = Some(List(20, 30))  scala&gt; List() res64: List[Nothing] = List()  scala&gt; res64.tailOption res65: Option[List[Nothing]] = None  # intersperse the list with additional elements scala&gt; List(10,20,30) res66: List[Int] = List(10, 20, 30)  scala&gt; res66.intersperse(1) res68: List[Int] = List(10, 1, 20, 1, 30)  # from list to List[List] of all possibilities scala&gt; List('a','b','c','d').powerset res71: List[List[Char]] = List(List(a, b, c, d), List(a, b, c), List(a, b, d), List(a, b), List(a, c, d), List(a, c), List(a, d), List(a), List(b, c, d), List(b, c), List(b, d), List(b), List(c, d), List(c), List(d), List())    Map fun    # alter one entry in a safe manner res77: scala.collection.immutable.Map[Char,Int] = Map(a -&gt; 10, b -&gt; 20) scala&gt; res77.alter('a')(f =&gt; f |+| some(5)) res78: Map[Char,Int] = Map(a -&gt; 15, b -&gt; 20)  # intersect two maps, and determine which value to keep of the keys that intersect scala&gt; val m1 =  Map('a' -&gt; 100, 'b' -&gt; 200, 'c' -&gt; 300) m1: scala.collection.immutable.Map[Char,Int] = Map(a -&gt; 100, b -&gt; 200, c -&gt; 300)  scala&gt; val m2 = Map('b' -&gt; 2000, 'c' -&gt; 3000, 'd' -&gt; 4000) m2: scala.collection.immutable.Map[Char,Int] = Map(b -&gt; 2000, c -&gt; 3000, d -&gt; 4000)  scala&gt; m1.intersectWith(m2)((m1v,m2v) =&gt; m2v) res23: Map[Char,Int] = Map(b -&gt; 2000, c -&gt; 3000)  scala&gt; m1.intersectWith(m2)((m1v,m2v) =&gt; m1v) res24: Map[Char,Int] = Map(b -&gt; 200, c -&gt; 300)    String fun    # Make a string plural (in a somewhat naive way) scala&gt; \"Typeclass\".plural(1) res26: String = Typeclass  scala&gt; \"Typeclass\".plural(2) res27: String = Typeclasss  scala&gt; \"Day\".plural(2) res28: String = Days  scala&gt; \"Weekly\".plural(2) res29: String = Weeklies  # safely parse booleans, bytes, shorts, longs, floats, double and ints scala&gt; \"10\".parseDouble res30: scalaz.Validation[NumberFormatException,Double] = Success(10.0)  scala&gt; \"ten\".parseDouble res31: scalaz.Validation[NumberFormatException,Double] = Failure(java.lang.NumberFormatException: For input string: \"ten\")    Conclusion  I hoped you liked this first short introduction into Scalaz. And as you've seen these simple functions already provide a lot of added value, without having to dive into the very complex inner workings of Scalaz. The pattern used here is just a simple TypeClass pattern used to add functionality to some standard Scala functionality.  In the next article, we'll look at some more complex functionality when we look at Monad Transformers.  ","categories": ["scala","scalaz"],
        "tags": [],
        "url": "http://www.smartjava.org/content/scalaz-features-everyday-usage-part-1-typeclasses-and-scala-extensions/",
        "teaser":null},{
        "title": "Scalaz features for everyday usage part 2: Monad Transformers and the Reader Monad",
        "excerpt":"For the second article of the \"Scalaz features for everyday usage\" we'll look at the subject of Monad transformers and the Reader monad.Let's start with Monad Transformers. Monad transformers come in handy when you have to deal with nested Monads, which happens suprisingly often. For instance when you have to work with nested Future[Option] or Future[Either], your for-comprehensions can quickly become unreadable, since you have to handle the None and Some cases for an Option and the Success and Failure cases explicitly. In this article I'll show some examples where Monad transformers come in handy, and how you can work with these.   The following articles are currently available in this series:      Scalaz features for everyday usage part 1: Typeclasses and Scala extensions    Scalaz features for everyday usage part 2: Monad Transformers and the Reader Monad   Scalaz features for everyday usage part 3: State Monad, Writer Monad and lenses  Working without Monad transformers  Like we mentioned in the introduction, Monad transformers are really useful when working with nested Monads. However, when would you encounter these? Well, a lot of Scala database libraries tend to be async (using Futures) and in some cases return Options. For instance you might query for a specific record which returns an Future[Option[T]]:   # from Phantom cassandra driver (without implicits), which returns Some(Record) if found # or None if nothing can be found def one(): Future[Option[Record]]  # or you might want to get the first element from a Slick query val res : Future[Option[Row]] = db.run(filterQuery(id).result.headOption)   Or you might just have your own trait or service defining functions that will eventually return an Option or an Either as a result:   # get an account, or none if no account is found def getAccount() : Future[Option[Account]]  # withdraw an amount from an account, returning either the new amount in the account # or a message explaining what went wrong def withdraw(account: Account, amount: Amount) : Future[\\/[String, Amount]]    Lets look at an example piece of ugly code you would get when not using Monad transformers:   def withdrawWithoutMonadTransformers(accountNumber: String, amount: Amount) : Future[Option[Statement]] = {   for {     // returns a Future[Option[Account]]     account &lt;- Accounts.getAccount(accountNumber)     // we can do a fold, using scalaz for the typed None, since a None isn't typed     balance &lt;- account.fold(Future(none[Amount]))(Accounts.getBalance(_))     // or sometimes we might need to do a patten match, since we've got two options     _ &lt;- (account, balance) match {       case (Some(acc), Some(bal)) =&gt; Future(Accounts.withdraw(acc,bal))       case _ =&gt; Future(None)     }     // or we can do a nested map     statement &lt;- Future(account.map(Accounts.getStatement(_)))   } yield statement }    As you can see when we've got to work with nested Monads, we need to handle the nested one in the right side of each step in the for-comprehensions. Scala is rich enough in its language that we've got plenty of different way to do this, but the code doesn't get more readable. We have to resort to nesting maps or flatmaps, using folds (in the case of an Option) or sometimes have to resort to pattern matching, if we're interested in multiple Options. There are probably other ways to do this, but all in all, the code doesn't get more readable. Since we have to deal with the nested Option explicitly.  Now with Monad transformers  With Monad transformers we can remove all this boilerplate, and get a very convenient way of working with these kind of nested constructs. Scalaz provides Monad transformers for the following types:   BijectionT EitherT IdT IndexedContsT LazyEitherT LazyOptionT ListT MaybeT OptionT ReaderWriterStateT ReaderT StateT StoreT StreamT UnWriterT WriterT    While some of these might seem a bit exotic, ListT, OptionT, EitherT, ReaderT and WriterT can be applied in whole lot of use cases. In this first example lets focus on the OptionT Monad transformer. First lets look at how we can create an OptionT monad. For our example we're going to create an OptionT[Future, A], which wraps an Option[A] inside a Future. We can create these like this from an A:   scala&gt; :require /Users/jos/.ivy2/cache/org.scalaz/scalaz-core_2.11/bundles/scalaz-core_2.11-7.2.1.jar Added '/Users/jos/.ivy2/cache/org.scalaz/scalaz-core_2.11/bundles/scalaz-core_2.11-7.2.1.jar' to classpath.  scala&gt; import scalaz._ import scalaz._  scala&gt; import Scalaz._ import Scalaz._                                ^  scala&gt; import scala.concurrent.Future import scala.concurrent.Future  scala&gt; import scala.concurrent.ExecutionContext.Implicits.global import scala.concurrent.ExecutionContext.Implicits.global  scala&gt; type Result[A] = OptionT[Future, A] defined type alias Result  scala&gt; 1234.point[Result] res1: Result[Int] = OptionT(scala.concurrent.impl.Promise$DefaultPromise@1c6ab85)  scala&gt; \"hello\".point[Result] res2: Result[String] = OptionT(scala.concurrent.impl.Promise$DefaultPromise@3e17219)  scala&gt; res1.run res4: scala.concurrent.Future[Option[Int]] = scala.concurrent.impl.Promise$DefaultPromise@1c6ab85     Note that we define an explicit type Result to be able to make point work. If you don't do this you'll get helpful error messages regarding type construction:   scala&gt; \"why\".point[OptionT[Future, String]] &lt;console&gt;:16: error: scalaz.OptionT[scala.concurrent.Future,String] takes no type parameters, expected: one               \"why\".point[OptionT[Future, String]]    You can only use point when you're dealing with the innervalue of the monad transformer. If you've already got a Future or an Option, we need to use the OptionT constructor instead.   scala&gt; val p: Result[Int] = OptionT(Future.successful(some(10))) p: Result[Int] = OptionT(scala.concurrent.impl.Promise$KeptPromise@40dde94)    With a Monad transformer we can automatically unwrap a nested Monad. Now that we now how to convert our values to an OptionT, lets look back at the example we previously saw, and rewrite it like this:   def withdrawWithMonadTransformers(accountNumber: String, amount: Amount) : Future[Option[Statement]] = {    type Result[A] = OptionT[Future, A]    val result = for {     account &lt;- OptionT(Accounts.getAccount(accountNumber))     balance &lt;- OptionT(Accounts.getBalance(account))     _ &lt;- OptionT(Accounts.withdraw(account,balance).map(some(_)))     statement &lt;- Accounts.getStatement(account).point[Result]   } yield statement    result.run }    Nice right? Instead of all the noise massaging the types into the correct type, we just create OptionT instances and return those. To get the stored value out of the OptionT we just call run.  Even though it is already much more readable. We now have the noise for create the OptionT. Even though it isn't much noise, it still is kind of distracting.  And some more syntax cleanup  We can even clean it up a bit more:   // with Monad transformers type Result[A] = OptionT[Future, A]  /**   * Unfortunately we can't use overloading, since we then run into   * type erase stuff, and the thrush operator not being able to find   * the correct apply function   */ object ResultLike {   def applyFO[A](a: Future[Option[A]]) : Result[A] = OptionT(a)   def applyF[A](a: Future[A]) : Result[A] = OptionT(a.map(some(_)))   def applyP[A](a: A) : Result[A] = a.point[Result] }  def withdrawClean(accountNumber: String, amount: Amount) : Future[Option[Statement]] = {    val result: Result[Statement] = for {     account &lt;- Accounts.getAccount(accountNumber)         |&gt; ResultLike.applyFO     balance &lt;- Accounts.getBalance(account)               |&gt; ResultLike.applyFO     _ &lt;- Accounts.withdraw(account,balance)               |&gt; ResultLike.applyF     statement &lt;- Accounts.getStatement(account)           |&gt; ResultLike.applyP   } yield statement    result.run }    In this approach we just create specific converters to get the results into an OptionT monad. The result is that the actual for-comprehensions is very readable, without any cruft. And at the right hand side, out of immediate eyesight, we do the conversion to OptionT. Note that this isn't the most clean solution, since we need to specify different apply functions. Overloading here doesn't work, since after type erasure the applyFO and applyF will have the same signature.  Reader monad  The Reader Monad is one of the standard monads provided by Scalaz. The Reader monad can be used to easily pass configuration (or other values) around, and can be used for stuff like dependency injection.  The Reader monad solution  The Reader monad allows you to do dependency injection in scala. Whether that dependency is a configuration object, a reference to some other service, doesn't really that much. We start with an example, since that best explains how to use a reader monad.  For this example we'll assume we have a service which requires a Session to do stuff. This could be a database session, some web service session, or something else. So lets replace the previous sample to this, and for now simplify it a bit by removing the futures:   trait AccountService {   def getAccount(accountNumber: String, session: Session) : Option[Account]   def getBalance(account: Account, session: Session) : Option[Amount]   def withdraw(account: Account, amount: Amount, session: Session) : Amount   def getStatement(account: Account, session: Session): Statement }  object Accounts extends AccountService {   override def getAccount(accountNumber: String, session: Session): Option[Account] = ???   override def getBalance(account: Account, session: Session): Option[Amount] = ???   override def withdraw(account: Account, amount: Amount, session: Session): Amount = ???   override def getStatement(account: Account, session: Session): Statement = ??? }    This seems a bit annoying, since everytime we want to call one of the services, we need to provide an implementation of Session. We could of course make Session implicit, but then we still need to make sure it is in scope, when we call the functions of this service. It would be nice if we could find a way to inject this session somehow. We could of course do this in the constructor of this service, but we could also use a Reader monad for this, which changes the code to this:   // introduce a Action type. This represents an action our service can execute. As you can see in // the declaration, this Action, requires a Session. type Action[A] = Reader[Session, A]  trait AccountService {   // return an account, or return none when account can't be found   def getAccount(accountNumber: String) : Action[Option[Account]]   // return the balance when account is opened, or none when it isn't opened yet   def getBalance(account: Account) :Action[Option[Amount]]   // withdraw an amount from the account, and return the new amount   def withdraw(account: Account, amount: Amount) : Action[Amount]   // we can also get an account overview statement, which somehow isn't async   def getStatement(account: Account): Action[Statement] }  object Accounts extends AccountService {   override def getAccount(accountNumber: String): Action[Option[Account]] = Reader((session: Session) =&gt; {     // do something with session here, and return result     session.doSomething     some(Account())   })    override def getBalance(account: Account): Action[Option[Amount]] = Reader((session: Session) =&gt; {     // do something with session here, and return result     session.doSomething     some(Amount(10,\"Dollar\"))   })    override def withdraw(account: Account, amount: Amount): Action[Amount] = Reader((session: Session) =&gt; {     // do something with session here, and return result     session.doSomething     Amount(5, \"Dollar\")   })    override def getStatement(account: Account): Action[Statement] = Reader((session: Session) =&gt; {     // do something with session here, and return result     session.doSomething     Statement(account)   }) }    As you can see, we're not returning the result, but wrap the result in a Reader. The cool thing is that we now can start composing stuff, since the Reader is just a monad.   def withdrawWithReader(accountNumber: String) = {    for {     account &lt;- Accounts.getAccount(accountNumber)     balance &lt;- account.fold(Reader((session: Session) =&gt; none[Amount]))(ac =&gt; Accounts.getBalance(ac))     _ &lt;- (account, balance) match {       case (Some(acc), Some(bal)) =&gt; Accounts.withdraw(acc,bal)       case _ =&gt; Reader((session: Session) =&gt; none[Amount])     }   statement &lt;- account match { case Some(acc) =&gt; Accounts.getStatement(acc)}   } yield statement }    This won't return the actual final value, but will return a Reader. We can now run the code by passing in a Session:   // function returns 'steps' to execute, run execute these steps in the context of 'new Session' withdrawWithReader(\"1234\").run(new Session())    When you look back at the withdrawWithReader function you can see that we once again have to manage the Option monad explicitly, and make sure that we always create a Reader as a result. Luckily, though, Scalaz also provides a ReaderT, which we could use to automatically handle a specific type of Monad. In the following code we've shown how to do this for this example:   // introduce a Action type. This represents an action our service can execute. As you can see in // the declaration, this Action, requires a Session. type Action[A] = ReaderT[Option, Session, A]  trait AccountService {   // return an account, or return none when account can't be found   def getAccount(accountNumber: String) : Action[Account]   // return the balance when account is opened, or none when it isn't opened yet   def getBalance(account: Account) :Action[Amount]   // withdraw an amount from the account, and return the new amount   def withdraw(account: Account, amount: Amount) : Action[Amount]   // we can also get an account overview statement, which somehow isn't async   def getStatement(account: Account): Action[Statement] }  object Accounts extends AccountService {   override def getAccount(accountNumber: String): Action[Account] = ReaderT((session: Session) =&gt; {     // do something with session here, and return result     session.doSomething     some(Account())   })    override def getBalance(account: Account): Action[Amount] = ReaderT((session: Session) =&gt; {     // do something with session here, and return result     session.doSomething     some(Amount(10,\"Dollar\"))   })    override def withdraw(account: Account, amount: Amount): Action[Amount] = ReaderT((session: Session) =&gt; {     // do something with session here, and return result     session.doSomething     Some(Amount(5, \"Dollar\"))   })    override def getStatement(account: Account): Action[Statement] = ReaderT((session: Session) =&gt; {     // do something with session here, and return result     session.doSomething     Some(Statement(account))   }) }  def withdrawWithReaderT(accountNumber: String) = {   for {     account &lt;- Accounts.getAccount(accountNumber)     balance &lt;- Accounts.getBalance(account)     _ &lt;- Accounts.withdraw(account, balance)     statement &lt;- Accounts.getStatement(account)   } yield statement }  withdrawWithReaderT(\"1234\").run(new Session)    As you can see, not that much has changed. The main thing we changed was changing the declaration of Action to use a ReaderT instead of a Reader, and we change the trait and the implementation to work with that. Now when you look at the withdrawWithReaderT function you can see that we don't need to handle the Option anymore, but it is handled by our ReaderT (which actually is a Kleisli but that's something for another artile). Cool right?  While this works great for just an Option, what would happen if we go back to the original example and want to deal with an Option nested inside a Future, and these once again inside an Reader? Well at that point, we might pass out of the scope of \"Scalaz features for everyday usage\", but the basic set up is the same:   // introduce a Action type. This represents an action our service can execute. As you can see in // the declaration, this Action, requires a Session. type OptionTF[A] = OptionT[Future, A] type Action[A] = ReaderT[OptionTF, Session, A]  trait AccountService {   // return an account, or return none when account can't be found   def getAccount(accountNumber: String) : Action[Account]   // return the balance when account is opened, or none when it isn't opened yet   def getBalance(account: Account) :Action[Amount]   // withdraw an amount from the account, and return the new amount   def withdraw(account: Account, amount: Amount) : Action[Amount]   // we can also get an account overview statement, which somehow isn't async   def getStatement(account: Account): Action[Statement] }  /**   * Normally you would wrap an existing service, with a readerT specific one, which would handle   * all the conversion stuff.   */ object Accounts extends AccountService {   override def getAccount(accountNumber: String): Action[Account] = ReaderT((session: Session) =&gt; {     // do something with session here, and return result     session.doSomething     // Assume we get a Future[Option[Account]]     val result = Future(Option(Account()))      // and we need to lift it in the OptionTF and return it.     val asOptionTF: OptionTF[Account] = OptionT(result)     asOptionTF   })    override def getBalance(account: Account): Action[Amount] = ReaderT((session: Session) =&gt; {     // do something with session here, and return result     session.doSomething     // assume we get a Future[Option[Amount]]     val result = Future(some(Amount(10,\"Dollar\")))     // convert it to the Action type, with explicit type to make compiler happy     val asOptionTF: OptionTF[Amount] = OptionT(result)     asOptionTF   })    override def withdraw(account: Account, amount: Amount): Action[Amount] = ReaderT((session: Session) =&gt; {     // do something with session here, and return result     session.doSomething     // assume we get a Future[Amount]     val result = Future(Amount(5, \"Dollar\"))     // convert it to the correct type     val asOptionTF: OptionTF[Amount] = OptionT(result.map(some(_)))     asOptionTF   })    override def getStatement(account: Account): Action[Statement] = ReaderT((session: Session) =&gt; {     // do something with session here, and return result     session.doSomething     // assume we get a Statement     val result = Statement(account)     // convert it to the correct type     result.point[OptionTF]   }) }  def withdrawWithReaderT(accountNumber: String) = {   for {     account &lt;- Accounts.getAccount(accountNumber)     balance &lt;- Accounts.getBalance(account)     _ &lt;- Accounts.withdraw(account, balance)     statement &lt;- Accounts.getStatement(account)   } yield statement }  // this is the result wrapped in the option val finalResult = withdrawWithReaderT(\"1234\").run(new Session) // get the Future[Option] and wait for the result println(Await.result(finalResult.run, 5 seconds))    We define a different ReaderT type, where we pass in an OptionT instead of just an Option. This OptionT will handle the Option/Future transformation. When we've got a new ReaderT we of course need to lift the results from our service calls to this monad, which needs some type coersion for the compiler to understand everything (Intellij, also doesn't understand this anymore). The result though is very nice. The actual for-comprehension stays exactly the same, but this time can handle Option inside Future inside a Reader!  Conclusions  In this article we've looked at two parts of Scalaz which really come in handy when dealing with nested monads, or when you want to better manage dependencies between components. The cool thing is, that it is fairly easy to use Monad Transformers together with the Reader monad. The overall result is, is that with a couple of small steps, we can completely hide the details of working (in this case) with the Future and Option moand, and have nice and clean for-comprehensions and other monadic goodies.  ","categories": ["posts","monad","scala","scalaz"],
        "tags": [],
        "url": "http://www.smartjava.org/content/scalaz-features-everyday-usage-part-2-monad-transformers-and-reader-monad/",
        "teaser":null},{
        "title": "Service Discovery with Docker and Consul: part 2",
        "excerpt":"So, welcome to the second part of the series on using Consul for service discovey together with docker. In this second article we'll look at how you can use a number of Consul related tools, so make service discovery, and a couple of other related functionalities a lot easier.  For the other articles in this series you can look here:     Presentation on Service discovery with consul   Service discover with Docker and Consul: Part 1  Review architecture and what are we going to do  In this article we'll expand on the setup we created in the previous article. In that article we created the following microservices architecture using Docker and Consul:    So, if you haven't done so, walk through the steps in that article, and make sure you've got a setup that can use Docker Swarm, Docker Compose and has the correct Docker network setup. To check whether everything is setup correctly, check the output of the following commands:   # See the previous article for the definitions of these commands # # First, check Consul. If consul isn't up, the swarm won't come up or work.  $ . dm-env nb-consul $ docker ps --format '\\t\\t\\t'  b5d55e6df248       progrium/consul \"/bin/start -server -\"  clever_panini  # if consul isn't up, make sure to start it, before trying any of the swarm # commands. $ . dm-env nb1 --swarm $ docker ps -a --format '\\t\\t\\t'  bf2000882dcc    progrium/consul \"/bin/start -ui-dir /\"  nb1/consul_agent_1 a1bc26eef516    progrium/consul \"/bin/start -ui-dir /\"  nb2/consul_agent_2 eb0d1c0cc075    progrium/consul \"/bin/start -ui-dir /\"  nb3/consul_agent_3 d27050901dc1    swarm:latest    \"/swarm join --advert\"  nb3/swarm-agent f66738e086b8    swarm:latest    \"/swarm join --advert\"  nb2/swarm-agent 0ac59ef54207    swarm:latest    \"/swarm join --advert\"  nb1/swarm-agent 17fc5563d018    swarm:latest    \"/swarm manage --tlsv\"  nb1/swarm-agent-master  # this should at least list the swarm master, swarm agents, and the consul agents. # if not, see previous article on how to setup your environment.  # the last thing we need to check is our network. When the swarm master is selected # run the following command: $ docker network ls | grep -i my-net 8ecec72e7b68        my-net                overlay  # if it shows an overlay network with my-net we're all set.    Should you see some frontend or backend services, it's probably best to stop and remove them. That way you can follow the commands in the rest of this article.  So what will we be doing in this article. Well, we'll show you how to use the following two tools from the Consul universe:     Consultemplate: With Consultemplate you can listen to events from Consul (e.g when a service is added), and based on these updates, rewrite and reload a configuration file. We'll use this to automatically update the configuration of an HAProxy based reverse proxy, with the latest set of healthy services.   EnvConsul: With Envconsul you can easily read environment variables directly from Consul, instead of having to pass them in manually when you start a docker container. We will show how you could use this with the services we use in these articles.  Consultemplate, docker and HAProxy  In the previous setup we created an architecture as shown in the figure above. While this already worked great by using the DNS functions of Consul for service discovery and some basic failover, we had to rely on DNS and socket timeouts, to determine when a service became unhealthy. While this worked, it was not that reliable, and only offered some simple failover and loadbalancing functionality. What we're going to do in this scenario is create the following:    So the scenario we'll have is the following:     A user will access our frontend service through the HAProxy.   HAProxy will forward the request to one of the healthy services.   The fronted service, wants to access a backend service. It also does this through the HAProxy component.  Consul will make sure that whenever a service registers itself with Consul, it updates the configuration of HAProxy.  How does it work  If you've pulled the repository for these articles (https://github.com/josdirksen/next-build-consul) you can also find a directory called extra/consul-template. In that directory is the HAProxy that we'll use for this example. I've also added it to dockerhub (https://hub.docker.com/r/josdirksen/demo-haproxy/), which makes using it a bit easier. Before we look at how we need to define our template, lets look at what this docker image does. Easiest is to look at the startup script:   #!/bin/bash  HAPROXY=\"/etc/haproxy\" PIDFILE=\"/var/run/haproxy.pid\" CONFIG_FILE=${HAPROXY}/haproxy.cfg  cd \"$HAPROXY\"  haproxy -f \"$CONFIG_FILE\" -p \"$PIDFILE\" -D -st $(cat $PIDFILE)  /usr/local/bin/consul-template -consul=${CONSUL_ADDRESS} -config=/consul.hcl    Nothing too special here, what happens it that when we start this container, consul-template will run with the specified configuration file. Note that we need to provide the CONSUL_ADDRESS environment variable to point consul-template to either one of our agents, or the consul server. The interesting stuff is in the consul.hcl file:   max_stale = \"10m\" retry     = \"10s\" wait      = \"5s:20s\"  template {   source = \"/etc/haproxy/haproxy.template\"   destination = \"/etc/haproxy/haproxy.cfg\"   command = \"/hap.sh\"   perms = 0600 }     this file is pretyy self explanatory. Basically what happens is that whenever something changes in Consul, the template haproxy.template will be ran against the information in Consul, and the result will replace the haproxy.cfg file. After that the hap.sh command is run to reload the configuration. For completeness sake, the hap.sh file looks like this:   #!/bin/bash  haproxy -f /etc/haproxy/haproxy.cfg -p /var/run/haproxy.pid -D -st $(cat /var/run/haproxy.pid)    What this does, is reload the configuration file in a clean manner. So, what is in the template? Lets look at the haproxy.template:   global   log 127.0.0.1 local0   log 127.0.0.1 local1 notice   chroot /var/lib/haproxy   user haproxy   group haproxy  defaults   log global   mode http   option httplog   option dontlognull   balance roundrobin   timeout connect 5000   timeout client 50000   timeout server 50000   errorfile 400 /etc/haproxy/errors/400.http   errorfile 403 /etc/haproxy/errors/403.http   errorfile 408 /etc/haproxy/errors/408.http   errorfile 500 /etc/haproxy/errors/500.http   errorfile 502 /etc/haproxy/errors/502.http   errorfile 503 /etc/haproxy/errors/503.http   errorfile 504 /etc/haproxy/errors/504.http  listen stats   bind *:8001   stats enable   stats uri /   stats auth admin:123123q   stats realm HAProxy\\ Statistics  frontend nb-front   bind *:1080   mode http   default_backend nb-frontend  frontend nb-back   bind *:1081   mode http   default_backend nb-backend  backend nb-frontend     balance roundrobin     server  : check  backend nb-backend    balance roundrobin    server  : check    The first couple of lines aren't interesting. It becomes interesting where we define the frontend and the backend elements. What we say here is that we specify that haproxy will listen to request for the frontend at port 1080, and forward those requests to the services defined in backend nb-frontend. In this element we configure a template. In this example we take all the services with the name frontend-service from Consul, and for each service we write down an entry. So when we call haproxy on port 1080, it'll forward the request to any of the services with the name frontend-service registerd in Consul. We do exactly the same for the backend-service.  Run it!  So, now that we know how it works, it's time to run this haproxy. We've defined a docker-compose file for this, which will run HAProxy in node nb1:   version: '2'  services:   nb-proxy:     image: josdirksen/demo-haproxy     container_name: nb-haproxy     ports:       - 1080:1080       - 1081:1081     environment:       - CONSUL_ADDRESS=192.168.99.106:8500       - \"constraint:node==nb1\"  networks:   default:     external:       name: my-net    Execute the following commands to do this:   # make sure we're at the swarm master $ . dm-env nb1 --swarm  # in the root of the nextbuild-consul project $ docker-compose -f ./docker-compose-haproxy.yml up -d Creating nb-haproxy  $ docker ps -a --format '\\t\\t\\t'  dc28caa4c420    josdirksen/demo-haproxy \"/startup.sh\"   nb1/nb-haproxy bf2000882dcc    progrium/consul \"/bin/start -ui-dir /\"  nb1/consul_agent_1 a1bc26eef516    progrium/consul \"/bin/start -ui-dir /\"  nb2/consul_agent_2 eb0d1c0cc075    progrium/consul \"/bin/start -ui-dir /\"  nb3/consul_agent_3 d27050901dc1    swarm:latest    \"/swarm join --advert\"  nb3/swarm-agent f66738e086b8    swarm:latest    \"/swarm join --advert\"  nb2/swarm-agent 0ac59ef54207    swarm:latest    \"/swarm join --advert\"  nb1/swarm-agent 17fc5563d018    swarm:latest    \"/swarm manage --tlsv\"  nb1/swarm-agent-master    In my setup I can see that the HAProxy has started. But since we haven't got any backend or frontend services running, the configuration of haproxy should reflect that:   $ docker exec -ti nb1/nb-haproxy cat /etc/haproxy/haproxy.cfg | tail -n 15 frontend nb-front   bind *:1080   mode http   default_backend nb-frontend  frontend nb-back   bind *:1081   mode http   default_backend nb-backend  backend nb-frontend     balance roundrobin  backend nb-backend    balance roundrobin    And if we open up either port 1080 (for the frontend) or 1081 (for the backend API), we'll see an error from HAProxy.    This is to be expected, since we haven't got any frontend or backend service running. So lets give HAProxy a number of backend services to work with:   $ docker-compose -f ./docker-compose-backend.yml up -d Creating Backend2 Creating Backend3 Creating Backend1    Now we should have a number of backends running behind HAProxy. First lets check if Consul updated our HAProxy instance:   $ docker exec -ti nb1/nb-haproxy cat /etc/haproxy/haproxy.cfg | tail -n 8 backend nb-frontend     balance roundrobin  backend nb-backend    balance roundrobin    server a1bc26eef516 10.0.9.7:8081 check    server bf2000882dcc 10.0.9.9:8081 check    server eb0d1c0cc075 10.0.9.8:8081 check    Cool, right! Haproxy now has three services defined. This should allow us to call http://nb1.local:1081 and it should return the API of one of the backend services:    If you refresh a couple of times, you should see it cycle throught the various services.  And if we kill one, we should see it automatically skip the killed one:   $ docker stop nb1/Backend1 nb1/Backend1  $ curl nb1.local:1081          {\"result\" : {           \"servername\" : \"Server2\",           \"querycount\" : 80           }         } $ curl nb1.local:1081          {\"result\" : {           \"servername\" : \"Server3\",           \"querycount\" : 86           }         }    Now lets see if our frontend services can use this in the same manner. For this we start the frontend components like this:   $ docker-compose up -f ./docker-compose-frontend-proxy  # remember we killed one of the backend services, so we only see two backend ones $ docker exec -ti nb1/nb-haproxy cat /etc/haproxy/haproxy.cfg | tail -n 10 backend nb-frontend     balance roundrobin     server a1bc26eef516 10.0.9.11:8090 check     server bf2000882dcc 10.0.9.9:8090 check     server eb0d1c0cc075 10.0.9.10:8090 check  backend nb-backend    balance roundrobin    server a1bc26eef516 10.0.9.7:8081 check    server eb0d1c0cc075 10.0.9.8:8081 check    And it looks like HAProxy updated correctly with the new services. Now we should be able to call port 1080 on haproxy, to get one of the frontend service, and use the button to call one of the available backends services (once again through HAproxy).    And it works as expected! If you refresh this page, you'll see it cycle through the frontend services, and when you hit the button a couple of times, it will only call the services that are available. And, as you'd expect, once we start backend service one again, it'll show up in the list when hitting the button:   $ docker start nb1/Backend1 nb1/Backend1    Results in:    HAProxy and consul template Conclusions  This was a very quick introduction into using Docker together with Consultemplate and HAProxy. As you've seen, getting all this to work is rather easy. Once you've got docker and consul configured tying in extra components becomes much easier. In a real-world scenario we'd also have HAProxy register itself with Consul, so other services can easily find the HAProxy instance.  As the last part of this article, we'll have a quick look at some of the feature EnvConsul provies.  EnvConsul, easily provide environment properties when starting up an application.  The normal way of passing configuration data into application (especially in a docker / microservices architecture) is by using environment variables. This is an easy, non-intrusive, language agnostic way of configuring your services. It's even one of the subjects of the 12 factor app.  \"The twelve-factor app stores config in environment variables (often shortened to env vars or env). Env vars are easy to change between deploys without changing any code; unlike config files, there is little chance of them being checked into the code repo accidentally; and unlike custom config files, or other config mechanisms such as Java System Properties, they are a language- and OS-agnostic standard.\"  However, it becomes very cumbersome when you're dealing with big applications, or large configurations, especially when you have to deal with escaping quotes / linebreaks etc.  Luckily we can solve this problem with Consul. As you probably know by now, Consul is also a distributed Key-Value store:    With EnvConsul, we can use information from the KV store inside Consul as environment parameters before starting our application. So how would this work? We can test this very easy, since EnvConsul is just a golang executable which we can run. I've added the Mac version in the repository (in the extras directory), but you can download builds for your OS from here: https://releases.hashicorp.com/envconsul/0.6.1/  We'll just run it, and see what happens:   ./envconsul -consul=nb-consul.local:8500 -prefix docker -once env ... network/v1.0/endpoint/815dd44b77f391bd9a63f4e107aa1a7d3f371c91427abcf4be34493aa7ec25cd/= nodes/192.168.99.112:2376=192.168.99.112:2376 swarm/nodes/192.168.99.110:2376=192.168.99.110:2376 ...    I've left most of the result out, but basically what we do here, is use env-consul to get all the keys it has stored in the docker tree, and add those as environment variables. After these are set we run the env command, which just outputs all the environment variables we have. If you run this yourself you'll see a whole lot of docker-network and docker-swarm related information set as environment variables.  We can of course also set a number of KV pairs ourselves:    And when we retrieve those, we can see that the values are passed directly into our command as environment variables:   $ ./envconsul -consul=nb-consul.local:8500 -prefix smartjava -once env | tail -n 2 key2=The value of key 2 key1=The Value of Key 1    This, however, isn't everything you can do with envconsul. It also provides an easy way to react to changes in the key value pairs. Imagine you have a service running, which is configured, through an env property. And if that env property changes, you should in fact restart the services. This is something EnvConsul can do for you:   $ cat env.sh #!/bin/bash env tail -f /dev/null  $ ./envconsul -consul=nb-consul.local:8500 -pristine -prefix=smartjava ./env.sh PWD=/Users/jos/dev/git/nextbuild-consul/extra/envconsul SHLVL=1 key2=The value of key 2 key1=The Value of Key 1 _=/usr/bin/env    At this point, the process will keep running, and envconsul will wait for updates to the Consul keystore. So when we update something there, this is the result:    As you can see in this image, whenever we change a value in the KV store, our script is restarted with the new configuration passed in as env variables.  Conclusions  In this article, we showed that using HAProxy is actually very easy to do. You can incorporate it in your architecture using the same concepts and ideas we saw in the previous article. Combine this with ConsulTemplate, and getting a highly configurable, software loadbalancer setup is a piece of cake.  Another cool piece of technology is EnvConsul, this nicely integrates with the distributed KV store provided by Consul. It even provides functionality to restart your application on configuration changes, which is a really powerful feature.  And that wraps it up for the second article in this series. In the next part of this series we'll look at how you can simplify registration of services with Consul. So replace the custom script we showed in the previous article with a more advanced solution.  ","categories": ["posts","consul","docker"],
        "tags": [],
        "url": "http://www.smartjava.org/content/service-discovery-docker-and-consul-part-2/",
        "teaser":null},{
        "title": "Render 3D Star Wars: The Force Awakens models in Blender and Three.js",
        "excerpt":"Recently video copilot released a free set of high-definition professional grade 3D models (which you can download for yourself from here). This model pack, which is over 400MB big) contains highly detailed model with a number of very high-quality textures. In this short article, I'll show you the workflow of converting these models into a format more easy to use in Three.js, and show you how you can render these in your browser. If your're impatient you can view the result here: http://www.smartjava.org/examples/threejs-starwars/src/html/  In this short article we'll take the following steps:     Use blender to explore the provided materials and textures.   Resize and convert the texture files to a more manageable format.   Shrink the model.   Use Three.js to render the final model.  Using Blender  I'm using Blender for the conversion and some testing, but you can use other tools as well. An advantage, though, of using Blender together with Three.js, is that you can use the Three.js Blender plugin, to export models in a format easily readable by Three.js. In this case, however, we'll just use the obj format, which is also supported by three.js.  Once you've downloaded the models, extract them somewhere and startup Blender. Remove the standard cube in the middle and use file-&gt;Import    Navigate to the directory where you extraced the models and select the one you want to load. For this short tutorial, I've used the R2D2 model. After the model is loaded you might need to zoom ut a bit to see the real model:    Already looks cool, right? It's a relatively low-poly model, which uses textures to create the renders. We however, need to take a couple of steps to 'fix' the model in Blender. When you import the model, and try to render it for the first time, or use material or texture viewport shading, you won't see a detailled R2D2 model, but just a gray one:    So, there seems to be something wrong with the materials. What the problem is, is that the image files aren't correctly mapped in the obj files (I assume they are in the Element 3D file format, but I don't have After Effects or Element 3D, so can't check). So we need to do some fixing. The advantage is that the UV mapping, luckily, is present for this model:    Now, we can just add the diffuse map, and we should see a correctly rendered model (also make sure you set the lights correctly, or else you'll get a completely black model). Select one of the components of the model (e.g the left foot), and in the material tab you'll see a texture named R2D2.001.    Select that one, and then select the texture tab. Create a new texture, here and assign the R2D2_Diffuse.dds from the Maps directory. This'll result in the following texture definition:    And if you now look at our R2D2 model, you'll see that we've got an already nicely rendered model.    Note that the images are very large 23MB per image, but we'll look later at how to shrink those. If you look into the directory you'll notice a couple of other image files:     R2D2_Diffuse.dds: The diffuse map contains the colors and texture for the various parts of the model. This is what makes the model look like an actual R2D2 when rendered.   R2D2_Normal.dds: This is a bump map (name is a bit confusing, since it doesn't seem to be a real normal map). When you apply this map, you can add bumps and depth to the model. The gray value define the height of the bump.   R2D2_Illumination.dds: This map defines the part of the model that emit light in a specific color. This provides R2D2 with it's red sensor and shows some additional parts of the model that output light.   R2D2_Reflection.dds: This map seems to add some additional detail and color to the model. It doesn't seem to contain actual reflections, but it does provide some better coloring after applying.   R2D2_Specular.dds: And this maps finally defines the shininess of your model.  And when you apply them, you'll get a model that looks something like this (quick render, without any material optimization):    When you apply them all, and render the model, it looks quite nice. But this is +100MB worth of textures, so that might be bit much if we want to use this model and textures from Three.js in our browser. Before we look at the Three.js part,  Creating smaller texture files  First we'll use ImageMagick to convert the dds files to PNGs.   # and to the others as well $ convert -define png:compression-filter=2 -define png:compression-level=9 -define png:compression-strategy=1 R2D2_Reflection.DDS R2D2_Reflection.png $ ls -l R2* -rwxr-xr-x@ 1 jos  staff  22369776 May  4 18:25 R2D2_Diffuse.dds -rw-r--r--  1 jos  staff  17350674 May  5 13:58 R2D2_Diffuse.png -rwxr-xr-x@ 1 jos  staff  22369776 May  4 18:25 R2D2_Illumination.DDS -rw-r--r--  1 jos  staff    327877 May  5 13:24 R2D2_Illumination.png -rwxr-xr-x@ 1 jos  staff  22369776 May  4 18:25 R2D2_Normal.DDS -rw-r--r--  1 jos  staff  14249444 May  5 14:00 R2D2_Normal.png -rwxr-xr-x@ 1 jos  staff  22369776 May  4 18:25 R2D2_Reflection.DDS -rw-r--r--  1 jos  staff  13336562 May  5 13:57 R2D2_Reflection.png -rwxr-xr-x@ 1 jos  staff  22369776 May  4 18:25 R2D2_Specular.DDS -rw-r--r--  1 jos  staff  18223665 May  5 14:00 R2D2_Specular.png    As you can see, some of the images shrink a lot, other a whole lot less. The main reason they take up so much space is because the images are rather big:   $ identify R2D2_Diffuse.png R2D2_Diffuse.png PNG 4096x4096 4096x4096+0+0 8-bit sRGB 18.63MB 0.010u 0:00.000    While this is nice, if want to make realistic looking renders, but it is a bit overkill for when we want to render these models using Three.js. So let's also convert them to some smaller texture sizes (1024x1024):   $ for file in R2D2*.png do     convert -resize 25% \"$file\" \"$(basename \"$file\" .png)-small.png\" done  $ ls -l R2*-small.png -rw-r--r--  1 jos  staff   1771310 May  5 14:07 R2D2_Diffuse-small.png -rw-r--r--  1 jos  staff     47188 May  5 14:07 R2D2_Illumination-small.png -rw-r--r--  1 jos  staff   1190588 May  5 14:08 R2D2_Normal-small.png -rw-r--r--  1 jos  staff   1560293 May  5 14:08 R2D2_Reflection-small.png -rw-r--r--  1 jos  staff   1650999 May  5 14:08 R2D2_Specular-small.png    Now, that looks a lot nicer. Instead of 110MB of textures, we have +/- 6MB of textures. This of course will affect how our model looks. If we rerender the model using the same settings with these textures instead of the HQ ones, we get the following render:    As you can see, still looks very nice. As long as you don't zoom into too much, that is. At this point, we've got a set of smaller textures, so the next  Shrink the model  We were able to drastically scale down the size of the textures. We, however, still have the large model we need to load, which comes in at a 4.5MB. While, not being really that large, it is still rather large. To solve this we can reduce the number of vertices, which will also make rendering quicker, or compress the obj file. While testing with the original number of vertices, there didn't seem to be any stuttering or other issues, so let's keep the number of vertices as they are. The easiest way to make the download quicker is by just enabling gzip support on your server:   # for example the normal and gzipped sizes: -rwxr-xr-x    1 jos  staff   4513121 May  4 18:25 R2D2_Standing.obj -rwxr-xr-x    1 jos  staff    987858 May  4 18:25 R2D2_Standing.obj.gz    If you don't want to do that, you might look into some of the javascript based gunzip libraries to decompress a gzipped file yourself. So to limit the size, we'll just assume we have a webserver which returns gzipped data.  Running in Three.js  Now what do you need to do, to get this running in Three.js. First off you can find a simple running example here: Three.js rendering of R2D2. You can use the mouse controls to orbit around R2D2.    Since you can see the complete sources in the example, I won't show the complete sources, but only the interesting parts: how to load the model, setup the lights, and move the head around a bit.  Loading the model  The first thing we need to do is load the model. Three.js already provides an importer for obj files, so we'll just use that one. Let's first look at the code:   var obj; var head; var frontP;  function loadModel() {      var textLoader = new THREE.TextureLoader();       var diffuseTexture = textLoader.load(\"../resources/R2D2_Diffuse-Reflection-Combined-small.png\");      var bumpTexture = textLoader.load(\"../resources/R2D2_Normal-small.png\");      var emissiveTexture = textLoader.load(\"../resources/R2D2_Illumination-small.png\");      var specularTexture = textLoader.load(\"../resources/R2D2_Specular-small.png\");       var loader = new THREE.OBJLoader();      loader.load(\"../resources/R2D2_Standing.obj\", function(model) {          model.children.forEach(function(child) {              var material = child.material;               // basic texture              material.map = diffuseTexture;               // bumps              material.bumpMap = bumpTexture;              material.bumpScale = 0.3;               // glow              material.emissive = new THREE.Color(0xffffff);              material.emissiveMap = emissiveTexture;               // specular              material.specularMap = specularTexture;               // enable shadows              child.receiveShadow = true;              child.castShadow = true;                if (child.name === \"Head\") head = child;              if (child.name === \"Front_Projector\") frontP = child;          });           model.scale.x = 0.15;          model.scale.y = 0.15;          model.scale.z = 0.15;           model.position.y = -10;          obj = model;          scene.add(obj);      })  }    If you've already done some Three.js development, this doesn't look that special. We create a number of textures, and after loading the obj model we assign the textures to all the children (the R2D2 model is divided into separate parts). Note that we also assign the head and the Front_Projector childs to separate objects. We do this so we can animate them later on. The interesting part from this code is in creating the textures. When you use the THREE.OBJLoader you get THREE.MeshPongMaterial materials. This material can be configured for a number of textures, which we use like this:     map: This is the default texture map. We assign the diffuse map to this material.   bumpMap: This allows us to define the bumps of our model. In our textures we have the Normal map (which, as we've seen is really a bump map), and we'll assign that one.   emissiveMap: Three.js also provides an emissive map, so we just assign the provided one here.   specularMap: And the final map we need to assign is the provided specular map. Three.js provides one for that as well.  If you've kept track, you might have notices that we didn't mention the provided reflection map (which isn't a real reflection map, as we've seen). Three.js provides an envMap which we could use for reflections, but that serves a bit of a different purpose (cubeMap env example). So what we did was, we combined the provided Diffuse and Reflection map together using Gimp.  The result looks like this:    Which was created using the following layer settings:    Setup lights and camera  Now that we've loaded the textures and the model, we can start with the lights. For this simple setup we add three lights. One to the left of the camera, one behind the camera and one to front (basic three light setup: https://en.wikipedia.org/wiki/Three-point_lighting):   // position and point the camera to the center of the scene camera.position.x = 0; camera.position.y = 20; camera.position.z = 30;  // add some lights var spotLight1 = new THREE.SpotLight(0xffffff, 0.3, 0, Math.PI/4); spotLight1.position.set(-20, 20, 5); scene.add(spotLight1);  var spotLight2 = new THREE.SpotLight(0xffffff, 0.6, 0, Math.PI/4); spotLight2.position.set(-5, 30, 30); spotLight2.castShadow = true; // tightly wrap shadow around object spotLight2.shadow = new THREE.LightShadow( new THREE.PerspectiveCamera( 30, 1, 10, 50 ) );  scene.add(spotLight2);  var spotLight3 = new THREE.SpotLight(0xffffff, 0.3, 0, Math.Pi/4); spotLight3.position.set(20, 20, -10); scene.add(spotLight3);    As you can see we allow the second spotlight to cast a shadow, which we wrap tightly around the object, to get nice looking shadows.  Rotate the head a bit`  And the last part is a little bit of animation. We just rotate the two parts that make up the head around by updating the values in our render loop:   var rotMax = 0.5 * Math.PI; var rotMin = -0.5 * Math.PI; var direction = 1;  function render() {     // render using requestAnimationFrame     requestAnimationFrame(render);     renderer.render(scene, camera);      if (head) {         var rotationY = (0.01 * direction);         head.rotation.y += rotationY;         frontP.rotation.y += rotationY;          if (head.rotation.y &gt; rotMax) direction = -1;         if (head.rotation.y &lt; rotMin) direction = 1;      } }    And that's it.  Conclusions  As you've seen, it isn't that hard to get the models we initially downloaded to render in Three.js. I'm used to first exploring the models and textures using Blender to see what I've got, and look whether the textures are useful, the UV mapping is setup correctly and whether we might need to simply the model.  So basically what we did was:     Analyse the model and materials in Blender    Shrink and combine the textures using ImageMagick and Gimp    Use the simplified textures in Three.js  I'm going to process the other models as well, and post them as demo linked from here.    ","categories": ["posts","three.js","webgl","blender"],
        "tags": [],
        "url": "http://www.smartjava.org/content/render-3d-star-wars-force-awakens-models-blender-and-threejs/",
        "teaser":null},{
        "title": "Getting started with scala-native",
        "excerpt":"This is just a quick recollection of the steps I've taken to get my own helloworld running using scala-native. I haven't really started looking at scala-native in detail, but the idea behind it is really nice. Some general input about scala-native can be found here:     The Github repository: https://github.com/scala-native/scala-native   Presentation from Scaladays NY: https://github.com/densh/talks/blob/517b20c30dd4aaf390785039cdd002f623eaa91e/2016-05-11-scala-goes-native.pdf   Twitter to follow: @scala_native  Since scala-native relies on the avalability of LLVM and Clang++ this setup might be different in your environment, than it was in mine. I'm still on OS-X Yosemite:   $ sw_vers ProductName:\tMac OS X ProductVersion:\t10.10.3 BuildVersion:\t14D136    But for Mac users the steps will be pretty much the same. Now lets get started setting up the environment so you can run the sample project provided by scala-native, and we'll create a basic helloworld project as well. Once again, these steps worked in my environment, and yours might be different. There are a couple of github issues you can follow or check for more information on how to set up your environment.  The steps that worked for me were the following:  Checking out the code  We of course first need to get the code, from the command line do the following:   $ git clone --recursive https://github.com/scala-native/scala-native Cloning into 'scala-native'... remote: Counting objects: 8295, done. remote: Compressing objects: 100% (14/14), done. remote: Total 8295 (delta 3), reused 0 (delta 0), pack-reused 8278 Receiving objects: 100% (8295/8295), 1.31 MiB | 94.00 KiB/s, done. Resolving deltas: 100% (2955/2955), done.    This should, in theory, get you all the required sources. I, however, ran into the fist issue, that the submodele scala wasn't pulled correctly. Somehow git wants to use public-key authentication, instead of using https. The easiest way to quickly solve this, is just do this:   $ cd scala-native/submodules $ git clone https://github.com/scala/scala Cloning into 'scala'... remote: Counting objects: 333614, done. remote: Total 333614 (delta 0), reused 0 (delta 0), pack-reused 333613 Receiving objects: 100% (333614/333614), 83.31 MiB | 5.63 MiB/s, done. Resolving deltas: 100% (226320/226320), done. Checking connectivity... done. Checking out files: 100% (10492/10492), done.  $ cd scala $ git checkout 2.11.x    Publish the libraries locally  At this point you'll have all the required sources, but when you try to build it you can possibly run into the following issues:   # from the scala-native directory $ sbt &gt; publish-local ... lots of output [trace] Stack trace suppressed: run last scalalib/compile:compileIncremental for the full output. [trace] Stack trace suppressed: run last javalib/compile:doc for the full output. [error] (scalalib/compile:compileIncremental) java.lang.NoClassDefFoundError: scala/scalanative/nscplugin/NirGlobalAddons$nirPrimitives$ [error] (javalib/compile:doc) java.nio.charset.MalformedInputException: Input length = 1    Somehow there seems to be something wrong with some scaladoc stuff which causes our project to fail. Open the build.sbt file and change the javalib project. To solve this we can just disable the scaladoc stuff like by adding the following two lines:   sources in (Compile,doc) := Seq.empty, publishArtifact in packageDoc := false    To here:   83 lazy val javalib = 84   project.in(file(\"javalib\")). 85     settings(libSettings, 86       sources in (Compile,doc) := Seq.empty, 87       publishArtifact in packageDoc := false 88     ). 89     dependsOn(nativelib)    Now do a local publish again, and that should at least succeed:   &gt; reload &gt; publish-local ... again, lots of output [info] \tpublished ivy to /Users/jos/.ivy2/local/org.scala-native/scalalib_2.11/0.1-SNAPSHOT/ivys/ivy.xml [success] Total time: 17 s, completed May 14, 2016 4:06:31 PM    At this point, we can try to run the demo application, but you'll probably run into the following two errors: (at least I did)   # This one: /Users/jos/dev/git/scala-native-clean/scala-native/demo-native/target/scala-2.11/demonative-out.ll:152:40: error: expected '{' in function body declare double @\"llvm.sqrt.f64\"(double)                                        ^  # And this one rt.cpp:3:10: fatal error: 'gc.h' file not found #include &lt;gc.h&gt;    Ah... to bad. Something else went wrong. The first error is caused by having an incorrect version of LLVM (3.7 is required, the version I have is 3.6), and the second one is because the Boehm GC libraries aren't available.  Setup LLVM and Clang++ correctly  I don't know what the version is on newer OS-X installations, but mine still had 3.6:   $ clang++ -v Apple LLVM version 6.1.0 (clang-602.0.49) (based on LLVM 3.6.0svn) Target: x86_64-apple-darwin14.3.0 Thread model: posix    To fix this, without messing with possibly other tools depending on 3.6, I used brew to install the 3.7 version of LLVM and Clang++. You might need to do **brew tap homebrew/versions** first, if you can't find llvm37   $ brew install llvm37 --with-clang ==&gt; Installing llvm37 from homebrew/versions ==&gt; Downloading https://homebrew.bintray.com/bottles-versions/llvm37-3.7.1.yosemite.bottle.1.tar.gz Already downloaded: /Library/Caches/Homebrew/llvm37-3.7.1.yosemite.bottle.1.tar.gz ==&gt; Pouring llvm37-3.7.1.yosemite.bottle.1.tar.gz ==&gt; Caveats Extra tools are installed in /usr/local/opt/llvm37/share/clang-3.7  To link to libc++, something like the following is required:   CXX=\"clang++-3.7 -stdlib=libc++\"   CXXFLAGS=\"$CXXFLAGS -nostdinc++ -I/usr/local/opt/llvm37/lib/llvm-3.7/include/c++/v1\"   LDFLAGS=\"$LDFLAGS -L/usr/local/opt/llvm37/lib/llvm-3.7/lib\"    You can see the directory where Clang and LLVM are installed. Before we start SBT again, we need to make a symlink so that the plugin can find the correct clang executable.   $ cd /usr/local/opt/llvm37/bin $ ln -s clang++-3.7 clang++    And finally we just need to update our PATH variable to point to the installation directory before starting SBT:   $ export PATH=/usr/local/opt/llvm37/bin:$PATH $ clang++ -v clang version 3.7.1 (tags/RELEASE_371/final) Target: x86_64-apple-darwin14.3.0 Thread model: posix    At this point we should have one of our problems, so lets just check what happens if we try to run the demo:   # go back to the directory where you cloned scala-native $ cd ~/dev/git/scala-native-clean/scala-native $ sbt &gt; demoNative/run [info] Updating {file:/Users/jos/dev/git/scala-native-clean/scala-native/}demoNative... [info] Resolving org.scala-lang#scalap;2.11.8 ... [info] Done updating. [info] Compiling 1 Scala source to /Users/jos/dev/git/scala-native-clean/scala-native/demo-native/target/scala-2.11/classes... warning: overriding the module target triple with x86_64-apple-macosx10.10.0 [-Woverride-module] 1 warning generated. /Users/jos/.scalanative/rtlib-0.1-SNAPSHOT/rt.cpp:3:10: fatal error: 'gc.h' file not found #include &lt;gc.h&gt;         ^ 1 error generated.    As you can see, we've got one error less, but there is still something wrong  Install Boehm GC Library  What is missing, and what the error is pointing to, is the Boehm GC library. There are a couple of different ways we can install this. We can use a prepackaged brew instance, or we can just download the sources directly and install Boehm in that way. In this example we'll do the latter. Which means following the instructions from here: http://hboehm.info/gc/   $ mkdir boehm $ cd boehm $ git clone https://github.com/ivmai/libatomic_ops.git $ git clone https://github.com/ivmai/bdwgc.git $ cd bdgwc $ ln -s ../libatomic_ops $ autoreconf -vif $ automake --add-missing $ ./configure $ make $ sudo make install    Alternatively, you can also install the bdw-gc package in brew.   $ brew install bdw-gc    And after that add the following build.sbt to the demo-native project with additional clang settings:   $ cat build.sbt nativeClangOptions := Seq(\"-I/usr/local/Cellar/bdw-gc/7.4.2/include\", \"-L/usr/local/Cellar/bdw-gc/7.4.2/lib\")    I, however, prefer the installing from source* option. At this point we should actually be able to run the demo.   &gt; demoNative/run warning: overriding the module target triple with x86_64-apple-macosx10.10.0 [-Woverride-module] 1 warning generated. Rendering (8 spp) 100.00%[success] Total time: 13 s, completed May 14, 2016 6:30:42 PM    Woohoo... success!!! To see that we're actually running the code, we'll make a small change to the source in the demoNative directory. In the file smallpt.scala change the number of samples to 8 like this:   153   final val W = 800 154   final val H = 600 155   final val SAMPLES = 8 156   def main(args: Array[String]): Unit = {    Now run the program again (which should take a lot more time):   &gt; demoNative/run [info] Compiling 1 Scala source to /Users/jos/dev/git/scala-native-clean/scala-native/demo-native/target/scala-2.11/classes... warning: overriding the module target triple with x86_64-apple-macosx10.10.0 [-Woverride-module] 1 warning generated. Rendering (32 spp) 100.00%[success] Total time: 52 s, completed May 14, 2016 6:34:11 PM    It seems to be working. By the way, the result of this program is a 3D rendered scene (the sample program is a raytracer). If you set the samples to a high number, you actually get quite a nice render:   &lt;a href=”/sites/www.smartjava.org/files/scala-native.png” width=700&gt;&lt;/a&gt;  We can also check if the output is a real native program:   otool -L demonative-out demonative-out: \t/usr/local/lib/libgc.1.dylib (compatibility version 2.0.0, current version 2.3.0) \t/usr/lib/libc++.1.dylib (compatibility version 1.0.0, current version 120.0.0) \t/usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1213.0.0)    Now lets create an empty helloworld project, where we create our own minimal native-scala program.  Helloworld scala-native  In the scala-native project create a new directory with the name helloworld-native. In that directory create the following file (**helloworld.scala**):   package helloworld  import scalanative.native._, stdlib._  object Main {    def main(args: Array[String]): Unit = {     fprintf(__stdoutp, c\"Hello World: Scala Native!!!\")   } }    Before we can run it, we also need to update the main build.sbt file and define this project. To do this, add the following:   lazy val helloworldNative =   project.in(file(\"helloworld-native\")).     settings(libSettings).     settings(       nativeVerbose := true,       nativeClangOptions := Seq(\"-O2\")     ).     dependsOn(scalalib)     Don't forget the Seq(\"-I/usr/local/Cellar/bdw-gc/7.4.2/include\", \"-L/usr/local/Cellar/bdw-gc/7.4.2/lib\") for your helloworld-native build definition if you don't install Boehm GC from scratch.   Now reload sbt, and you should be able to run our own scala-native application:   &gt; helloworldNative/run [info] Compiling 1 Scala source to /Users/jos/dev/git/scala-native-clean/scala-native/helloworld-native/target/scala-2.11/classes... warning: overriding the module target triple with x86_64-apple-macosx10.10.0 [-Woverride-module] 1 warning generated. Hello World: Scala Native!!!    Or we can just run it from the command line directly:   $ pwd /Users/jos/dev/git/scala-native-clean/scala-native/helloworld-native/target/scala-2.11 $ ./helloworldnative-out Hello World: Scala Native!!!    I hope you can make a start as well with this short introduction. If time permits I've planned to dive a little bit deeper into scala-native, and explore some of its features.  ","categories": ["posts","scala-native","scala"],
        "tags": [],
        "url": "http://www.smartjava.org/content/getting-started-scala-native/",
        "teaser":null},{
        "title": "Scalaz features for everyday usage part 3: State Monad, Writer Monad and lenses",
        "excerpt":"In this article in the mini series on Scalaz, we'll look at a couple of additional monads and patterns available in Scalaz. Once again, we'll look at stuff that is practical to use, and avoid the inner details or Scalaz. To be more precise, in this article we'll look at:     Writer monad: Keep track of a sort of logging during a set of operations   State monad: Have an easy way of tracking state across a set of computations   Lenses: Easily access deeply nested attributes and make copying case classes more convenient  The following articles are currently available in this series:     Scalaz features for everyday usage part 1: Typeclasses and Scala extensions    Scalaz features for everyday usage part 2: Monad Transformers and the Reader Monad   Scalaz features for everyday usage part 3: State Monad, Writer Monad and lenses    We'll start with one of the additional monads provided by Scalaz.  Writer monad  Basically each writer has a log and a return value. This way you can just write your clean code, and at a later point determine what you want to do with the logging (e.g validate it in a test, output it to the console, or to some log file). So we could use a writer, for instance, to keep track of the operations we've executed to get to some specific value.  So lets look at the code and see how this thing works:   import scalaz._ import Scalaz._  object WriterSample extends App {    // the left side can be any monoid. E.g something which support   // concatenation and has an empty function: e.g. String, List, Set etc.   type Result[T] = Writer[List[String], T]    def doSomeAction() : Result[Int] = {     // do the calculation to get a specific result     val res = 10     // create a writer by using set     res.set(List(s\"Doing some action and returning res\"))   }    def doingAnotherAction(b: Int) : Result[Int] = {     // do the calculation to get a specific result     val res = b * 2     // create a writer by using set     res.set(List(s\"Doing another action and multiplying $b with 2\"))   }    def andTheFinalAction(b: Int) : Result[String] = {     val res = s\"bb:$b:bb\"      // create a writer by using set     res.set(List(s\"Final action is setting $b to a string\"))   }    // returns a tuple (List, Int)   println(doSomeAction().run)    val combined = for {     a &lt;- doSomeAction()     b &lt;- doingAnotherAction(a)     c &lt;- andTheFinalAction(b)   } yield c    // Returns a tuple: (List, String)   println(combined.run) }    In this sample we've got three operations that do something. In this case, they don't really do that much, but that doesn't matter. The main thing is, that instead of returning a value we return a Writer (note that we could have also created the writer in the for comprehension), by using the set function. When we call run on a Writer, we not just get the result of the operation, but also the aggregated values collected by the Writer. So when we do:   type Result[T] = Writer[List[String], T]  def doSomeAction() : Result[Int] = {   // do the calculation to get a specific result   val res = 10   // create a writer by using set   res.set(List(s\"Doing some action and returning res\")) }  println(doSomeAction().run)    The result looks like this: (List(Doing some action and returning res),10). Not that exciting, but it comes more interesting when we start using the writers in a for-comprehension.   val combined = for {   a &lt;- doSomeAction()   b &lt;- doingAnotherAction(a)   c &lt;- andTheFinalAction(b) } yield c  // Returns a tuple: (List, String) println(combined.run)    When you look at the output from this you'll see something like:   (List(Doing some action and returning res,      Doing another action and multiplying 10 with 2,      Final action is setting 20 to a string)  ,bb:20:bb)    As you can see we've gathered up all the different log messages in a List[String] and the resulting tuple also contains the final calculated value.  When you don't want to add the Writer instantiation in your functions you can also just create the writers in a for-comprehension like so:     val combined2 = for {     a &lt;- doSomeAction1()     set(\" Executing Action 1 \")   // A String is a monoid too     b &lt;- doSomeAction2(a)    set(\" Executing Action 2 \")     c &lt;- doSomeAction2(b)    set(\" Executing Action 3 \") //  c &lt;- WriterT.writer(\"bla\", doSomeAction2(b))   // alternative construction   } yield c    println(combined2.run)    The result of this sample is this:   ( Executing Action 1  Executing Action 2  Executing Action 3 ,5)    Cool right? For this sample we've only shown the basic Writer stuff, where the type is just a simple type. You can of course also create Writer instances from more complex types. An examples of this can be found here: http://stackoverflow.com/questions/35362240/creating-a-writertf-w-a-from-a-writerw-a  The state monad  Another interesting monad, is the State monad. The state monad provides a convient way to handle state that needs to be passed through a set of functions. You might need to keep track of results, need to pass some context around a set of functions, or require some (im)mutable context for another reason. With the (Reader monad) we already saw how you could inject some context into a function. That context, however, wasn't changeable. With the state monad, we're provided with a nice pattern we can use to pass a mutable context around in a safe and pure manner.  Lets look at some examples:     case class LeftOver(size: Int)    /** A state transition, representing a function `S =&gt; (S, A)`. */   type Result[A] = State[LeftOver, A]    def getFromState(a: Int): Result[Int] = {     // do all kinds of computations     State[LeftOver, Int] {       // just return the amount of stuff we got from the state       // and return the new state       case x =&gt; (LeftOver(x.size - a), a)     }   }    def addToState(a: Int): Result[Int] = {     // do all kinds of computations     State[LeftOver, Int] {       // just return the amount of stuff we added to the state       // and return the new state       case x =&gt; (LeftOver(x.size + a), a)     }   }    val res: Result[Int] = for {     _ &lt;-  addToState(20)     _ &lt;- getFromState(5)     _ &lt;- getFromState(5)     a &lt;- getFromState(5)     currentState &lt;- get[LeftOver]                // get the state at this moment     manualState &lt;- put[LeftOver](LeftOver(9000)) // set the state to some new value     b &lt;- getFromState(10) // and continue with the new state   } yield {     println(s\"currenState: $currentState\")     a   }    // we start with state 10, and after processing we're left with 5   // without having to pass state around using implicits or something else   println(res(LeftOver(10)))    As you can see, in each function we get the current context, make some changes to it, and return a tuple consisting of the new state, and the value of the function. This way each function has access to the State, can return a new one, and returns this new state, together with the function's value as a Tuple. When we run the above code we see the following:   currenState: LeftOver(15) (LeftOver(8990),5)    As you can see each of the functions does something with the state. With the get[S] function we can get the value of the state at the current moment, and in this example we print that out. Besides using the get function, we can also set the state directly using the put function.  As you can see, a very nice, and simple to use pattern, but great when you need to pass some state around a set of functions.  Lenses  So enough with the monads for now, lets look at Lenses. With Lenses it is possible to easily (well more easy than just copying case classes by hand) change values in nested object hierarchies. Lenses can do a whole lot of things, but in this article I'll introduce just some basic features. First, the code:   import scalaz._ import Scalaz._  object LensesSample extends App {    // crappy case model, lack of creativity   case class Account(userName: String, person: Person)   case class Person(firstName: String, lastName: String, address: List[Address], gender: Gender)   case class Gender(gender: String)   case class Address(street: String, number: Int, postalCode: PostalCode)   case class PostalCode(numberPart: Int, textPart: String)    val acc1 = Account(\"user123\", Person(\"Jos\", \"Dirksen\",                 List(Address(\"Street\", 1, PostalCode(12,\"ABC\")),                      Address(\"Another\", 2, PostalCode(21,\"CDE\"))),                 Gender(\"male\")))     val acc2 = Account(\"user345\", Person(\"Brigitte\", \"Rampelt\",                 List(Address(\"Blaat\", 31, PostalCode(67,\"DEF\")),                      Address(\"Foo\", 12, PostalCode(45,\"GHI\"))),                 Gender(\"female\")))     // when you now want to change something, say change the gender (just because we can) we need to start copying stuff   val acc1Copy = acc1.copy(     person = acc1.person.copy(       gender = Gender(\"something\")     )   )    In this sample we defined a couple of case classes, and want to change a single value. For case classes this means, that we have to start nesting a set of copy operations to correctly change one of the nested values. While this can be done for simple hierarchies, it quickly becomes cumbersome. With lensen you're offered a mechanism to do this in a composable way:   val genderLens = Lens.lensu[Account, Gender](    (account, gender) =&gt; account.copy(person = account.person.copy(gender = gender)),    (account) =&gt; account.person.gender  )   // and with a lens we can now directly get the gender  val updated = genderLens.set(acc1, Gender(\"Blaat\"))  println(updated)  #Output: Account(user123,Person(Jos,Dirksen,List(Address(Street,1,PostalCode(12,ABC)),          Address(Another,2,PostalCode(21,CDE))),Gender(Blaat)))    So we define a Lens, which can change a specific value in the hierarchy. With this lens we can now directly get or set a value in a nested hierarchy. We can also create a lens which modifies a value and returns the modified object in one go by using the =&gt;= operator.    // we can use our base lens to create a modify lens  val toBlaBlaLens = genderLens =&gt;= (_ =&gt; Gender(\"blabla\"))  println(toBlaBlaLens(acc1))  # Output:  Account(user123,Person(Jos,Dirksen,List(Address(Street,1,PostalCode(12,ABC)),            Address(Another,2,PostalCode(21,CDE))),Gender(blabla)))   val existingGender = genderLens.get(acc1)  println(existingGender)  # Output: Gender(male)    And we can use the &gt;=&gt; and the &lt;=&lt; operators to combine lenses together. For example in the following code sample, we create to separate lenses which are then combined and executed:    // First create a lens that returns a person  val personLens = Lens.lensu[Account, Person](    (account, person) =&gt; account.copy(person = person),    (account) =&gt; account.person  )   // get the person lastname  val lastNameLens = Lens.lensu[Person, String](    (person, lastName) =&gt; person.copy(lastName = lastName),    (person) =&gt; person.lastName  )    // Get the person, then get the lastname, and then set the lastname to  // new lastname  val combined = (personLens &gt;=&gt; lastNameLens) =&gt;= (_ =&gt; \"New LastName\")   println(combined(acc1))   # Output: Account(user123,Person(Jos,New LastName,List(Address(Street,1,PostalCode(12,ABC)),            Address(Another,2,PostalCode(21,CDE))),Gender(male)))    Conclusions  There are still two subjects I want to write about, and that are Validations and Free monads. In the next article in this series I'll show how you can use ValidationNEL for validations. Free Monads however, I think, doesn't really fall in the category of everyday usage, so I'll spent a couple of other articles on that in the future.  ","categories": ["posts","scala","scalaz"],
        "tags": [],
        "url": "http://www.smartjava.org/content/scalaz-features-everyday-usage-part-3-state-monad-writer-monad-and-lenses/",
        "teaser":null},{
        "title": "First steps with Angular 2: 8 cool features",
        "excerpt":"I’ve been doing some work the last couple of weeks with Angular2. I really like it. Not just because it uses typescript, but also because it feels really natural and straightforward while working with it. No more string based dependency injection, or strange digest cycle stuff, it just seems to work. This last week I’ve migrated our beta-13 Angular app to the latest rc-1, and used that to keep track of the fun and easy stuff Angular 2 provides. Note though, that the application we’re developing is really that complex, so I can only assume we’ll run into more complex Angular2 features in the near future. For now, though, let me share some general tips and tricks we’ve encountered thus far (in no particular order). Oh, all examples are in typescript, since after using that, I really don’t want to go back to plain old javascript (POJS?).   1. Pipes for formatting   If you need to format some output string, in the old Angular version you used Filters. In Angular 2, though, the name of filters have changed and they are now called pipes. Using pipes is very easy and only takes a couple of steps to get working. First, you need to define a pipe:    import {Pipe} from '@angular/core'; import {FormatUtils} from '../utils/formatUtils';  @Pipe ({   name : \"humanReadableNumber\" }) export class HumanReadableNumber {    transform(value: any) {     // converts a size to something human readable     return FormatUtils.humanSize(+value);   } }    Now that we’ve got a pipe we can import it in our component, and use it to format a value in our page. We import is like this, and add it to the pipes property of the    import {Component} from '@angular/core'; import {CORE_DIRECTIVES} from '@angular/common'; import {HumanReadableNumber} from '../../../shared/pipes/humanReadablePipe'  @Component({ \ttemplateUrl: 'some-template.html',   directives: [CORE_DIRECTIVES],   pipes: [HumanReadableNumber] }) export class SomeCmp {   ... }    And using it in our code is just as simple:    &lt;div class=\"col-sm-3\"&gt;   &lt;span class=\"text-red\"&gt;&lt;/span&gt; &lt;/div&gt;    And the cool thing is you can chain pipes together, and of course also create parameterized pipes. The official documentation for pipes can be found here: https://angular.io/docs/ts/latest/guide/pipes.html   2. HTTP is easy to use    You can’t have a web application without making some HTTP calls to pull in data. With Angular 2 this is suprisingly easy. Just inject the http component, pass in the headers and parameters, and you get back on observable. As soon as you subscribe to the observable, the call will be executed, and you can easily use the functions provides by rx.js to transform the response (see next tip):    private getSomeData(timePeriod: TimePeriod): Observable&lt;any&gt; {    let headers = new Headers();   headers.append('Content-Type', 'application/json');    // handle the today case differently, since we need to go to start of day   let [from, to] = (timePeriod.name === Configuration.timePeriod.today.name)     ? [moment().utc().hours(0), moment()]     : [moment().subtract(timePeriod.amount, timePeriod.period), moment()];    // setup and add the query parameters   let params = new URLSearchParams();   params.set('environment', 'test');   params.set('interval', timePeriod.period);   params.set('from', from.toISOString());   params.set('to', to.toISOString());    return this.http.get('http://the.url', { headers: this.getHeaders(), search: params })     .map((res: any) =&gt; res.json()); }    And the result is an observable, which can be consumed like this:    someService.getSomeData(period).subscribe((json: any) =&gt; ({// do something with the json}));    The official documentation for HTTP can be found here: https://angular.io/docs/ts/latest/guide/server-communication.html   3. Observables are really cool   In Angular 1 when communicating over HTTP, or doing stuff asynchronously you would use promises. In Angular 2, promises have been replaced with Observables. Without going too deep into what Observables are, in short they provide a way to subscribe to a sequence of events and act whenever the observable emits a new event. The really cool thing is that you can apply all kinds of operations and transformations on the events before they are processed by a subscriber. You can for instance map it to a domain object, group multiple events together and much, much more (see the rx.js documentation for all the options: http://reactivex.io/rxjs/class/es6/Observable.js~Observable.html)   The easiest example of an Observable is when you want to do an HTTP request in Angular 2. Instead of working with promises you now do something like this:    import {Http} from '@angular/http'; import {Inject} from '@angular/core'; import {Headers} from '@angular/http';  export class SomeService {    constructor(@Inject(Http) private http: Http) {};    getPerson() : Observable&lt;Person&gt; {     let headers = new Headers();     headers.append('Content-Type', 'application/json');      return this.http.get('http://the.url.endpoint', {headers})       .map(res =&gt; res.json())       .map(res =&gt; new Person(res.name, res.age));   } }    As you can see the result is an Observable&lt;/strong&gt;. We can now use this Observable very simply like this:    import {Component} from '@angular/core'; import {Person} from '../shared/domain'; import {SomeService} from 'some-service';  @Component({   templateUrl: 'someTemplate.html' }) export class AnotherCmp {    public person: Person;    constructor(private someService: SomeService) {     someService.getPerson().subscribe((person) =&gt; (this.person = person));   } }    We subscribe to the observable. When we do this, the HTTP call will be made, and when it returns, we just assign the response to a public variable. Another feature of Observables we use is to wait for multiple calls to finish, before sending the message to a subscriber. This uses the forkJoin function provided by rxjs.    public getMonthlyInfo(): Observable&lt;MonthData[]&gt;  {   let observables: Observable&lt;MonthData&gt;[] = [];    // create the observables for the different calls   // and add them to the observables array   ...    // Intellij throws a false error: https://youtrack.jetbrains.com/issue/WEB-20992   // this returns another observable, which returns an array of results from the   // observables we passed in to the forkJoin   return Observable.forkJoin(observables); }    And this is just touching upon the surface of what is possible with Observables. On a side note, if you prefer to work with Promises, you can use the toPromise function on an observable to turn it into a promise.   4. Start with use angular2 seed    Maybe I should have added this as the first tip, but setting up a build environment using gulp systemJS (or webpack) can be a bit of an hassle. Luckily, though, there is an easy way to get started using angular seed. There are multiple angular seeds available, but I use this one: https://github.com/mgechev/angular2-seed. It provides a systemJS / gulp based seed setup, including all kinds of nice features. Getting started with it, is also very trivial:    git clone --depth 1 https://github.com/mgechev/angular2-seed.git cd angular2-seed # install the project's dependencies npm install # watches your files and uses livereload by default npm start # api document for the app npm run build.docs  # dev build npm run build.dev # prod build npm run build.prod    If you want to use it as the base for your own project, it might be good to run the following git commands after cloning the repository, to point it to your own repository / project:    rm -rf .git git init git remote add origin https://giturl.of.your.project.git     5. Intercept requests for authentication   One of the first things I wanted to add to our application was a way to check whether a user was logged in. I was looking for global filters, and stumbled upon this example: https://github.com/auth0-blog/angular2-authentication-sample. This shows how to use a RouterOutlet to check for certain conditions, before handling the request. Basically what you do is, you define a custom RouterOutlet:   From (https://github.com/auth0-blog/angular2-authentication-sample/blob/master/src/app/LoggedInOutlet.ts):    import {Directive, Attribute, ViewContainerRef, DynamicComponentLoader} from '@angular/core'; import {Router, RouterOutlet, ComponentInstruction} from '@angular/router-deprecated'; import {Login} from '../login/login';  @Directive({   selector: 'router-outlet' }) export class LoggedInRouterOutlet extends RouterOutlet {   publicRoutes: any;   private parentRouter: Router;    constructor(_viewContainerRef: ViewContainerRef, _loader: DynamicComponentLoader,               _parentRouter: Router, @Attribute('name') nameAttr: string) {     super(_viewContainerRef, _loader, _parentRouter, nameAttr);      this.parentRouter = _parentRouter;     // The Boolean following each route below     // denotes whether the route requires authentication to view     this.publicRoutes = {       'login': true,       'signup': true     };   }    activate(instruction: ComponentInstruction) {     let url = instruction.urlPath;     if (!this.publicRoutes[url] &amp;&amp; !localStorage.getItem('jwt')) {       // todo: redirect to Login, may be there a better way?       this.parentRouter.navigateByUrl('/login');     }     return super.activate(instruction);   } }    And this allows you to check certain conditions before normally processing the request, or passing it on. I can explain how it works here, but just look at the github repository which will do a much better job.   6. Directives are very easy to write   In Angular2 the distinction between controllers and directives is removed. A directive is just a simple component, where some of it’s values are populated when the component is instantiated. For instance a simple component that wraps one of the great https://github.com/valor-software/ng2-charts charts looks like this:    import {Component, Input} from '@angular/core'; import {CORE_DIRECTIVES} from '@angular/common'; import {CHART_DIRECTIVES} from 'ng2-charts/ng2-charts'; import {DecisionNodeSummary} from '../../../shared/domain/dagSummary';  @Component({   selector: 'dag-decnode-graph',   templateUrl: 'decisionnode-graph.html',   directives: [CHART_DIRECTIVES, CORE_DIRECTIVES] }) export class DagDecNodeGraph {    public title = \"\";    @Input()   set nodeSummary(nodeSummary: NodeSummary) {     this.routingStatusChartData = [nodeSummary.falsePath, nodeSummary.truePath];     this.title = nodeSummary.name;   };    public routingStatusChartLabels: string[] = ['false', 'true'];   public routingStatusChartType: string = 'doughnut';   public routingStatusChartLegend: boolean = false;   public routingStatusChartData: number[] = [0, 0];   public routingStatusChartOptions: any = {};    constructor() {   }  }    And the relevant template:    &lt;div class=\"col-md-4\"&gt;   &lt;h4 style=\"text-align: center\"&gt;&lt;/h4&gt;   &lt;base-chart class=\"chart\"               [data]=\"routingStatusChartData\"               [labels]=\"routingStatusChartLabels\"               [legend]=\"routingStatusChartLegend\"               [chartType]=\"routingStatusChartType\"&gt;&lt;/base-chart&gt; &lt;/div&gt;    Nevermind the model which is passed in, but this is basically all you need to do to define a directive. To use this directive in a page you just do something like this:    &lt;div *ngFor='let node of nodes'&gt;&lt;dag-decnode-graph [nodeSummary]=\"node\"&gt;&lt;/dag-decnode-graph&gt;&lt;/div&gt; &lt;/div&gt;    How easy is that! No more specific javascript, or custom functions. Just a simple component, just like all the other components.    7. Typescript: domain objects are cheap and easy, type everything.   I couldn’t write this, without at least mentioning typescrypt. I really like typescript I come from a Java and Scala background and am used to having compile-time typechecking (and using IDEs to autocomplete some stuff). With typescript this also becomes available to Angular, and it works really great. What makes it even better is that creating basic domain objects is really easy and quick:    export class Person {   constructor(public name: number, public age: number) {} }    And you can easily access the public properties, and hide the private properties. It even support default values. It isn’t quite up to Scala case classes yet, but it is a great way of defined a good domain model in your frontend applications. This combined with the map function of the Observable, it is very easy and convient to use these domain objects throughout your application.   8. Use jquery    In many cases there isn’t a big need for jQuery in Angular projects. However, there is big ecosystem of great jQuery based components and libraries that will come in handy. While using jquery in Angular 2 isn’t that difficult, getting the typescript compiler to compile your code when including jquery in your dependencies might be harder. When you include jQuery you’ll quickly run into an error message something like this:    Subsequent variable declarations must have the same type.   Variable '$' must be of type 'cssSelectorHelper', but here has type 'JQueryStatic'. Web\\Scripts\\typings\\jquery\\jquery.d.ts   3936    Basically, there are two different libraries claiming the $ name. This can be quickly solved by replacing the last couple of lines from the jquery/index.d.ts to this:    interface JQuery {     // add additional functions you might want to expose from other libraries     // that append the jquery object     chosen(options?:any):JQuery;     bootstrapTable(...options:any[]):JQuery; }  declare module \"jquery\" {     export = JQuery; } declare var jQuery: JQueryStatic;    Now you can use the jquery library like this in code:    declare var jQuery:JQueryStatic;  ... // do whatever you want, just use jQuery instead of $ jQuery('.elem')    While this works, you’ll notice that you have to repeat this process after each npm install or whenever you add a new library. Luckily you can override the typings that are retrieved when doing an npm install. So to permanently apply these settings, save the typings to a separate directory and change the jquery entry in the typings.json file to this:    \"jquery\": \"file:overrides/typings/jquery/index.d.ts\",    This points to our own custom jquery typings file in a separate directory. finally to get everything to work also update the tsconfig.json file to exclude this file or else you’ll get duplicate definitions errors:    \"exclude\": [   \"node_modules\",   \"dist\",   \"typings/browser.d.ts\",   \"typings/browser\",   \"src\",   \"overrides\" ]     Conclusions   This is just a quick set of observations from a couple of days intensive Angular 2 development. My main conclusion, though, it that it really is fun doing frontend development with Angular 2. It might because of typescript, but so far it feels like a really great, well thought out framework. It just works and a lot of the annoying things from Angular 1 have been solved.  ","categories": ["angular2","posts","angular","typescript"],
        "tags": [],
        "url": "http://www.smartjava.org/content/first-steps-angular-2-8-cool-features/",
        "teaser":null},{
        "title": "Analyzing swearing in movies Part 1: Swears per minute",
        "excerpt":"In this short article I’d like to explain the process I took to create some simple D3.js visualizations based on the number of swears per minute in movies. I’m going to do some additional analysis on this, but for now I’d just like to so you some samples, and code fragments that explains the process I’ve used. This article is inspired by some data from http://fivethirtyeight.com that showed the swearing in tarantino movies: http://fivethirtyeight.com/features/complete-catalog-curses-deaths-quentin-tarantino-films/   For the fivethirtyeight article, someone actually counted the individual swearwords, which, although I like Tarantino movies, isn’t really something I have the time for to do. In this article I show you how you can automate this.   First off, though, lets look at some results. Note that I haven’t spent much time on the visuals, so it are just rather basic charts at the moment.      Swears per minute as a bar graph: http://www.smartjava.org/examples/swears/timeLine.html   Swears per minute as a heatmap: http://www.smartjava.org/examples/swears/timeLine.html   For those not wanting to see the real D3.js SVGs, the following images show pretty much the same.   Bars:    Heatmap:    So how do we do this. Well, the first thing to do is get the number of swearwords per minute. I mentioned that for the original article someone just counted every swearwords, in our case, we’re just going to parse a subtitle file, and extract the swear words from that.   Scala analysis   Without going into too much detail, you can find the code I’ve experimtend with in this gist (it’s very ugly code, since I just hacked something together that worked).       The code isn't that interesting. A couple of pointers though:  &lt;ol&gt;   &lt;li&gt;I've used the srt parser from here: https://github.com/implicitdef/srt-parser, this allows me to simply parse a srt file, and access the content.&lt;/li&gt;   &lt;li&gt;In this code we check for swearwords against a file containing all the swears. This file is based on a slightly modified version from here: https://gist.github.com/ryanlewis/a37739d710ccdb4b406d&lt;/li&gt;   &lt;li&gt;For now I just output the swears per minute, but this can also be used to easily count which swearwords are the most popular, etc.&lt;/li&gt; &lt;/ol&gt;  The output from this program is a csv list in the following format:     time,swears 0,0 1,2 2,3 3,0 4,2 5,2 6,6 7,13 8,1 9,3 10,6 11,3 12,3 13,1 14,3 15,9 16,0 17,1 18,1   ```   Where time is the minute we’re in, and swears are the number of swear words said in that minute. So now that we’ve got the swear words, we can use D3.js to visualize them.   Bar graph   For the bar graph the following code is used:     Heat map   And for the heatmap we use the following:     If you’re familiar with D3.js you can see that it are just the basic graps extended for this purpose.   And that’s it for this very simple first part of visualzing dirty words ;) More in the coming weeks, since I’ve got a couple of other visualizations in mind.  ","categories": ["visualization","d3.js","posts"],
        "tags": [],
        "url": "http://www.smartjava.org/content/analyzing-swearing-movies-part-1-swears-minute/",
        "teaser":null},{
        "title": "Dynamic component loading with Angular2: replace $compile",
        "excerpt":"Note that this has become outdated… again. But it has become part of the standard Angular distribution. Please look at the latest angular documentation here: Angular docs   Just a quick article to show how you can do dynamic compilation in Angular 2. In Angular 1 we had the $compile directive, which we could use to programmatically compile a string and resolve any directives and other variable interpolations. In angular 2 there isn’t really an alternative for this and we have to jump through some hoops to get the same behavior. Luckily though a couple of smart guys at stackoverflow found out how to do this with the newer Angular 2 releases http://stackoverflow.com/questions/34784778/equivalent-of-compile-in-angular-2. This article provides a summary of how to do this yourself, since I had a bit of trouble getting it to work just from the answer on stackoverflow. The solution shown here simplifies some steps from the stackoverflow answer.   I’ve tested this setup using Angular2 RC-4, and it should work with future versions as well. To get everything working we need to take the following steps:      Create a component dynamically where we pass in our HTML Template, and any required directives.   In the component where we want to use this, define a provider and inject the builder.   And add a reference in the template where we want to inject the content.   Now we create our component dynamically and inject it.   Before we look at the individual steps, lets look at the final result. Below you can find an embedded plunkr, showing dynamic components in action:     Now lets look at the steps   Create a component dynamically   To dynamically add content and have angular compile it, we need to create a simple component dynamically which will contain our template and references any directives or pipes we’d like to use. Doing this is really suprisingly easy:   import { Component } from '@angular/core';  export class DynamicBuilder {    public createComponent(template: String, directives: any[], pipes: any[]) : any {      @Component({       template: template,       directives: directives,       pipes: pipes     })     class CustomComponent {        // The name of the root property we'll bind to. Can be       // anything, just needs to be something we can expose.       public props: any;     };      return CustomComponent;   } }    As you can see we just return an annotated class, which is configured through the createComponent function. So whenever we call this function, we get a component back, configured with the provided template, directives and pipes. Now that we’ve got a way to create dynamic components, lets inject this builder into our parent component.   In the component where we want to use this, define a provider and inject the builder   Before we fill in all the elements of our parent component, lets first make sure our parent component can access the builder we defined in the previous step. To do this, we just add it as a provider in this component, and inject it using the constructor.   import { Component, ComponentResolver } from '@angular/core'; import { DynamicBuilder } from 'app/dynamic.component'  @Component({   selector: 'my-app',   templateUrl: 'app/app.component.html',   providers: [DynamicBuilder] }) export class AppComponent {    ...   constructor(private builder: DynamicBuilder, private componentResolver: ComponentResolver) {};   ... }    Note that besides the dynamicBuilder, we also inject a ComponentResolver. We’ll use that in the last step together with the dynamicbuilder to inject content. Before we do that, though, lets look at how we can determine where we want to add content.   Add a reference in the template where we want to inject the content.   Our complete template for this example looks like this:   &lt;h1&gt;Dynamic Component Loading&lt;/h1&gt;  &lt;h2&gt;Dynamic data:&lt;/h2&gt; &lt;div #dynamicChild&gt;&lt;/div&gt;    Very basic, the only thing we add here is that we add the #dynamicChild value to the div element. We’ll use this attribute to lookup the div and inject our dynamic component inside that div.   This lookup is done like this in our parent component:   import { Component, ViewChild, ViewContainerRef, ComponentFactory, ComponentResolver } from '@angular/core'; import { DynamicBuilder } from 'app/dynamic.component' import { timer } from 'rxjs/observable/timer';  @Component({   selector: 'my-app',   templateUrl: 'app/app.component.html',   providers: [DynamicBuilder] }) export class AppComponent {    // we need the viewcontainer ref, so explicitly define that, or we'll get back   // an element ref.   @ViewChild('dynamicChild', {read: ViewContainerRef})   private target: ViewContainerRef;    constructor(private builder: DynamicBuilder, private componentResolver: ComponentResolver) {}; }    As you can see in this code, we use the @ViewChild annotation to get a reference to the div element we defined in our template. Note that we need to explicitly tell this annotation that we want a ViewContainerRef by using the read property.   Now we create our component dynamically and inject it.   We now have all the parts in place to create a new dynamic component and attach it to div from our parent element’s template. The complete code for our parent component is shown below:   import { Component, ViewChild, ViewContainerRef, ComponentFactory, ComponentResolver } from '@angular/core'; import { DynamicBuilder } from 'app/dynamic.component' import { timer } from 'rxjs/observable/timer';  @Component({   selector: 'my-app',   templateUrl: 'app/app.component.html',   providers: [DynamicBuilder] }) export class AppComponent {    // we need the viewcontainer ref, so explicitly define that, or we'll get back   // an element ref.   @ViewChild('dynamicChild', {read: ViewContainerRef})   private target: ViewContainerRef;    constructor(private builder: DynamicBuilder, private componentResolver: ComponentResolver) {};   private timer = timer(2000,1000);    public ngOnInit() {     let template = `       &lt;span&gt;Dynamic property hello: &lt;span&gt;&lt;br&gt;       &lt;span&gt;Dynamic property world: &lt;span&gt;&lt;br&gt;       &lt;span&gt;Dynamic async property: &lt;span&gt;&lt;br&gt;     `;      // the properties we want to bind     this.props = {       hello: 'hello',       world: 'world',       obs: this.timer     };      //create the component, and set the properties     let component = this.builder.createComponent(template, [], []);     this.componentResolver.resolveComponent(component)       .then( (factory: ComponentFactory) =&gt; {         // create a component and assign it to the first slot         let dynComponent = this.target.createComponent(factory,0);         let instance: any = dynComponent.instance;         instance.props = this.props;       });   } }    The interesting code here is in the ngOnInit function. In that function we first define the template we want to render, and the properties we want to bind (note that this can be pretty much anything). Next we create the dynamic component we want to add using the createComponent function we defined earlier, passing in the template and any directives or pipes that might be needed (none in this sample). The final thing to do is use the componentResolver to process our custom component, and when that is done, connect it to the target we retrieved through the @ViewChild annotation. Finally we set the properties and we’re done.   Conclusions   This might seem like a lot of steps, but this can be easily hidden besides a simple helper function or class. The main thing that needs to be done is getting a reference to the location where you want to add your custom component, and create a definition for your custom component. Like I mentioned in the beginning of this article all credit goes to Radim Köhler who provided an answer on stackoverflow about this subject here: http://stackoverflow.com/questions/34784778/equivalent-of-compile-in-angular-2  ","categories": ["angular","angular2","typescript","posts"],
        "tags": [],
        "url": "http://www.smartjava.org/content/dynamic-component-loading-angular2-replace-compile/",
        "teaser":null},{
        "title": "Voronoi Fractal with D3.js",
        "excerpt":"I’ve been very busy the last couple of months. Started a new project at Philips, started writing on D3.js for Packt, and besides that participating in two startups. So time to update this blog has been limited. While thinking about how to best present the voronoi feature of D3.js, I ran across an example where they used the result of a single voronoi partitioning as an input for another. So you’d get a nested set of voronois. This actually looked really nice, so I set out to do that myself, so I could use that example in the book as well.   Results  The results look really nice (at least I think so):      I’ve uploaded the code to blocks, so you can experiment with it yourself here: blocks   More results  A couple of samples with different color schemes are shown below:        ","categories": ["voronoi d3.js","posts"],
        "tags": [],
        "url": "http://www.smartjava.org/content/voronoi-fractal-d3js/",
        "teaser":null},{
        "title": "Three.js: Devoxx presentation 2018",
        "excerpt":"Hi everyone,   It’s been a whole two years since I posted stuff here, but as an early new year resolution, I decided to revive this blog, and share with you all the stuff I’ve been doing the last couple of years. As a start I’d like to share the presentation I gave on Three.js a couple of weeks ago at devoxx.   The presentation on youtube     Hope you like this, and expect more coming soon.  ","categories": ["posts","devoxx","threejs"],
        "tags": [],
        "url": "http://www.smartjava.org/content/threejs-devoxx-presentation-2018/",
        "teaser":null},{
        "title": "10 Practical Kubernetes Tips for Day to Day Kubernetes usage",
        "excerpt":"A couple of years ago I wrote a number of articles on how to work with docker. Since that time a lot has changed in the docker and container world. Docker has matured, and provides a large number of interesting features, accompanying tools and lots and lots of other fun stuff. For my last project, though, we moved our docker swarm setup to a kubernetes setup, since that provided us with more control on how to deploy our components.   So in this article, I’ll show some of the stuff I often use in my daily dealing with kubernetes. Note that this is just a very small glimpse into what is possible through the kubernetes CLI.   1. Extending kubectl   A really nice feature of kubectl is that it is really easy to create new commands that wrap the standard kubectl functionality. One of the things I used to do often with kubernetes is list pods, to see if something is started, whether everything is running etc.   ~ kubectl get pods NAME    READY   STATUS    RESTARTS   AGE pod-1   1/1     Running   0          20s pod-2   1/1     Running   0          8s   While this works nice, it quickly becomes annoying to type this. We can of course just alias this on bash level, but kubectl also provides an alternative way to do this, by creating custom commands. For instance we can create an kubectl ls command which does the same:   ~ kubectl ls NAME    READY   STATUS    RESTARTS   AGE pod-1   1/1     Running   0          2m pod-2   1/1     Running   0          1m   What happens here is that when kubectl encounters a command it doesn’t know, it’ll look for a specific command in the PATH. In this case it’ll try to find the kubectl-ls command. This is in my path and looks like this:   #!/bin/bash  kubectl get pods   Something else which I find myself often doing, especially with a large number of running pods, is grepping the output of the kubectl get pods command. This can be easily written as a custom command like this:   #!/bin/bash    kubectl get pods | grep -i ${1}   And you use it like this:   ~ kubectl ls NAME        READY   STATUS              RESTARTS   AGE another-1   0/1     ContainerCreating   0          4s pod-1       1/1     Running             0          8m  ~ kubectl grep another another-1   1/1     Running   0          9s   I’ve noticed that whenever I get annoyed at having to type a large set of somewhat similar commands I just write a simple wrapper, since it is really easy.   2. Kubernetes exec   This is something I guess everybody already knows, but I’ll add it here anyways. When developing new services, or running pods, sometimes we just want to look inside the container to see what is really happening. When doing docker, we’d just run docker exec -ti &lt;container&gt; bash and started working in the container. For kubernetes this is exactly the same!   ~ kubectl ls NAME        READY   STATUS    RESTARTS   AGE another-1   1/1     Running   0          6m pod-1       1/1     Running   0          14m ~ kubectl exec -ti another-1 bash root@another-1:/#   Now you can do anything from the context of that container. Really easy to check network issues, status of running services.   3. Kubernetes get and jq   Just like we’ve got docker inspect we’ve got kubectl describe which provides us information on running resources. While normal describe return information in a readable format, you can also just use kubectl get &lt;&gt; -o json to get information in json format, which makes processing and querying in your scripts easier.   ~ kubectl get pod another-1 -o json {     \"apiVersion\": \"v1\",     \"kind\": \"Pod\",     \"metadata\": {         \"creationTimestamp\": \"2018-12-17T11:03:55Z\",         \"labels\": {             \"run\": \"another-1\"         },         \"name\": \"another-1\",         \"namespace\": \"default\"         \"...\" : \"...\"     } }           And when we’ve got the json, parsing this and using the output in your own scripts becomes really easy by just using jq:   ~ kubectl get pod another-1 -o json | jq '.status.hostIP' -r 192.168.65.3 ~ kubectl get pod another-1 -o json | jq '.status.podIP' -r 10.1.0.57   4. Copying data from running containers   Often you can do all you need to do, just by running kubectl exec .... Sometimes, though, it would be really nice if you’ve got the actual data, to play around with. For instance to reproduce some annoying issue, or get some relevant data to work with during development. Luckily kubectl also provides a cp command, just like docker does.   ~ kubectl cp --help Copy files and directories to and from containers.  Examples:   # !!!Important Note!!!   # Requires that the 'tar' binary is present in your container   # image.  If 'tar' is not present, 'kubectl cp' will fail.    # Copy /tmp/foo_dir local directory to /tmp/bar_dir in a remote pod in the default namespace   kubectl cp /tmp/foo_dir &lt;some-pod&gt;:/tmp/bar_dir    # Copy /tmp/foo local file to /tmp/bar in a remote pod in a specific container   kubectl cp /tmp/foo &lt;some-pod&gt;:/tmp/bar -c &lt;specific-container&gt;    # Copy /tmp/foo local file to /tmp/bar in a remote pod in namespace &lt;some-namespace&gt;   kubectl cp /tmp/foo &lt;some-namespace&gt;/&lt;some-pod&gt;:/tmp/bar    # Copy /tmp/foo from a remote pod to /tmp/bar locally   kubectl cp &lt;some-namespace&gt;/&lt;some-pod&gt;:/tmp/foo /tmp/bar  Options:   -c, --container='': Container name. If omitted, the first container in the pod will be chosen  Usage:   kubectl cp &lt;file-spec-src&gt; &lt;file-spec-dest&gt; [options]   An important note is that the tar binary needs to be present in the container for this to work, but since most often this is the case, this cp command will work in most cases. Using it is really simple. For instance if we want to copy the /etc/hosts file from a container to our local machine:   ~ kubectl cp another-1:/etc/hosts .  ~ cat hosts # Kubernetes-managed hosts file. 127.0.0.1\tlocalhost ::1\tlocalhost ip6-localhost ip6-loopback fe00::0\tip6-localnet fe00::0\tip6-mcastprefix fe00::1\tip6-allnodes fe00::2\tip6-allrouters 10.1.0.57\tanother-1   5. Run and attach directly for one of jobs   Sometimes you just want to run one of commands, instead of deploying and running a pod using create or apply. For this kubernetes provides you with the run command, where you can very easily fire up a container:   ~ kubectl run one-off --rm --restart=Never -it --image=ubuntu -- cat /etc/lsb-release DISTRIB_ID=Ubuntu DISTRIB_RELEASE=18.04 DISTRIB_CODENAME=bionic DISTRIB_DESCRIPTION=\"Ubuntu 18.04.1 LTS\" pod \"one-off\" deleted   Or if you want a shell to do some stuff, and which is cleaned up when you’re done:   ~ kubectl run my-bash --rm --restart=Never -it --image=ubuntu -- bash If you don't see a command prompt, try pressing enter. root@my-bash:/# ls bin   dev  home  lib64  mnt  proc  run   srv  tmp  var boot  etc  lib   media  opt  root  sbin  sys  usr root@my-bash:/# exit exit pod \"my-bash\" deleted   Really useful if you quickly want to check some settings from the context of a pod or namespace. This is also a great way to test around whether a service account has the correct rights, as you’ll see further down.   6. Kubernetes UI is great   Not much to add here. I really like the standard UI from kubernetes. Just launch a local proxy:   ~ kubectl proxy Starting to serve on 127.0.0.1:8001   And you’ve got a great UI like this:      You can make it even nice by adding   ~ kubectl apply -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/rbac/heapster-rbac.yaml ~ kubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml ~ kubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/grafana.yaml ~ kubectl create -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/heapster.yaml   You might need to restart the dashboard after these changes, and you should have something running like this in the kube-system namespace:   ~ kubectl get pods --namespace kube-system NAME                                         READY   STATUS    RESTARTS   AGE etcd-docker-for-desktop                      1/1     Running   0          126d heapster-7ff8d6bf9f-kgpt5                    1/1     Running   0          2m kube-apiserver-docker-for-desktop            1/1     Running   0          126d kube-controller-manager-docker-for-desktop   1/1     Running   0          126d kube-dns-86f4d74b45-kqdhn                    3/3     Running   0          126d kube-proxy-l559k                             1/1     Running   0          126d kube-scheduler-docker-for-desktop            1/1     Running   0          126d kubernetes-dashboard-7d5dcdb6d9-ph2gq        1/1     Running   1          126d monitoring-grafana-68b57d754-8p8nc           1/1     Running   0          2m monitoring-influxdb-cc95575b9-q2v9s          1/1     Running   0          2m   Now when you open up the UI, you don’t just see the default information, but also see a lot of details and metrics on how your system is running:      I noticed myself that I’m using this dashboard quite often. With docker I did pretty much everything using the CLI, but for quick overviews of what is happening, this dashboard (especially with the metrics), really just works.   The final part, not really related to the dashboard but interesting anyhow, is that this also sets up grafana. We can now connect directly to grafana and setup our own metrics.   ~ kubectl --namespace kube-system get service monitoring-grafana NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE monitoring-grafana   ClusterIP   10.98.182.140   &lt;none&gt;        80/TCP    8m  ~ kubectl --namespace kube-system port-forward svc/monitoring-grafana 1080:80 Forwarding from 127.0.0.1:1080 -&gt; 3000   Now open up your browser at localhost:1080 and you can start your own dashboards. You need to take into account though that by default grafana and influxdb don’t store their data on a persistent volume, so if you want to keep metrics for longer time, and avoid having to recreate your graphs, you might need to change the deployments for these components.      7. Container not starting up? overwrite the entry point and just run it from bash   Another useful approach is to override the entrypoint when your container doesn’t start as expected. Say we’ve got a simple pod like this:   apiVersion: v1 kind: Pod metadata:   name: wget spec:   restartPolicy: Never   containers:   - name: wget     image: mwendler/wget   This pod isn’t normally something you’d run as a pod, but more as a one time action using kubectl run. This pod wraps the wget command, and since we didn’t specify any arguments it’ll fail when we create it:   ~ kubectl create -f simple.yml pod/wget created  ~ kubectl ls NAME        READY   STATUS    RESTARTS   AGE another-1   1/1     Running   0          3h pod-1       1/1     Running   0          3h wget        0/1     Error     0          8s   If this is an image that contains some initialization scripting, or has a more complex startup procedure, it’d be handy to be able to just start a shell, and see the environment variables, run the script from there, maybe finetune it on the container to see what works instead of rebuilding images. For this we can just specify a command in the yaml.   apiVersion: v1 kind: Pod metadata:   name: wget spec:   restartPolicy: Never   containers:   - name: wget     image: mwendler/wget     command: [sh, -c, \"sleep 1000\"]   And run it, and use kubectl exec to connect to it.   ~ kubectl create -f simple.yml pod/wget created ~  tmp kubectl ls NAME        READY   STATUS    RESTARTS   AGE another-1   1/1     Running   0          3h pod-1       1/1     Running   0          3h wget        1/1     Running   0          10s ~ kubectl exec -ti wget sh / # which wget /usr/bin/wget / # uname -a Linux wget 4.9.93-linuxkit-aufs #1 SMP Wed Jun 6 16:55:56 UTC 2018 x86_64 Linux / #   And this will work with most containers, and is a very easy way to see why a specific entrypoint.sh might be failing, or to test other parts of that container.   8. Running an embedded kubernetes from docker   There are lots and lots of resources out there that explain how to setup a kubernetes cluster. For quick testing I often use the embedded one from Docker. Docker provides this for a while now:      Kubernetes is available in Docker for Mac 17.12 CE Edge and higher, and 18.06 Stable and higher , this includes a standalone Kubernetes server and client, as well as Docker CLI integration. The Kubernetes server runs locally within your Docker instance, is not configurable, and is a single-node cluster.    To setup your own local kubernetes cluster all you have to do is enable kubernetes in the kubernetes tab of the docker preferences:      And you’ve got yourself a locally running kubernetes setup. You might need to install kubectl, but docker will configure your local .kube/config file so you work with the cluster directly.   9. Using apply instead of create   Moving to the end of the 10 tips, which is basically an interesting one, if you want to do more controlled rollouts. If you’ve deployed a deployment consisting out of three instances, you’d normally don’t want to update them all at the same time. You just want to apply the new settings, one at a time. The most basic way to do this, is to just use apply when you create a set of resources from a file, instead of using create. If you do a create on the same resource, kubernetes will complain that the resources probably already exist, and it won’t recreate them. When using apply however, kubernetes will check for any changes, and determine which resources to reload.   The cool thing about this is, that it’ll do this pod by pod. If we’ve got this simple description:   apiVersion: apps/v1 kind: Deployment metadata:   name: simple-deployment   labels:     app: simple spec:   replicas: 3   selector:     matchLabels:       app: simple   template:     metadata:       labels:         app: simple     spec:       containers:       - name: wget         image: mwendler/wget         command: [sh, -c, \"sleep 1000\"]   And deploy it, we’ve got three pods:   ~ kubectl apply -f ./simple.yml deployment.apps/simple-deployment unchanged  ~ kubectl ls NAME                                READY   STATUS    RESTARTS   AGE simple-deployment-759f65d94-62sjf   1/1     Running   0          26s simple-deployment-759f65d94-cmxtm   1/1     Running   0          26s simple-deployment-759f65d94-cr2cw   1/1     Running   0          26s   Now when we change a single property in the deployment, and do apply again, you’ll see that it updates one by one:   ~ kubectl apply -f ./simple.yml ~ kubectl ls NAME                                 READY   STATUS              RESTARTS   AGE simple-deployment-759f65d94-62sjf    1/1     Terminating         0          2m simple-deployment-759f65d94-cmxtm    1/1     Running             0          2m simple-deployment-759f65d94-cr2cw    1/1     Terminating         0          2m simple-deployment-7f6f9d4dcf-2mhws   0/1     ContainerCreating   0          4s simple-deployment-7f6f9d4dcf-m9kvr   1/1     Running             0          8s simple-deployment-7f6f9d4dcf-w8jkf   1/1     Running             0          13s   10. Working with service accounts   As a final note, some info on service accounts. Kubernetes provides a great API to monitor, control the complete cluster. For certain applications and services it’s very useful to access that API. However, you don’t want to give a pod complete access to the whole API, but only to the part where it is allowed. This is where RBAC and service accounts come into play. There are enough sites that explain how to set this up, so I only want to focus on the practical part of it, on how to use this for your own.   For this we’ll use a simple ubuntu container and add some extra resources:   --- apiVersion: v1 kind: ServiceAccount metadata:   name: simple-view   namespace: default  --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata:   name: simple-view rules: - apiGroups:   - \"\"   resources:   - endpoints   - pods   - namespaces   verbs:   - get   - list   - watch  --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata:   name: simple-bind roleRef:   kind: ClusterRole   name: simple-view   apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount   name: simple-view   namespace: default  --- apiVersion: v1 kind: Pod metadata:   name: wget spec:   serviceAccount: simple-view   containers:   - name: wget     image: ubuntu     command: [sh, -c, \"sleep 1000\"]   This fires up an ubuntu container. The reason is that some alpine versions seem to have an issue with DNS resolving, so we’ll just use an standard ubuntu container, and install wget on that.   ~ kubectl create -f ./simple.yml serviceaccount/simple-view created clusterrole.rbac.authorization.k8s.io/simple-view created clusterrolebinding.rbac.authorization.k8s.io/simple-bind created pod/wget created   ~ kubectl exec -ti wget bash # apt-get update ... # apt-get install wget ... # cd /var/run/secrets/kubernetes.io/serviceaccount # ls -l total 0 lrwxrwxrwx    1 root     root            13 Dec 17 15:21 ca.crt -&gt; ..data/ca.crt lrwxrwxrwx    1 root     root            16 Dec 17 15:21 namespace -&gt; ..data/namespace lrwxrwxrwx    1 root     root            12 Dec 17 15:21 token -&gt; ..data/token   Here we’ve got a token file. This file contains the bearer token which we can include in our requests to the API. If we don’t add this and try to query the API from within the container we see something like this:   # wget --no-check-certificate -S -O - https://kubernetes/api/v1/namespaces --2018-12-17 15:41:17--  https://kubernetes/api/v1/namespaces Resolving kubernetes (kubernetes)... 10.96.0.1 Connecting to kubernetes (kubernetes)|10.96.0.1|:443... connected. WARNING: cannot verify kubernetes's certificate, issued by 'CN=kubernetes':   Unable to locally verify the issuer's authority. HTTP request sent, awaiting response...   HTTP/1.1 403 Forbidden   Content-Type: application/json   X-Content-Type-Options: nosniff   Date: Mon, 17 Dec 2018 15:41:17 GMT   Content-Length: 289 2018-12-17 15:41:17 ERROR 403: Forbidden.   We don’t have the correct rights to access this specific endpoint. Now when we use the token:  # TOKEN=$(cat token) # wget --header=\"Authorization: Bearer $TOKEN\" --no-check-certificate -S -O - https://kubernetes/api/v1/namespaces | head HTTP request sent, awaiting response...   HTTP/1.1 200 OK   Content-Type: application/json   Date: Mon, 17 Dec 2018 15:42:30 GMT   Transfer-Encoding: chunked Length: unspecified [application/json] Saving to: 'STDOUT'  -                              [ &lt;=&gt;                                  ]   2.64K  --.-KB/s    in 0s  2018-12-17 15:42:30 (22.3 MB/s) - written to stdout [2699]  {   \"kind\": \"NamespaceList\",   \"apiVersion\": \"v1\",   \"metadata\": {     \"selfLink\": \"/api/v1/namespaces\",     \"resourceVersion\": \"547678\"   },   \"items\": [     {       \"metadata\": {   You can see that we’re allowed to access the endpoint and get back the results as expected. Easy, simple and secure!   Conclusions   I want to leave it at these couple of small tips, and just like to thank everyone that helped create these great set of tools!  ","categories": ["posts","docker","kubernetes","top"],
        "tags": [],
        "url": "http://www.smartjava.org/content/practical-kubernetes-tips-for-day-to-day-kubernetes-usage/",
        "teaser":null},{
        "title": "D3.js: Devoxx presentation 2017",
        "excerpt":"I just noticed that I never published the D3.js presentation I gave last year on Devoxx. So a bit late, but you can find the video for that presentation below:   The presentation on youtube     ","categories": ["posts","devoxx","d3"],
        "tags": [],
        "url": "http://www.smartjava.org/content/d3-js-devoxx-presentation-2017/",
        "teaser":null},{
        "title": "All (150+) learning three.js third edition examples online",
        "excerpt":"A couple of months ago the third edtion of my book on learning three.js was released: View on Packt. Just as for the previous examples I’ve put all the examples online so you can easily see what’s in the book, or just explore the samples, without the book.   The sources can also be found on github here: https://github.com/josdirksen/learning-threejs-third   Chapter 1   Example 01.01 - Basic skeleton  Example 01.02 - First Scene  Example 01.03 - Materials and light  Example 01.04 - Materials, light and animation  Example 01.05 - Control gui  Example 01.06 - Screen size change   Chapter 2   Example 02.01 - Basic Scene  Example 02.02 - Foggy Scene  Example 02.03 - Override Material  Example 02.04 - Geometries  Example 02.05 - Custom geometry  Example 02.06 - Mesh Properties  Example 02.07 - Cameras  Example 02.08 - Cameras   Chapter 3   Example 03.01 - Ambient Light  Example 03.03 - Spot Light  Example 03.02 - point Light  Example 03.04 - Directional Light  Example 03.05 - Directional Light  Example 03.06 - Area Light  Example 03.07 - Lensflarest   Chapter 4   Example 04.01 - MeshBasicMaterial  Example 04.02 - Depth Material  Example 04.03 - Combined Material  Example 04.04 - Mesh normal material  Example 04.05 - Mesh face material  Example 04.06 - Mesh Lambert material  Example 04.07 - Mesh Phong material  Example 04.08 - Mesh Toon material  Example 04.09 - Mesh Standard material  Example 04.10 - Mesh Physical material  Example 04.11 - Shader material - http://glsl.heroku.com/  Example 04.12 - Linematerial  Example 04.13 - Linematerial Dashed   Chapter 5   Example 05.01 - Basic 2D geometries - Plane  Example 05.02 - Basic 2D geometries - Circle  Example 05.03 - Basic 3D geometries - Ring  Example 05.04 - Basic 2D geometries - Shape  Example 05.05 - Basic 2D geometries - Cube  Example 05.06 - Basic 3D geometries - Sphere  Example 05.07 - Basic 3D geometries - Cylinder  Example 05.08 - Basic 3D geometries - Cone  Example 05.09 - Basic 3D geometries - Torus  Example 05.10 - Basic 3D geometries - Torusknot  Example 05.11 - Basic 3D geometries - Polyhedron   Chapter 6   Example 06.01 - Advanced 3D geometries - Convex Hull  Example 06.02 - Advanced 3D geometries - Lathe  Example 06.03 - Extrude Geometry  Example 06.04 - Extrude TubeGeometry  Example 06.05 - Extrude SVG  Example 06.06 - Parametric geometries  Example 06.07 - Text geometry  Example 06.08 - Binary operations   Chapter 7   Example 07.01 - Sprites  Example 07.02 - Points  Example 07.03 - Basic Point Cloud  Example 07.04 - Particles - Canvas based texture  Example 07.05 - Points - Canvas based texture - WebGL  Example 07.04 - Particles - Canvas based texture  Example 07.07 - Particles - Rainy scene  Example 07.08 - Particles - Snowy scene  Example 07.09 - Sprites  Example 07.10 - Sprites in 3D  Example 07.11 - 3D Torusknot   Chapter 8   Example 08.01 - Grouping  Example 08.02 - Merge objects  Example 08.03 - Save and Load  Example 08.04 - Load and save scene  Example 08.05 - Load blender model   Example 08.06 - Load OBJ model   Example 08.07 - Load OBJ and MTL   Example 08.08 - Load collada model   Example 08.09 - Load stl model   Example 08.10 - Load ctm model   Example 08.11 - Load vtk model   Example 08.12 - Load pdb model   Example 08.13 - Load ply model   Example 08.14 - Load awd model   Example 08.15 - Load assimp model   Example 08.16 - Load vrml model   Example 08.17 - Load babylon model   Example 08.18 - Load TDS model   Example 08.19 - Load 3FM model   Example 08.20 - Load AMF model   Example 08.21 - Load Play Canvas model   Example 08.22 - Load Draco model   Example 08.23 - Load PRWM model   Example 08.24 - Load GCode model   Example 08.25 - Load nrrd model   Example 08.26 - Load svg model    Chapter 9   Example 09.01 - Simple animation  Example 09.02 - Selecting objects  Example 09.03 - Load ply model   Example 09.04 - Trackball controls   Example 09.05 - Fly controls   Example 09.06 - FPS controls   Example 09.07 - Orbit controls   Example 09.08 - Morph targets  Example 09.09 - Manual morph targets  Example 09.10 - Bones manually  Example 09.11 - Animation from Blender  Example 09.12 - Animation from Collada  Example 09.13 - Animation from MD2  Example 09.14 - gltf loader  Example 09.15 - Animation from FBX  Example 09.16 - Animation from x  Example 09.11 - Animation from BVH  Example 09.18 - Animation from SEA   Chapter 10   Example 10.01 - Basic Textures  Example 10.01 - Basic Textures  Example 10.02 - Basic Textures  Example 10.03 - Basic Textures PVR  Example 10.04 - Basic Textures TGA  Example 10.05 - KTX Textures  Example 10.06 - EXR Textures  Example 10.07 - HDR/RGBE Textures  Example 10.08 - Bump map  Example 10.09 - Normal map  Example 10.10 - Displacement map  Example 10.11 - Ambient occlusion map  Example 10.12 - Light map  Example 10.14 - Metalness and Roughness  Example 10.14 - Alpha map  Example 10.15 - Emissive  Example 10.16 - Specular  Example 10.17 - Envmap static  Example 10.17 - Envmap dynamic  Example 10.19 - UV Mapping  Example 10.20 - UV Mapping Manual  Example 10.21 - Repeat wrapping  Example 10.22 - Canvas Texture  Example 10.22 - Bump map from Canvas  Example 10.24 - Video texture   Chapter 11   Example 11.01 - Basic Effect Composer  Example 11.02 - Simple Pass Effects - 1  Example 11.03 - Simple Pass Effects - 2  Example 11.04 - Post processing masks  Example 11.05 - Bokeh  Example 11.06 - Ambient Occlusion  Example 11.07 - Shader pass simple  Example 11.08 - Blur passes  Example 11.09 - Basic Effect Composer  Example 11.03 - Post processing masks  Example 11.04 - Shader Pass simple  Example 11.04 - Shader Pass simple  Example 11.06 - Advanced  Example 11.07 - custom shaderpass   Chapter 12   Example 12.01 - Dominos  Example 12.02 - Material  Example 12.03 - Shapes  Example 12.04 - PointConstraint  Example 12.05 - Sliders and hinges  Example 12.06 - DOF Constraint  Example 12.07 - Audio  ","categories": ["posts","devoxx","three.js"],
        "tags": [],
        "url": "http://www.smartjava.org/content/all-learning-three-js-third-edition-examples-online/",
        "teaser":null},{
        "title": "Getting started with FP in Kotlin and Arrow: Typeclasses",
        "excerpt":"I’ve recently made the effort to try and pick up Kotlin (again). The last couple of years I’ve been doing   mostly scala work (with some other languages in between), and was wondering how the functional concepts of  scala programming transfer to Kotlin. Main reason is that I don’t see myself returning to Java any time soon  but, Kotlin seems to fix many of the verbose parts of Java. With the Arrow  Kotlin also gets some FP concepts, so it looked like the right time to really dive into Kotlin, and see how FP  in Kotlin holds up.   We’ll just start this series with looking at how we can do typeclasses using Arrow in kotlin. Before we start a quick  note. In these examples I’ve used Gson for JSON marshalling, a better approach would probably have been using a more  functional / immutable JSON library. So I might change that if I ever get the time for it.   Setting up the environment   Since this is the first article, we’ll start with setting up the environment. I’ve just followed the instructions  on Arrow, and ended up with the following gradle files:   build.gradle  plugins {     id 'org.jetbrains.kotlin.jvm' version '1.3.0' }  group 'org.smartjava' version '1.0-SNAPSHOT'  repositories {     mavenCentral()     jcenter()     maven { url 'https://jitpack.io' }     maven { url 'https://dl.bintray.com/spekframework/spek-dev' } }  dependencies {      testImplementation (\"org.spekframework.spek2:spek-dsl-jvm:$spek_version\")  {         exclude group: 'org.jetbrains.kotlin'     }     testImplementation 'org.amshove.kluent:kluent:1.45'     testRuntimeOnly (\"org.spekframework.spek2:spek-runner-junit5:$spek_version\") {         exclude group: 'org.junit.platform'         exclude group: 'org.jetbrains.kotlin'     }      compile \"org.jetbrains.kotlin:kotlin-stdlib-jdk8\"     compile \"ch.qos.logback:logback-classic:1.2.3\"     compile \"io.ktor:ktor-gson:$ktor_version\"      compile \"io.ktor:ktor-server-core:$ktor_version\"     compile \"io.ktor:ktor-server-netty:$ktor_version\"      compile \"io.arrow-kt:arrow-core:$arrow_version\"     compile \"io.arrow-kt:arrow-syntax:$arrow_version\"     compile \"io.arrow-kt:arrow-typeclasses:$arrow_version\"     compile \"io.arrow-kt:arrow-data:$arrow_version\"     compile \"io.arrow-kt:arrow-instances-core:$arrow_version\"     compile \"io.arrow-kt:arrow-instances-data:$arrow_version\"     kapt    \"io.arrow-kt:arrow-annotations-processor:$arrow_version\"      compile \"io.arrow-kt:arrow-free:$arrow_version\" //optional     compile \"io.arrow-kt:arrow-instances-free:$arrow_version\" //optional     compile \"io.arrow-kt:arrow-mtl:$arrow_version\" //optional     compile \"io.arrow-kt:arrow-effects:$arrow_version\" //optional     compile \"io.arrow-kt:arrow-effects-instances:$arrow_version\" //optional     compile \"io.arrow-kt:arrow-effects-rx2:$arrow_version\" //optional     compile \"io.arrow-kt:arrow-effects-rx2-instances:$arrow_version\" //optional     compile \"io.arrow-kt:arrow-effects-reactor:$arrow_version\" //optional     compile \"io.arrow-kt:arrow-effects-reactor-instances:$arrow_version\" //optional     compile \"io.arrow-kt:arrow-effects-kotlinx-coroutines:$arrow_version\" //optional     compile \"io.arrow-kt:arrow-effects-kotlinx-coroutines-instances:$arrow_version\" //optional     compile \"io.arrow-kt:arrow-optics:$arrow_version\" //optional     compile \"io.arrow-kt:arrow-generic:$arrow_version\" //optional     compile \"io.arrow-kt:arrow-recursion:$arrow_version\" //optional     compile \"io.arrow-kt:arrow-instances-recursion:$arrow_version\" //optional     compile \"io.arrow-kt:arrow-integration-retrofit-adapter:$arrow_version\" //optional     compile \"org.jetbrains.kotlin:kotlin-script-runtime:1.3.10\" }  compileKotlin {     kotlinOptions.jvmTarget = \"1.8\" } compileTestKotlin {     kotlinOptions.jvmTarget = \"1.8\" }  apply plugin: 'application' apply plugin: 'kotlin-kapt'  mainClassName = 'io.ktor.server.netty.EngineMain'   generated-kotlin-sources.gradle  apply plugin: 'idea'  idea {     module {         sourceDirs += files(                 'build/generated/source/kapt/main',                 'build/generated/source/kapt/debug',                 'build/generated/source/kapt/release',                 'build/generated/source/kaptKotlin/main',                 'build/generated/source/kaptKotlin/debug',                 'build/generated/source/kaptKotlin/release',                 'build/tmp/kapt/main/kotlinGenerated')         generatedSourceDirs += files(                 'build/generated/source/kapt/main',                 'build/generated/source/kapt/debug',                 'build/generated/source/kapt/release',                 'build/generated/source/kaptKotlin/main',                 'build/generated/source/kaptKotlin/debug',                 'build/generated/source/kaptKotlin/release',                 'build/tmp/kapt/main/kotlinGenerated')     } }   gradle.properties  kotlin.code.style=official ktor_version=1.0.1 arrow_version=0.8.1 spek_version=2.0.0-alpha.2   Note that if you want to use the annotation processing and you use Intellij Idea for development, you might   run into the issue that no sources are being generated for the Arrow annotations. The reason is that Intellij uses it’s own system to   build the sources, and not necessary your complete gradle file. This can easily be fixed though by changing   enabling the following setting:      At this point we should be able to use the annotations provided by Arrow and all the other provided FP functionality.   Creating typeclasses   I’m just going to assume you already know what typeclasses are. If not there are many resources out there that explain what they  do and why they are really useful:      Scala typeclass explained: Implement a String.read function   Wikipedia typeclass explanation   Cats: typeclasses   Haskell: typeclases   Typeclasses are also often used to marshal types from JSON to a domain object and back again. So in this example we’re   going to use that as the example. We’ll start with the interface definition of our marshallers:   typealias Result&lt;T&gt; = Either&lt;Exception, T&gt;  object Marshallers {      fun &lt;T : Any&gt;convertToJson(m: T, ev: JsonMarshaller&lt;T&gt;): JsonElement = ev.run { m.toJson() }      interface JsonMarshaller&lt;T : Any&gt; {         fun T.toJson(): JsonElement          companion object {}     } }  object Unmarshallers {      fun &lt;T : Any&gt;convertFromJson(j: String, ev: JsonMarshaller&lt;T&gt;): Result&lt;T&gt; = ev.run { fromJson(j) }      interface JsonMarshaller&lt;T : Any&gt; {         fun fromJson(j: String): Result&lt;T&gt;          companion object {}     }  }    Here you can see that we’ve defined an interface that can handle marshalling a T to a JsonElement and one that does  the opposite. Unmarshal an incoming String to a Either&lt;T&gt;. We’ve also added a helper method that takes an instance of a   typeclass and calls the relevant function. Before we look at implementations of these typeclasses let’s take a quick look  at a very simple model.   object Model {     data class Product(val productId: String, val description: String) {         companion object {}     }      data class OrderLine(val lineId: Int, val quantity: Int, val product: Product) {         companion object {}     }      data class Order(val orderId: String, val orderLines: List&lt;OrderLine&gt;) {         companion object {}     } }   Create typeclass instances for the marshallers   The first thing we’re going to do is define the marhsallers. The marshaller will take a T and convert it to a JSONElement. With Kotlin,  creating this is pretty simple. Let’s start with the most simple approach. A quick note on this code and the code later on in the article. This  is purely to demonstrate how typeclasses work in Kotlin. In real code we would do this differently (especially use a different JSON library), so  don’t look to much at the something convoluted code in the marshallers and the unmarshallers.       val orderLineMarshaller = object : JsonMarshaller&lt;Model.OrderLine&gt; {         override fun Model.OrderLine.toJson(): JsonElement {             val orderLine = gson.toJsonTree(this).asJsonObject             orderLine.add(\"product\", Marshallers.convertToJson(this.product, MarshallerInstances.productMarshaller))             return orderLine         }     }      val productMarshaller = object : JsonMarshaller&lt;Model.Product&gt; {         override fun Model.Product.toJson(): JsonElement = gson.toJsonTree(this)     }          val orderMarshaller = object : JsonMarshaller&lt;Model.Order&gt; {         override fun Model.Order.toJson(): JsonElement {             val orderLines = this.orderLines.map { Marshallers.convertToJson(it, orderLineMarshaller) }             val withoutOrderLines = this.copy(orderLines = listOf())             val json = gson.toJsonTree(withoutOrderLines)              val f = JsonArray()             val acc = orderLines.foldLeft(f) { res, el -&gt; res.add(el); res}             val result = json.asJsonObject             result.remove(\"orderLines\")             result.add(\"customorderlines\", acc)              return result         }     }   In this code fragment you can see that we create a numbr of instances of the JsonMarshaller interface. And use the Marshallers.converToJson   function to convert the nested objects to JSON. We could of course also have used the gson.toJsonTree(this) call everywhere, but that  wouldn’t have allowed us to easily customize the way the various objects are marshalled.   Now lets set up a simple test to see what is happening when we run this code:      describe(\"The product marshaller instance\") {          val product = Model.Product(\"product\", \"description\")          it(\"should marshall an product to a JSON value\") {             val result = MarshallerInstances.productMarshaller.run {                 product.toJson().toString()             }              result `should be equal to` \"\"\"{\"productId\":\"product\",\"description\":\"description\"}\"\"\"         }     }       describe(\"The orderline marshaller instance\") {          val orderLine = OrderLine(1, 2, Model.Product(\"\", \"\"))          it(\"should marshall an orderline to a JSON value\") {             val result = MarshallerInstances.orderLineMarshaller.run {                 orderLine.toJson().toString()             }              result `should be equal to` \"\"\"{\"lineId\":1,\"quantity\":2,\"product\":{\"productId\":\"\",\"description\":\"\"}}\"\"\"         }     }      describe(\"The order marshaller instance\") {          val orderLines = listOf(OrderLine(1,2, Model.Product(\"\", \"\")), OrderLine(2,3, Model.Product(\"\", \"\")))         val order = Order(\"orderId\", orderLines)          it(\"should marshall an order to a JSON value\") {             val result = MarshallerInstances.orderMarshaller.run {                 order.toJson().toString()             }              result `should be equal to` \"\"\"{\"orderId\":\"orderId\",\"customorderlines\":[{\"lineId\":1,\"quantity\":2,\"product\":{\"productId\":\"\",\"description\":\"\"}},{\"lineId\":2,\"quantity\":3,\"product\":{\"productId\":\"\",\"description\":\"\"}}]}\"\"\"         }     }    All these tests pass, since we’re just calling them directly. We can also use the function defined in the Marshallers object:   fun &lt;T : Any&gt;convertToJson(m: T, ev: JsonMarshaller&lt;T&gt;): JsonElement = ev.run { m.toJson() } ... val orderLine = OrderLine(1, 2, Model.Product(\"\", \"\")) Marshallers.convertToJson(orderline, MarshallerInstances.orderLineMarshaller)   The big advantage of calling it like this, is that the compiler checks for us that we’ve used the correct typeclass instance. If we try to call this with a  typeclass instance of the incorrect type, the compiler starts complaining:   val orderLines = listOf(OrderLine(1,2, Model.Product(\"\", \"\")), OrderLine(2,3, Model.Product(\"\", \"\"))) val order = Order(\"orderId\", orderLines) Marshallers.convertToJson(order, MarshallerInstances.orderLineMarshaller)   This will fail:   e: fskotlin/src/test/kotlin/org/smartjava/typeclasses/MarshallersSpecTest.kt: (51, 21): Type inference failed: Cannot infer type parameter T in fun &lt;T : Any&gt; convertToJson(m: T, ev: Marshallers.JsonMarshaller&lt;T&gt;): JsonElement None of the following substitutions (Model.OrderLine,Marshallers.JsonMarshaller&lt;Model.OrderLine&gt;) (Model.Order,Marshallers.JsonMarshaller&lt;Model.Order&gt;) (Any,Marshallers.JsonMarshaller&lt;Any&gt;) can be applied to (Model.Order,Marshallers.JsonMarshaller&lt;Model.OrderLine&gt;)   As you might have noticed, we have to pass in the specific instance of the typeclass that we want to use. I come from a   scala background and am used to passing in typeclasses as implicit parameters (or use a context bounded type parameter). With  that approach it is enough for the typeclass to be (implicitly) in scope, so we don’t have to explicitly pass it into the function.   For Kotlin there is a proposal (KEEP-87), which proposes something similar. There  is also a reference implementation available, that already allows you to play around with it. So in a future article I’ll do the same  as in this article, but then with that implementation.   Now let’s also quickly implement the unmarshallers to get back from JSON to Kotlin.   Create typeclass instances for the marshallers   For a very naive implementation of the unmarshallers we can create something like this:   object UnmarshallerInstances {      val gson = Gson()     val parser = JsonParser()          val productUnmarshaller = object : JsonUnmarshaller&lt;Model.Product&gt; {         override fun fromJson(j: String): Result&lt;Model.Product&gt; = try {             Either.right(gson.fromJson(j, Model.Product::class.java))         } catch (e: Throwable){             Either.left(e)         }     }          val orderLineUnmarshaller = object : JsonUnmarshaller&lt;Model.OrderLine&gt; {         override fun fromJson(j: String): Result&lt;Model.OrderLine&gt; = try {             // first use the productUnmarshaller to get a Result&lt;Product&gt;             val jsonObject = parser.parse(j).asJsonObject             val jsonProduct = jsonObject.get(\"product\")             val product = productUnmarshaller.fromJson(jsonProduct.toString())              // if product is right, convert it to a product, else we get the error.             product.map{ p -&gt; Model.OrderLine(jsonObject.get(\"lineId\").asInt, jsonObject.get(\"quantity\").asInt, p)}         } catch (e: Throwable){             Either.left(e)         }     }      val orderUnmarshaller = object : JsonUnmarshaller&lt;Model.Order&gt; {         override fun fromJson(j: String): Result&lt;Model.Order&gt; = try {              val jsonObject = parser.parse(j).asJsonObject             val jsonOrderLines = jsonObject.get(\"customorderlines\").asJsonArray              // convert using the correct unmarsahller             val orderLines = jsonOrderLines.map { ol -&gt; orderLineUnmarshaller.fromJson(ol)}              // now we've got a List&lt;Result&lt;OrderLine&gt;&gt;. We'll reduce it to a Result&lt;List&lt;OrderLine&gt;             // so that we only continue of all succeed. Missing pattern matching and scala collections here.             val rs = Either.Right(listOf&lt;Model.OrderLine&gt;())             val orderLinesK = orderLines.foldLeft&lt;Result&lt;Model.OrderLine&gt;, Result&lt;List&lt;Model.OrderLine&gt;&gt;&gt;(rs) { res, ol -&gt;                when (res) {                     is Either.Left -&gt; res                     is Either.Right -&gt; when(ol) {                         is Either.Left -&gt; ol                               // set the error                         is Either.Right -&gt; Either.right(res.b.plus(ol.b))                     }                 }             }              // and finally return the unmarshalled object             orderLinesK.map{ ols -&gt; Model.Order(jsonObject.get(\"orderId\").asString, ols)}         } catch (e: Throwable){             Either.left(e)         }     } }   Without going into too much detail here. You can see that we do some custom JSON unmarshalling here to convert our custom generated JSON back to  our data classes. The productUnmarshaller is really straightforward, and we just use the standard gson unmarshall functionality. For  the orderLineUnmarshaller we reuse the productUnmarshaller and use the map function to creae a OrderLine is the Product was parsed  successfully. And for the complete order, we reuse the orderLineUnmarshaller to convert the incoming data to a List&lt;Result&lt;OrderLines&gt;&gt;. We  fold this into a Result&lt;List&lt;OrderLines&gt; failing if one of the orderLines is a Left. Finally we use this result to create the Ordere. At this   point we also see some limitations in the type interference of Kotlin. We need to make explicitly pass in the types for the foldLeft function, so  that the Kotlin compiler understands what is happening.   For completeness sake lets also create some tests for this, so that we know that the marshalles actually do what they’re supposed to do.   object UnmarshallersSpecTest: Spek({      describe(\"The product unmarshaller instance\") {          val productJson = \"\"\"{\"productId\":\"product\",\"description\":\"description\"}\"\"\"         val product = Model.Product(\"product\", \"description\")          it(\"should marshall an product to a JSON value\") {              val result = UnmarshallerInstances.productUnmarshaller.run {                 fromJson(productJson)             }              result.isRight() `should be` true             result.map {                 it `should equal` product             }         }          it(\"should return a Left when JSON is invalid\") {             val result = UnmarshallerInstances.productUnmarshaller.run {                 fromJson(\"invalid\")             }              result.isLeft() `should be` true         }     }       describe(\"The orderline unmarshaller instance\") {          val orderLine = OrderLine(1, 2, Model.Product(\"\", \"\"))         val orderLineJson = \"\"\"{\"lineId\":1,\"quantity\":2,\"product\":{\"productId\":\"\",\"description\":\"\"}}\"\"\"          it(\"should marshall an orderline to a JSON value\") {             val result = UnmarshallerInstances.orderLineUnmarshaller.run {                 fromJson(orderLineJson)             }              result.isRight() `should be` true             result.map {                 it `should equal` orderLine             }         }          it(\"should return a Left when JSON is invalid\") {             val result = UnmarshallerInstances.orderLineUnmarshaller.run {                 fromJson(\"invalid\")             }              result.isLeft() `should be` true         }     }      describe(\"The order unmarshaller instance\") {          val orderLines = listOf(OrderLine(1,2, Model.Product(\"\", \"\")), OrderLine(2,3, Model.Product(\"\", \"\")))         val order = Order(\"orderId\", orderLines)         val orderJson = \"\"\"{\"orderId\":\"orderId\",\"customorderlines\":[{\"lineId\":1,\"quantity\":2,\"product\":{\"productId\":\"\",\"description\":\"\"}},{\"lineId\":2,\"quantity\":3,\"product\":{\"productId\":\"\",\"description\":\"\"}}]}\"\"\"          it(\"should marshall an order to a JSON value\") {             val result = UnmarshallerInstances.orderUnmarshaller.run {                 fromJson(orderJson)             }              result.isRight() `should be` true             result.map {                 it `should equal` order             }         }          it(\"should return a Left when JSON is invalid\") {             val result = UnmarshallerInstances.orderUnmarshaller.run {                 fromJson(\"invalid\")             }              result.isLeft() `should be` true         }     } })   Add this point we’ve seen a little bit of the Arrow functionality already. We’ve used the Either typeclass, and also used the foldLeft and map extensions  Arrow provides on top of the base classes. In the next section we’ll look a bit closer at using the typeclasses directly and use a custom annotation  from Kotlin, that helps in making typeclass instances easier.   Use Arrow functionality: extension annotation   In the previous code fragments we’ve seen that we can create type classes by implementing an interface and assigning it to a value which we then  can use directly. If you’ve only got a small number of typeclasses to create this works nice, but you do have to create the wrappers and assign  values yourself. With the @extension annotation, Arrow generates code which makes it easy for you to get the instance for a specific type.   The following example shows how to use the @extension annotation for our Unmarshallers       @extension     interface OrderLineUnmarshallerInstance : JsonUnmarshaller&lt;Model.OrderLine&gt; {         override fun fromJson(j: String): Result&lt;Model.OrderLine&gt; = try {             // first use the productUnmarshaller to get a Result&lt;Product&gt;             val jsonObject = parser.parse(j).asJsonObject             val jsonProduct = jsonObject.get(\"product\")             val product = productUnmarshaller.fromJson(jsonProduct.toString())              // if product is right, convert it to a product, else we get the error.             product.map{ p -&gt; Model.OrderLine(jsonObject.get(\"lineId\").asInt, jsonObject.get(\"quantity\").asInt, p)}         } catch (e: Throwable){             Either.left(e)         }     }   As you can see not that much has changed. The main thing that has changed is that we now define an interface, and not an val or fun. What  Arrow does, it will generate code that looks like this:   package org.smartjava.fskotlin.orderline.jsonUnmarshaller  import arrow.core.Either import kotlin.String import kotlin.Suppress import kotlin.Throwable import kotlin.jvm.JvmName import org.smartjava.fskotlin.Model.OrderLine.Companion import org.smartjava.fskotlin.OrderLine import org.smartjava.fskotlin.OrderLineUnmarshallerInstance  @JvmName(\"fromJson\") @Suppress(         \"UNCHECKED_CAST\",         \"USELESS_CAST\",         \"EXTENSION_SHADOWED_BY_MEMBER\",         \"UNUSED_PARAMETER\" ) fun fromJson(j: String): Either&lt;Throwable, OrderLine&gt; = org.smartjava.fskotlin.Model.OrderLine    .jsonUnmarshaller()    .fromJson(j) as arrow.core.Either&lt;kotlin.Throwable, org.smartjava.fskotlin.OrderLine&gt;  fun Companion.jsonUnmarshaller(): OrderLineUnmarshallerInstance = object : org.smartjava.fskotlin.OrderLineUnmarshallerInstance {  }   As you can see this will create an OrderLineUnmarshallerInstance on the companion object of the OrderLine class, so we can easily  access it, and create a helper function for easy access to the fromJson function. Throughout the Arrow code base this is used extensively,  which is a good thing, to make sure that all the typeclasses follow the same pattern. For your own projects, I don’t think you should need  this, and it’s probably easier and more straightforward to just define the typeclasses directly and assign them to a fun or a val.   Use Arrow functionality: Foldable typeclass   Before ending this article, I want to clean up a couple of code fragments by using some other arrow typeclasses. The first one is the  orderMarshaller:       val orderMarshaller = object : JsonMarshaller&lt;Model.Order&gt; {         override fun Model.Order.toJson(): JsonElement {             val orderLines = this.orderLines.map { Marshallers.convertToJson(it, orderLineMarshaller) }             val withoutOrderLines = this.copy(orderLines = listOf())             val json = gson.toJsonTree(withoutOrderLines)              val f = JsonArray()             val acc = orderLines.foldLeft(f) { res, el -&gt; res.add(el); res}             val result = json.asJsonObject             result.remove(\"orderLines\")             result.add(\"customorderlines\", acc)              return result         }     }   While this works, it isn’t the best way to do this. We first have to explicitly map the orderLines to a List&lt;JsonElement&gt;, we  can’t use this list directly, but have to convert it again to a JsonArray, before we can use it. What would be nice is if we could  have a JsonMarshaller which would be able to automatically marshall a list or set of T to a JsonArray. If we look at the previous code  what we need to do is some mapping from T to a JsonElement and some folding. To go from T to JsonElement we’ve already got out  JsonMarshaller, and Arrow provides a typeclass called Foldable that allow us to fold over a specific container. With these two   typeclasses we can create a JsonMarshaller instance that is able to create the JsonArray for arbitrary foldable containers like this:       /**      * For the type of Kind&lt;F, T&gt; e.g ListK&lt;T&gt; we can automatically fold them using the provided typeclass. What      * we need to know is evidence that it's foldable, and evidence on how to apply the marshaller to the embedded T      * elements.      */     fun &lt;F, T: Any&gt;foldableToJsonArrayMarshaller(fev: Foldable&lt;F&gt;, ev: JsonMarshaller&lt;T&gt;) = object: JsonMarshaller&lt;Kind&lt;F, T&gt;&gt; {         override fun Kind&lt;F, T&gt;.toJson(): JsonElement = fev.run {             foldLeft(JsonArray()) { b, a -&gt;                 b.add(ev.run { a.toJson() })                 b             }         }     }   Here we use the provided foldable, to invoke foldLeft on the container, and the provided marshaller to convert the T elements contained  in the foldable to a JsonElement. All the converted Ts are aggregated into a JsonArray, which is returned. Ignore the Kind&lt;F, T&gt; stuff  for now, since that is something for a different article. It is enough to know for now, that this is the way Arrow allows us to use   higher kinded types for extensions. With this generic marshaller, we can now also change the orderMarshaller to something like this:       val orderMarshaller = object : JsonMarshaller&lt;Model.Order&gt; {         override fun Model.Order.toJson(): JsonElement {             val json = gson.toJsonTree(this.copy(orderLines = listOf())).asJsonObject             val orderLinesJson = MarshallerInstances.foldableToJsonArrayMarshaller(foldable(), orderLineMarshaller).run { ListK(orderLines).toJson() }              json.remove(\"orderLines\")             json.add(\"customorderlines\", orderLinesJson)              return json         }     }   And you can see that we use the foldableToJsonArrayMarshaller and pass in a foldable() which can be retrieved from the List companion object, and  our marshaller. The rest is done in the same way as for the other typeclasses. This makes our code much more clean, and if we have other containers with T  elements, we can reuse this foldableToJsonArrayMarshaller marshaller.   Conclusion   All in all it isn’t that hard to use and create typeclasses in Kotlin. With Arrow we get a lot of functionality, that makes functional programming,   working with monads and typeclasses much easier. What you do see is that in certain parts the standard library of Kotlin is limited. Pattern matching  is limited, and the lack of implicit arguments in functions makes using typeclasses kind of intrusive. Besides that, the lack of a good API on top of   immutable collections in Kotlin makes me miss standard functionality in other functional languages.   But, Arrow in itself looks great. While it takes some effort to get my head in the right place, the provided type and data classes work as expected (so far)  and I’ll continue experimenting with the various parts to see how Arrow and Kotlin can work together.   ","categories": ["posts","visualization","d3"],
        "tags": [],
        "url": "http://www.smartjava.org/content/kotlin-arrow-typeclasses/",
        "teaser":null},{
        "title": "Webflux with Kotlin and Arrow",
        "excerpt":"A month into the new year and I’m already lagging behind my two articles per month goal. So for Februari I’m going to try and do three articles. To kick of the month in this article we’re going to look into a setup for using kotlin together with webflux and arrow, to create a simple basic REST service. Nothing too fancy yet, just a simple setup.   The code for this example can be found in github: https://github.com/josdirksen/arrow-flux, and I’ll skip the basic gradle stuff for now and just show how everything is connected to each other.   Application structure   For the application structure we’ll use the most simple layered approach we can think off:      REST layer: Based on Spring Webflux, which just offers some JSON endpoints.   Service layer: Plain kotlin service layer, no spring dependencies, just simply orchestrates the calls to the various repositories.   Repository layer: We’ll use mongoDB for storing the data, since that also provides a reactive API.   More information on how to setup gradle, and import the correct libraries, see here: kotlin-arrow-typeclasses   Repository layer   We’ll start at the bottom of the stack, and look at how we define our repositories.   /**  * @param F Wrapper for the single results e.g a ForMonoK or a ForId  * @param S Wrapper for the streaming results e.g a ForFluxK or a ForListK  */ interface ItemRepository&lt;F, S&gt; {      fun storeItem(item: Item): Kind&lt;F, Item&gt;     fun getItem(itemId: UUID): Kind&lt;F, Item&gt;     fun getAllItems(): Kind&lt;F, List&lt;Item&gt;&gt;     fun getAllItemsStreaming(): Kind&lt;S, Item&gt; }   The previous code fragment shows how we can define an interface to a repository. This repository allows us to retrieve Items wher an item is just  a simple data class: data class Item(val id: UUID = UUID.randomUUID(), val name: String, val description: String). The repository is parameterized with two types T ans S. Which are used to define the box in which the items are returned. For this project the idea is to define the F for normal results, and the S as the type for streaming results. But you don’t have to use async types, for instance for testing we could just use the Id monad. For this simple service, though, we’ll implement it with Arrow’s wrappers around a Mono and a Flux. A Mono returns a single async result and is done after that single result, a Flux can return multiple results. I won’t go to deep into the details of these classes here, since that’s a bit out of scope for this article. More information on this can be found in the execellent reactor documentation: Mono-Flux   Before we look a bit closer at how Arrow can help us in the implementation and working with Mono and Flux, a quick note on using Kind&lt;F, Item&gt;. The reason we need to do this, is that Kotlin doesn’t allow us to specify that the F and the S types we define in the interface have to be type constructors. To work around that problem Arrow introduces the Kind&lt;A, B&gt; construct which allows us to define type constructors in a different way. In our example it means that the type F is a Box with an Item. More on this can be found here: https://arrow-kt.io/docs/patterns/glossary/#type-constructors   One of the great features of Arrow is that it allows us to do scala like for-comprehension on the monads that is provides. This allows us to write clean code, instead of going into heavily nested flatmap-map statements. For instance lets say that we’ve got a number of optional values (e.g. value in kotlin that are nullable, since I don’t find working with nullables in Kotlin intuitive, I’d usually just wrap them in an arrow Option).   As an example we’ll take the Item data class. Say that I’m getting the fields as nullables, and want to create an Item from it. The code you can use is something like  this:   val optionUUID: Option&lt;UUID&gt; = ... val optionName: Option&lt;String&gt; = ... val optionDescription: Option&lt;String&gt; = ...  val optionItem = optionUUID.flatMap { uuid -&gt;     optionName.flatMap { name -&gt;         optionDescription.map { desc -&gt;             Item(uuid, name, desc)         }     } }   This will result in an Option&lt;Item&gt;, but as more steps are added, and the nesting becomes deeper, this is really hard to read and reason about. With Arrow, we’ve also got something called a binding. With this we can rewrite the above piece of code to this:   val optionItem = binding {     val uuid = optionUUID.bind()     val name = optionName.bind()     val description = optionName.bind()      Item(uuid, name, description) }   Which is much more readable, and accomplishes exactly the same. More on using binding (and bindingCatch, which provides a good way when interacting with code that can throw exceptions), can be found in the arrow documentation: https://arrow-kt.io/docs/patterns/monad_comprehensions/, but the previous code fragment nicely captures the general idea.  Arrow provides a binding for all of its data types, but also provides extensions for a couple of external libraries. One of them being Reactor which is where the Flux and Mono classes come from. With Arrow we get a MonoK and a FluxK that wrap the Mono and Flux classes, and allows you to use the same binding approach.   For me, at least, this allows us to write cleaner code, avoid unexpected exceptions, and avoid nesting flatMap. An example from the arrow documentation nicely shows this. Without using the arrow wrappers we’ve got this:   getSongUrlAsync()   .map { songUrl -&gt; MediaPlayer.load(songUrl) }   .flatMap {     val totalTime = musicPlayer.getTotaltime()     Flux.interval(Duration.ofMillis(100))       .flatMap {         Flux.create { musicPlayer.getCurrentTime() }           .map { tick -&gt; (tick / totalTime * 100).toInt() }       }       .takeUntil { percent -&gt; percent &gt;= 100 }   }   And with the arrow wrappers we get this:  bindingCatch {   val (songUrl) = getSongUrlAsync()   val musicPlayer = MediaPlayer.load(songUrl)   val totalTime = musicPlayer.getTotaltime()        val end = DirectProcessor.create&lt;Unit&gt;()   Flux.interval(Duration.ofMillis(100)).takeUntilOther(end).bind()        val (tick) = musicPlayer.getCurrentTime()   val percent = (tick / totalTime * 100).toInt()   if (percent &gt;= 100) {     end.onNext(Unit)   }   percent }   Much more readable, and even catches unexpected exceptions. So with this small sidestep, back to the repository. We’re going to implement it by storing the data in mongodb. The implementation for this looks like this:   class ReactiveItemRepository(val mongoTemplate: ReactiveMongoTemplate) : ItemRepository&lt;ForMonoK, ForFluxK&gt;, ReactiveMongoOperations by mongoTemplate {      val ITEM_COLLECTION_NAME = \"items\"      override fun getItem(itemId: UUID): Kind&lt;ForMonoK, Item&gt; =          StoreUtil.asMono {             findById(itemId, Item::class.java, ITEM_COLLECTION_NAME)                 .errorIfEmpty(ITEM_COLLECTION_NAME, itemId.toString())         }      override fun getAllItems(): Kind&lt;ForMonoK, List&lt;Item&gt;&gt; =         StoreUtil.asMono {             findAll(Item::class.java, ITEM_COLLECTION_NAME)                 .collectList()         }      override fun storeItem(order: Item): Kind&lt;ForMonoK, Item&gt; =         StoreUtil.asMono {             insert(order, ITEM_COLLECTION_NAME)                 .mapToError(ITEM_COLLECTION_NAME, order.id.toString())         }      override fun getAllItemsStreaming(): Kind&lt;ForFluxK, Item&gt; =         StoreUtil.asFlux {             findAll(Item::class.java, ITEM_COLLECTION_NAME)         } }   We use the Arrow provided ForMonoK and ForFluxK types. These are defined as a typealias like this: typealias MonoKOf&lt;A&gt; = arrow.Kind&lt;ForMonoK, A&gt;. Now we can just have our functions return a MonoK&lt;T&gt;. The result from the functions on the mongoTemplate are either a Mono or a Flux, to convert these to the relevant MonoK or FluxK we’ve got a simple helper in the StoreUtil class:       fun &lt;T&gt;asMono(thunk: () -&gt; Mono&lt;T&gt;): MonoK&lt;T&gt; {         return monokBindingCatch {             thunk().k().bind()         }     }      fun &lt;T&gt;asFlux(thunk: () -&gt; Flux&lt;T&gt;): FluxK&lt;T&gt; {         return fluxkBindingCatch {             thunk().k().bind()         }     }   We’ve also use some other helper functions, to map the Mongo specific errors to our own domain specific errors. Since a Mono and a Flux can carry these errors, there is no need to wrap the result of these functions in an Either or a Try.   Service layer   Since we don’t want to call the reposity layer directly, lets wrap it with a (in the example kind of useless) service layer. First the interface and the implementation, which are very similar to the repository we just saw:   interface ItemService&lt;F&gt; {      fun getItem(itemId: UUID): Kind&lt;F, Item&gt;     fun createItem(order: Item): Kind&lt;F, Item&gt;     fun getAllItems(): Kind&lt;F, List&lt;Item&gt;&gt; }  class ReactiveItemService(val itemRepository: ItemRepository&lt;ForMonoK&gt;) :     ItemService&lt;ForMonoK&gt; {      override fun getAllItems(): Kind&lt;ForMonoK, List&lt;Item&gt;&gt; = itemRepository.getAllItems()     override fun createItem(item: Item): Kind&lt;ForMonoK, Item&gt; = itemRepository.storeItem(item)     override fun getItem(itemId: UUID): Kind&lt;ForMonoK, Item&gt; = itemRepository.getItem(itemId) }   Since we don’t really have any business logic, connect to other components, brokers etc. or need to combine results from multiple repositories, this is a really basic implementation. But lets pretend we also want to create the following function:   fun updateItemIExistsAndReturnAllItems(toUpdate: UUID, description: String): Kind&lt;F, Item&gt;   An implementation of this function would look something like this:       override fun updateItemIExistsAndReturnAllItems(toUpdate: UUID, description: String): Kind&lt;ForMonoK, List&lt;Item&gt;&gt; = binding {         val existingItem = itemRepository.getItem(toUpdate).bind()         val updatedItem = existingItem.copy(description = description)         val storedItem = itemRepository.storeItem(updatedItem).bind()          itemRepository.getAllItems().bind()     }   As you can see, just a sequence of steps, even though all the steps themselves are using reactor elements. If one of the intermediary steps fails, the Mono as a whole will fail. With this approach we can write clean, easily understandable code, without having to deal with coroutines, or reactor internals ourselves.   Rest layer with WebFlux   The final step is exposing the service throug a REST api. For this demo I’ve just Spring Webflux, since that nicely ties into the whole reactor ecosystem. Let’s start with the code:   object ItemsRoute {      const val ITEMS_PATH = \"items\"     const val PATH_ID = \"id\"      class ItemRoutes(itemsRouteHandler: ItemsRouteHandler) {          val route: RouterFunction&lt;ServerResponse&gt; = router {              (path(\"$BASE_ROUTE/$ITEMS_PATH\") and accept(APPLICATION_JSON)).nest {                 POST(\"\", itemsRouteHandler::createOrder)                 GET(\"\", itemsRouteHandler::getAllOrders)                 GET(\"/{$PATH_ID}\", itemsRouteHandler::getOrder)             }         }     }      class ItemsRouteHandler(val itemService: ItemService&lt;ForMonoK, ForFluxK&gt;) {          fun getAllOrders(request: ServerRequest): Mono&lt;ServerResponse&gt; = okJsonList {             itemService.getAllItems().fix()         }          fun getOrder(request: ServerRequest): Mono&lt;ServerResponse&gt; = okJson {             binding {                 val uuid = pathParamToUUID(request, PATH_ID).bind()                 itemService.getItem(uuid).bind()             }         }          fun createOrder(request: ServerRequest): Mono&lt;ServerResponse&gt; = okJson {             binding {                 val item = request.bodyToMono&lt;Item&gt;().k().bind()                 itemService.createItem(item).bind()             }         }     } }   Based on what we’ve seen so far, this shouldn’t be too difficult to understand, since everywhere throughout this example we use the same approach:      Make sure we get MonoK instances.   Sequence them through a binding   Return the result of the last bind, which result in a new MonoK   There are a couple of specifics here, for instance the helper okJson function:       inline fun &lt;reified T&gt;okJson(p: () -&gt; MonoK&lt;T&gt;) = ServerResponse.ok()         .contentType(MediaType.APPLICATION_JSON)         .body(BodyInserters             .fromPublisher(p().mono, T::class.java))   Which just return a 200 and converts the result from the MonoK using the default marshallers. Another custom piece is the val uuid = pathParamToUUID(request, PATH_ID).bind() part. Where we convert an incoming parameter to a MonoK.       fun pathParamToUUID(request: ServerRequest, param: String): MonoK&lt;UUID&gt; = Mono             .just(request.pathVariable(param))             .map { UUID.fromString(it) }             .onErrorMap { t -&gt; PropertyValidationException(                 invalidProperty = param,                 cause = t,                 message = \"Parameter {$param} can't be converted to a UUID\")             }.k()   Here we get the variable from the path (which we wrap in a normal Mono). If it fails, we map it to an exception, finally, we use the k() function to convert the Mono to a MonoK. The result from this validation step is then used in a sequence of MonoKs:   binding {     val uuid = pathParamToUUID(request, PATH_ID).bind()     itemService.getItem(uuid).bind() }   Another important part is where we define the route:   val route: RouterFunction&lt;ServerResponse&gt; = router {      (path(\"$BASE_ROUTE/$ITEMS_PATH\") and accept(APPLICATION_JSON)).nest {         POST(\"\", itemsRouteHandler::createOrder)         GET(\"\", itemsRouteHandler::getAllOrders)         GET(\"/{$PATH_ID}\", itemsRouteHandler::getOrder)     } }   This maps the url path to the function, that calls into a service.   Setup the application   So far we’ve pretty much avoided Spring and Spring Boot. We just define services and use constructors to inject the correct depedencies. Webflux, however, does require a spring bean context to tie everything together. Since I don’t really like having all kinds of annotations and spring specific stuff in the core of the application, we only use Spring in this setup to connect the beans together.   class Application {      val log = LoggerFactory.getLogger(Application::class.java)      private val httpHandler: HttpHandler     private val server: HttpServer      /**      * Setup the http server. Delegates to the bean named \"webHandler\" defined by      * the beans function.      */     constructor() {          // setup the context containing all our spring stuff         val context = GenericApplicationContext().apply {             allBeans().initialize(this)             refresh()         }         server = HttpServer.create().port(Config.SERVER_PORT)          // create the http handler, and assign filters         httpHandler = WebHttpHandlerBuilder                 .applicationContext(context)                 .build()     }      fun startAndAwait() {         server.handle(ReactorHttpHandlerAdapter(httpHandler))             .bindUntilJavaShutdown(Duration.ofSeconds(Config.SERVER_SHUTDOWN_TIMEOUT)) { t -&gt;             log.info(\"Server Started\")         }     } }  /**  * Starts the service, and waits until shutdown signal is received.  */ fun main(args: Array&lt;String&gt;) {     Application().startAndAwait() }   The setup which you see above, starts a simple server. The important part of this is the httpHandler = WebHttpHandlerBuilder.applicationContext(context).build(), which uses the supplied application context to create the reactive HTTP handler. To create this application context you can use any approach that Spring provides. For Kotlin, Spring provides a more functional DSL, to declaratively setup this context. We do this in the allbeans() function that ius called when this class is instantiated. This function looks like this:   fun allBeans() = beans {     routeBeans(this)     filterBeans(this)     serviceBeans(this)     repoBeans(this)     errorHandlerBeans(this) }  fun filterBeans(ctx: BeanDefinitionDsl) = with (ctx) {     bean { CorsWebFilter { CorsConfiguration().applyPermitDefaultValues() } } }  fun routeBeans(ctx: BeanDefinitionDsl) = with (ctx) {     bean(WebHttpHandlerBuilder.WEB_HANDLER_BEAN_NAME) {         RouterFunctions.toWebHandler(ref&lt;ItemRoutes&gt;().route, HandlerStrategies.withDefaults())     }      bean&lt;ItemRoutes&gt;()     bean&lt;ItemsRouteHandler&gt;() }  fun serviceBeans(ctx: BeanDefinitionDsl) = with (ctx) {     bean&lt;ReactiveItemService&gt;() }  fun repoBeans(ctx: BeanDefinitionDsl) = with (ctx) {     bean { ReactiveMongoTemplate(ref(), Config.DATABASE_NAME) }     bean&lt;MongoClient&gt; { MongoClients         .create(             MongoClientSettings.builder()                 .applyToClusterSettings {it.hosts(listOf(ServerAddress(Config.DATABASE_HOST, Config.DATABASE_PORT)))}                 .build())     }     bean&lt;ReactiveItemRepository&gt;() }  fun errorHandlerBeans(ctx: BeanDefinitionDsl) = with (ctx) {     bean&lt;ErrorWebFluxAutoConfiguration&gt;()     bean&lt;ErrorWebExceptionHandler&gt; {         ref&lt;ErrorWebFluxAutoConfiguration&gt;().errorWebExceptionHandler(ref&lt;DefaultErrorAttributes&gt;())     }      bean&lt;DefaultErrorAttributes&gt; {         ErrorMapping.registerExceptions()         object: DefaultErrorAttributes() {              override fun getError(request: ServerRequest?): Throwable {                  val originalError = super.getError(request)                  val originalErrorClass = originalError.javaClass                   return ErrorMapping.mappings.get(originalErrorClass)?.invoke(originalError) ?: originalError             }         }     }     bean&lt;ResourceProperties&gt;()     bean&lt;ErrorProperties&gt;()     bean&lt;ServerProperties&gt;()     bean&lt;DefaultServerCodecConfigurer&gt;() }   Here we use the bean&lt;&gt; and ref() functions to create all the beans which are needed to setup the correct a WebFlux application. Note that there is also a spring-boot way of doing this, but I decided that I wanted to have control on how everything was set up, so created the needed beans using this approach.   Conclusion   I haven’t added the tests yet to the github project so I’ll get back to that in a later article. Easiest way to learn a bit more on how these tools work together might be to just check out the sample: https://github.com/josdirksen/arrow-flux, and play around with it a bit.   Should you have any questions, just let me know.  ","categories": ["arrow","webflux","kotlin"],
        "tags": [],
        "url": "http://www.smartjava.org/content/webflux-arrow/",
        "teaser":null},{
        "title": "Scala/Kotlin developer learning Haskell - Part 1 - Rest endpoint with JSON",
        "excerpt":"I’ve been doing scala and FP for a while now, and during that time I’ve tried a couple of times to pickup haskell as well. What I noticed  was that I knew too little about monad transformers, typeclasses and a lot of other concepts which are much more in the forefront when  doing Haskell, compared to Scala (or Kotlin with Arrow for that matter). That combined with the foreign syntax, caused me to abandon those  attempts.   So a new year, and a new attempt. This time I’m going to try by just building applications, since that usually seems to work best for   me when learning a new language. So in this series prepare to see a lot of stupidly solved haskell issues, so bear with me.   Simple REST webservice: skeleton   As a first step I’m going to creat a very simple webservice, with just a couple of routes, and implement it using Haskell. I’ve been looking  for a simple REST framework, and landed on Scotty https://github.com/scotty-web/scotty. Scotty is a   haskell microservices framework that should be really easy to use.   After some messing around with all the different build tools, I’ve got a simple web server:   {-# LANGUAGE OverloadedStrings #-}  module Main where  import Web.Scotty (scotty, ScottyM, ActionM, text, get, param)  paramRoute :: ScottyM () paramRoute = get \"/something/:id\" $ do   id &lt;- param \"id\"   text $ \"this is a text with id: \" &lt;&gt; id  main = do   putStrLn \"Start simple server\"   scotty 3000 $ do     paramRoute   Which even returns what I expected!   curl -v localhost:3000/something/123 * Connected to localhost (127.0.0.1) port 3000 (#0) &gt; GET /something/123 HTTP/1.1 &gt; Host: localhost:3000 &gt; User-Agent: curl/7.54.0 &gt; Accept: */* &gt; &lt; HTTP/1.1 200 OK &lt; Transfer-Encoding: chunked &lt; Date: Mon, 18 Feb 2019 18:07:37 GMT &lt; Server: Warp/3.2.26 &lt; Content-Type: text/plain; charset=utf-8 &lt; this is a text with id: 123%   While the code is only a couple of lines. For me it took some effort to understand what is actually happening here. Let’s start   at the top. {-# LANGUAGE OverloadedStrings #-} allows scotty to parse strings to a RouterPattern (more and that further down). Next we have some module definitions, and some imports which aren’t that interesting. Next lets look at the server entrpoint:   main = do   putStrLn \"Start simple server\"   scotty 3000 $ do     paramRoute   Here we have a main method, which can output some IO stuff. It’s signature is main :: IO(). So the result of the main function should be an IO monad. I’ll get back to this monad in a later article, since it’s kind of special one. For now it is enough to know that the IO monad is used to do non-pure stuff like filo IO, stdout/stdin stuff, and I assume also network IO. With do we start a monad comprehension (just like the for-comprehensions from scala or binding approach from Arrow-KT). putStrLn returns an IO (), and scotty, which we use to launch the service, also results in an IO monad, so we can nicely combine these using do.   The signature of scotty is:   scotty :: Port -&gt; ScottyM () -&gt; IO ()   So we’ve already provided scotty with a port, and next we need to add a ScottyM () which will result in an IO(). Note that we can use a $ to separate function parts. In this case  it means we first run the do paramRoute part, and pass the result into the scotty function (we could also have used parenthesis if we wanted).   You can see that we pass in the paramRoute which returns this ScottyM () function we need:   paramRoute :: ScottyM () paramRoute = get \"/something/:id\" $ do   id &lt;- param \"id\"   text $ \"this is a text with id: \" &lt;&gt; id   This works because the signature of get is:   get :: RoutePattern -&gt; ActionM () -&gt; ScottyM ()   We pass in:     RoutePattern: This is the \"/something/:id\" string, which works because of the overloaded language feature.   do ...: And we pass in a monad comprehension, which should result in a ActionM () for us to return.   Scotty comes with a whole set of building blocks we can use to create the ActionM ()monad.   text :: Text -&gt; ActionM () file :: FilePath -&gt; ActionM () html :: Text -&gt; ActionM () json :: ToJSON a =&gt; a -&gt; ActionM ()   And provides other REST/HTTP related ActionM functions like:   param :: Trans.Parsable a =&gt; Text -&gt; ActionM a body :: ActionM ByteString headers :: ActionM [(Text, Text)]   In our case we use the param function to get the id from the url, and the text function to create a ActionM (). And with that our function is done,   and our minimal webserver works.   Add more routes   With this basic setup we can easily add more routes by just creating functions that return a ScottyM:   paramRoute :: ScottyM () paramRoute = get \"/something/:id\" $ do   id &lt;- param \"id\"   text $ \"this is a text with id: \" &lt;&gt; id  htmlRoute :: ScottyM () htmlRoute = get \"/thisishtml\" $ do   html \"&lt;h1&gt;Woohoo&lt;/h1&gt;\"  simpleRoute :: ScottyM () simpleRoute = get \"/and/a/nested/one\" $ do   text \"This is from the nested route\"  -- entrypoint to the server main = do   putStrLn \"Start simple server\"   scotty 3000 $ do     paramRoute     htmlRoute     simpleRoute   Which is pretty convient, and actually pretty much similar as WebFlux does it or Akka-HTTP. At this point I was already pretty proud of myself, since  I’ve come further with Haskell than in all my previous tries. When looking through the scotty doc I saw an example of how to add JSON support, so as  a final step, let’s do that here as well.   Add JSON support   Scotty has standard JSON support using Aeson. And to use it, we can use the json function:   json :: ToJSON a =&gt; a -&gt; ActionM ()   I had some trouble reading this function the first time I saw it. But apparently the stuff before the =&gt; tells us that we need to have a ToJson typeclass for the type a. So to use this, we need to define such an instance for our type. Looking at the documentation for ToJSON:   -- * The compiler can provide a default generic implementation for -- 'toJSON'. -- -- To use the second, simply add a @deriving 'Generic'@ clause to your -- datatype and declare a 'ToJSON' instance...   So we can create a data type and add Generic to it (we also add Show to it, so we can print it from the REPL)   data Item = Item { itemId :: Int, name :: String, description :: String } deriving (Show, Generic) instance ToJSON Item   And that should be enough. We can test this in the REPL using the encode function from Aeson (encode :: (ToJSON a) =&gt; a -&gt; L.ByteString)   *Main Lib&gt; import Data.Aeson *Main Lib Data.Aeson&gt; spork = Item {itemId = 1, name = \"Spork\", description = \"Is it a spoon, is it a fork? I don't know!!\"} *Main Lib Data.Aeson&gt; encode spork \"{\\\"itemId\\\":1,\\\"name\\\":\\\"Spork\\\",\\\"description\\\":\\\"Is it a spoon, is it a fork? I don't know!!\\\"}\"   And it actually works. So we’ve created the ToJSON typeclass for our data type, and it seems to be automatically in scope as well, which is nice. So with   this setup, we can add a new function that outputs JSON, and our complete example looks like this:   {-# LANGUAGE OverloadedStrings #-} {-# LANGUAGE DeriveGeneric #-}  module Main where  import Web.Scotty (scotty, ScottyM, ActionM, text, html, get, param, json) import GHC.Generics import Data.Aeson (FromJSON, ToJSON)  data Item = Item { itemId :: Int, name :: String, description :: String } deriving (Show, Generic) spork = Item {itemId = 1, name = \"Spork\", description = \"Is it a spoon, is it a fork? I don't know!!\"}  instance ToJSON Item  paramRoute :: ScottyM () paramRoute = get \"/something/:id\" $ do   id &lt;- param \"id\"   text $ \"this is a text with id: \" &lt;&gt; id  htmlRoute :: ScottyM () htmlRoute = get \"/thisishtml\" $ do   html \"&lt;h1&gt;Woohoo&lt;/h1&gt;\"  simpleRoute :: ScottyM () simpleRoute = get \"/and/a/nested/one\" $ do   text \"This is from the nested route\"  jsonRoute :: ScottyM () jsonRoute = get \"/json\" $ do   json spork  -- entrypoint to the server main = do   putStrLn \"Start simple server\"   scotty 3000 $ do     paramRoute     htmlRoute     simpleRoute     jsonRoute   And it works right away!   curl -v localhost:3000/json * Connected to localhost (127.0.0.1) port 3000 (#0) &gt; GET /json HTTP/1.1 &gt; Host: localhost:3000 &gt; User-Agent: curl/7.54.0 &gt; Accept: */* &gt; &lt; HTTP/1.1 200 OK &lt; Transfer-Encoding: chunked &lt; Date: Mon, 18 Feb 2019 19:18:46 GMT &lt; Server: Warp/3.2.26 &lt; Content-Type: application/json; charset=utf-8 &lt; * Connection #0 to host localhost left intact {\"itemId\":1,\"name\":\"Spork\",\"description\":\"Is it a spoon, is it a fork? I don't know!!\"}%   Conclusions   Everything went suprisingly well this time. I didn’t run into too weird operators and syntax stuff and could focus on trying to  understand what was happening in the code. I really enjoyed this first exploration of  Haskell and was really pleasantly suprised.  ","categories": ["haskell","fp","posts"],
        "tags": [],
        "url": "http://www.smartjava.org/content/learning-haskell-part-1/",
        "teaser":null},{
        "title": "Scala/Kotlin developer learning Haskell - Part 2 - Monad Transformers",
        "excerpt":"In the previous article learning haskell part 1 I walked through the steps I took to   get a very simple and basic REST endpoint up and running. All that went suprisingly well and during that simple exercise I learned a lot about the  syntax of Haskell, and for the first actually felt part of the language click. Based on that positive experience I decided to go ahead and dive  deeper into how Scotty (the REST framework I used) actually worked. More specifically, what is this ScottyM thing I have to pass in, to get the   server up and running:   scotty :: Port -&gt; ScottyM () -&gt; IO () -- where ScottyM is type ScottyM = ScottyT Text IO -- and ScottyT is newtype ScottyT e m a = ScottyT { runS :: State (ScottyState e m) a }     deriving ( Functor, Applicative, Monad ) -- and ScottyState data ScottyState e m =     ScottyState { middlewares :: [Wai.Middleware]                 , routes :: [Middleware m]                 , handler :: ErrorHandler e m                 }   I guess ScottyState is where Scotty keeps track of how it is configured. So when adding routes, they’re probably added to the ScottyState managed by the State monad. Since that part seemed to be more part of the Scotty internals, I decided not to follow that part, but focus on this newtype construct since  that apparently is how you define new types in haskell. Looking through some examples online, using newtype is often used to define a stack of monad transformers. I’ve used monad transformers here and there in my scala projects, but doing that in Scala always felt a bit convoluted and required quite a bit of   boilerplate code to get the desired effect. So as an exercise I decided to see how that works in Haskell, since that also allows me to look into some of the standard monads in Haskell.   Reader and Writer monads   Most functional languages come with a Reader and a Writer monad. Basically a Reader monad allows you to run your functions within a specific context (e.g pass in an environment, with some configuration), where we Writer monad allows you to provide some output when running this monad (e.g write logs, collect metrics). I’ve used both in Scala. The first problem I had was, how can I test working with Reader and Writer. But in Haskell the monad class is defined like this:   class Monad m where   (&gt;&gt;=)  :: m a -&gt; (  a -&gt; m b) -&gt; m b   (&gt;&gt;)   :: m a -&gt;  m b         -&gt; m b   return ::   a                 -&gt; m a   fail   :: String -&gt; m a   Which, I guess, means that with the return function we can create a Monad from a value. And (&gt;&gt;) seems to be the some sequencing function, where we pass in a m a, then a m b and return the m b. And (&gt;&gt;=) looks like flatMap, where we pass in a function from a to m b and return that m b.   *Main Lib&gt; data Environment = Environment {foo :: String} *Main Lib&gt; let a1 = return \"Some string\" :: Reader Environment Int *Main Lib&gt; let a1 = return \"Some string\" :: Reader Environment String *Main Lib&gt; :t a1 a1 :: Reader Environment String   And that works! Now, if we can use return to create the monad, we can probably also use that in a do notation and access the environment as well.   data Environment = Environment {foo :: String} env1 = Environment \"Hello\"  withdo :: Reader Environment String withdo = do     config &lt;- ask     return $ \"Value from config: \" &lt;&gt; foo config  *Main Lib&gt;  runReader withdo env1 \"a valueHello\"   In this code fragment we access the environment using ask: ask   :: m r, which allows us access to the passed in environment. The foo value from this passed in environment is concatenated to a string, and returned in a monad again. To run the monad with a passed in environment we need to call the runReader function: runReader :: Reader r a -&gt; r -&gt; a. This one takes our monad as first argument, the environment as the second one, and returns the boxed value.   So far, so good. Using the reader monad directly is really trivial, and the writer monad probably isn’t that much more complex:   writer1 :: Int -&gt; Writer [String] Int writer1 i = do   _ &lt;- tell [\"A first message\"]   _ &lt;- tell [\"A second message\"]   return i  *Main Lib&gt; let w = writer1 1 *Main Lib&gt; execWriter w [\"A first message\",\"A second message\"]   Here we define a function which takes an Int uses the writer monad to add two messages to the [String] (which adds them to the list automatically!). So using and creating the monads in themselves is very straightforward. Using the &gt;&gt;= flatmap kind of thingy also works as expected!   reader1 :: Reader Environment String reader1 = do     config &lt;- ask     return $ \"The first step: \" &lt;&gt; foo config  reader2 :: String -&gt; Reader Environment String reader2 s = do     config &lt;- ask     return $ s &lt;&gt; \"The second step\" &lt;&gt; foo config  let m = (reader1) &gt;&gt;= (\\p -&gt; (reader2 p)) *Main Lib&gt; runReader m env1 \"The first step: HelloThe second stepHello\"   Where we can rewrite the lambda thingy (lambdas in Haskell start with a \\) to this: let m = reader1 &gt;&gt;= reader2   Combining a Reader, Writer and IO monad   So far, I’m getting the hang of the Haskell syntax a bit, now let’s look at how we do monad transformers in Haskell.   What I want to have is an:     IO monad to which I can do a putStrLn   a Writer monad to write some logging   a Reader monad to pass in some environment   In Haskell for most of the standard monads we can get the Monad transformer variant by just adding a T. So we’ve got a ReaderT, and a Writer T. The IO monad is a special one, and apparently needs to be the most inner one.   Using monad transformers in haskell can be done by just defining them as the return type. So in our case that’d be something like that: ReaderT Environment (WriterT String IO) ().   And the code to use this would be this:   wlog :: Show w =&gt; w -&gt; ReaderT Environment (WriterT String IO) () wlog w = do   p &lt;- ask   tell (\"INFO (env:  \" &lt;&gt; (foo p) &lt;&gt; \") - \" &lt;&gt; show w)  actions :: ReaderT Environment (WriterT String IO) () actions = do   localEnv &lt;- ask   _ &lt;- wlog \"Running action\"   liftIO $ putStrLn (\"Doing IO with env: \" &lt;&gt; foo localEnv)  main = execWriterT (runReaderT (actions) env1) &gt;&gt;= putStrLn  *Main Lib&gt; main Doing IO with env: Hello INFO (env:  Hello) - \"Running action\"   While this works, it is kind of annoying. We have to specify the type in each of our functions, and if we want to add another transformer to the stack we’d have to change the signature of all the functions.   Haskell allows us, luckily, to define new types. This is also what happens in the Scotty type configuration we saw in the beginning:   newtype ScottyT e m a = ScottyT { runS :: State (ScottyState e m) a }     deriving ( Functor, Applicative, Monad )   So with this, in Haskell we define a new type called ScottyT, which wraps a State monad. This new type takes 3 type parameters e, m and a. If we want to do something similar for the ReaderT Environment (WriterT String IO) () type, we get something like this:   newtype App a = App {runApp :: ReaderT Environment (WriterT String IO) a} deriving                 (Functor, Applicative, Monad, MonadIO, MonadWriter String, MonadReader Environment)   In this case we’ve created a new type called App. And through the deriving clause at the end, all the provided type classes are automatically implemented by this new type. With this type we can now change the signature of our functions to this:   -- helper function which returns the current environment, wrapped in our custom stack env :: App Environment env = ask  -- simple very basic logger, which wlog :: Show w =&gt; w -&gt; App () wlog w = do   p &lt;- ask   tell (\"INFO (env:  \" &lt;&gt; (foo p) &lt;&gt; \") - \" &lt;&gt; show w)  -- runs a number of steps all inside our custom transformer stack. This -- would normally be your complete application actions :: App () actions = do   localEnv &lt;- env   _ &lt;- wlog \"Running action\"   liftIO $ putStrLn (\"Doing IO with env: \" &lt;&gt; foo localEnv)   While this cleans up the signatures, we now do have to change how we can run this monad. We first have to call runApp to create this stack of monad transformers.   *Main Lib&gt; let mt = runApp actions *Main Lib&gt; :type mt mt :: ReaderT Environment (WriterT String IO) ()   Now that we have this stack, we can do the normal runReaderT and execWriter stuff again. So the resulting call looks like this:   main = execWriterT (runReaderT ( runApp actions) env1) &gt;&gt;= putStrLn  *Main Lib&gt; main Doing IO with env: Hello INFO (env:  Hello) - \"Running action\"   So defining a newtype which returns a stack of monadtransformers is really easy. It took some time for me to get this far, though. But with what I know now, it shouldn’t be too difficult to add more monads to this stack.   Adding a State monad to the stack   To add a state transformer, we first create a new newtype:   newtype App2 a = App2 {runApp2 :: StateT LocalState (ReaderT Environment (WriterT [String] IO)) a} deriving                 (Functor, Applicative, Monad, MonadIO, MonadWriter [String], MonadReader Environment)   To use a state we’ve got the following two functions:   get :: m s put :: s -&gt; m ()   We’ve a get function to get the current state and with put we can replace the current state. So let’s add this to a log function, so we keep track of how many time it has been called.   wlogs :: Show w =&gt; w -&gt; App2 () wlogs w = do   p &lt;- ask   s &lt;- get   _ &lt;- put $ LocalState $ count s + 1   u &lt;- get   tell ([\"INFO (env:  \" &lt;&gt; (foo p) &lt;&gt; \") - \" &lt;&gt; (show $ count u) &lt;&gt; \" - \" &lt;&gt; show w ])   There are probably better ways to do this, but in this log function, we again log the environment, and also log the localstate whose count we increase by 1. We can make this part of our main do flow:   actions :: App2 () actions = do   localEnv &lt;- env   _ &lt;- wlogs \"Doing something\"   _ &lt;- wlogs \"And doing some more\"   liftIO $ putStrLn (\"Doing IO with env: \" &lt;&gt; foo localEnv)   And run it like this (I’ve also ):   main = execWriterT (runReaderT (runStateT ( runApp2 actions) state1) env1) &gt;&gt;= mapM_ putStrLn  *Main Lib&gt; main Doing IO with env: Hello INFO (env:  Hello) - 1 - \"Doing something\" INFO (env:  Hello) - 2 - \"And doing some more\"   I’ve also changed the writer to use an array of Strings, so we can separate the output on each line. And this just works, which is really cool. At least for me that is.   Conclusion   My initial goal was to also append the standard Scotty monad to use this kind of logging and passing in of an environment, but for me getting this working was enough for this part.   In the end though, I feel like I’m progressing quite nicely. I now understand the monad transformers a bit more and like the way how you can just define new types to clean up the signatures. In the next part I’ll come back to scotty and start looking into connecting databases and adding some logging.   ","categories": ["haskell","fp","posts"],
        "tags": [],
        "url": "http://www.smartjava.org/content/learning-haskell-part-2/",
        "teaser":null},{
        "title": "Setting up a simple Kafka cluster with docker for testing",
        "excerpt":"In this short article we’ll have a quick look at how to set up a Kafka cluster locally, which can be easily   accessed from outside of the docker container. The reason for this article is that most of the example you can  find either provide a single Kafka instance, or provide a way to set up a Kafka cluster, whose hosts can only be accessed from within the docker container.   I ran into this issue when I needed to reproduce some strange issues with the Kafka instance provided by our private cloud provider. When maintenance happened and the nodes were being cycled, some topics and consumers seemed to completely loss track, and were unable to recover. They needed a restart of the service, before messages were being processed again.   Docker / Kafka setup   The main setup here is just a simple docker-compose file based on the great set of docker images from https://github.com/wurstmeister/kafka-docker. So without much further introduction, we’ll look at the docker-compose file I used for this:   version: '2' services:   zookeeper:     image: wurstmeister/zookeeper     ports:       - \"2181:2181\"   kafka-1:     image: wurstmeister/kafka     ports:       - \"9095:9092\"     environment:       KAFKA_ADVERTISED_HOST_NAME: kafka1.test.local       KAFKA_ADVERTISED_PORT: 9095       KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181       KAFKA_LOG_DIRS: /kafka/logs       KAFKA_BROKER_ID: 500       KAFKA_offsets_topic_replication_factor: 3     volumes:       - /var/run/docker.sock:/var/run/docker.sock       - ${KAFKA_DATA}/500:/kafka    kafka-2:     image: wurstmeister/kafka     ports:       - \"9096:9092\"     environment:       KAFKA_ADVERTISED_HOST_NAME: kafka2.test.local       KAFKA_ADVERTISED_PORT: 9096       KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181       KAFKA_LOG_DIRS: /kafka/logs       KAFKA_BROKER_ID: 501       KAFKA_offsets_topic_replication_factor: 3     volumes:       - /var/run/docker.sock:/var/run/docker.sock       - ${KAFKA_DATA}/501:/kafka    kafka-3:     image: wurstmeister/kafka     ports:       - \"9097:9092\"     environment:       KAFKA_ADVERTISED_HOST_NAME: kafka1.test.local       KAFKA_ADVERTISED_PORT: 9097       KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181       KAFKA_LOG_DIRS: /kafka/logs       KAFKA_BROKER_ID: 502       KAFKA_offsets_topic_replication_factor: 3     volumes:       - /var/run/docker.sock:/var/run/docker.sock       - ${KAFKA_DATA}/502:/kafka   What we see here is a simple docker-compose file where we define a single Zookeeper node and three kafka nodes. Note that   I’ve also expect the KAFKA_DATA variable to be set, which is used as an external volume. That way we don’t lose the data  when we remove the cluster. Let’s look a bit closer at the individual Kafka nodes:     kafka-3:     image: wurstmeister/kafka     ports:       - \"9097:9092\"     environment:       KAFKA_ADVERTISED_HOST_NAME: kafka1.test.local       KAFKA_ADVERTISED_PORT: 9097       KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181       KAFKA_LOG_DIRS: /kafka/logs       KAFKA_BROKER_ID: 502       KAFKA_offsets_topic_replication_factor: 3     volumes:       - /var/run/docker.sock:/var/run/docker.sock       - ${KAFKA_DATA}/502:/kafka   Here we see the following:      We expose the Kafka port 9092 on the external host on a unique port 9097 (we do this for each Kafka node in the cluster).   To make this work correctly we also set the KAFKA_ADVERTISED_PORT to 9097, so clients can connect to the nodes correctly after discovery.   We need a unique host name for each node, if not. The cluster will complain the it already has a node with that name. So we set the unique name using the KAFKA_ADVERTISED_HOST_NAME. We use kafka1.test.local, kafka2.test.local and kafka3.test.local as host name for our cluster.   Besides the settings above, we also give each node a unique id, using the KAFKA_BROKER_ID property.   The final step to take to get this working is having to make sure we can resolve the hosts (kafka1.test.local) specified here correctly. We can run our own dns server for this, but it is easier to just update the local host file /etc/hosts.   10.82.6.17 kafka1.test.local 10.82.6.17 kafka2.test.local 10.82.6.17 kafka3.test.local   Running the cluster   At this point we can simply start the cluster using docker-compose:   $ export KAFKA_DATA=/Users/jos/dev/data/cer/kafka                                   $ docker-compose -f ./docker-compose-local-kafka-cluster.yml up -d                Starting local_zookeeper_1 ... done Creating local_kafka-3_1   ... done Creating local_kafka-1_1   ... done Creating local_kafka-2_1   ... done  $ docker ps                                                                  CONTAINER ID        IMAGE                    COMMAND                  CREATED             STATUS              PORTS                                                NAMES da9d108cbc0e        wurstmeister/kafka       \"start-kafka.sh\"         7 seconds ago       Up 6 seconds        0.0.0.0:9095-&gt;9092/tcp                               local_kafka-1_1 3819d7ca3d7c        wurstmeister/kafka       \"start-kafka.sh\"         7 seconds ago       Up 6 seconds        0.0.0.0:9096-&gt;9092/tcp                               local_kafka-2_1 f8dc6ff937c6        wurstmeister/kafka       \"start-kafka.sh\"         7 seconds ago       Up 6 seconds        0.0.0.0:9097-&gt;9092/tcp                               local_kafka-3_1 62dfc7ea32c6        wurstmeister/zookeeper   \"/bin/sh -c '/usr/sb…\"   2 days ago          Up 6 seconds        22/tcp, 2888/tcp, 3888/tcp, 0.0.0.0:2181-&gt;2181/tcp   local_zookeeper_1   We can quickly check which nodes are part of the cluster by running a command against zookeeper:   $ docker exec -ti 62dfc7ea32c6 ./bin/zkCli.sh ls /brokers/ids ... WATCHER::  WatchedEvent state:SyncConnected type:None path:null [501, 502, 500]   And that’s it. We’ve now got a cluster up and running, which we can use from outside the docker container, by just connecting to one of the three hosts. From the outside world, it’ll look like a valid cluster and we can test failover scenarios or other settings by simply bringing nodes down and seeing how the clients react.   Final note   On a final note, you might notice the KAFKA_offsets_topic_replication_factor: 3 setting. This setting defines the replication factor of the topic used to store the consumers offset. In the default case this is set to 1. So the consumer offsets for a particular topic will only be present on a single node. If that node goes down, consumers will lose track of where they are, since they can’t update the consumer offsets. This was the main problem in our case. By setting this to 3 we could safely cycle the nodes of the cluster, without it affecting the coonsumers.  ","categories": ["kafka","docker","posts"],
        "tags": [],
        "url": "http://www.smartjava.org/content/setting-up-kafka-cluster-docker-copy/",
        "teaser":null},{
        "title": "Running puppeteer headless with extensions in docker",
        "excerpt":"Puppeteer (https://github.com/puppeteer/puppeteer) is a great way to write scrapers, integration tests, or automate boring tasks and web forms. For a lot of scenarios you can run puppeteer, which wraps Chrome, using Chrome’s headless mode. That way you won’t see a browser window popping up, and chrome just runs as headless background process. Not all the features of Chrome, however, are available when running in this mode. For instance you can’t use extensions when running in this mode.   When you live in the EU, you know about the ‘Cookie Consent’ popups shown in many, many, many pages. If you want to write a scraper it quickly becomes very annoying to add all these extra steps to your puppeteer scripts. The same goes for all the adverts you might one to block and not handle individually. There are all kinds of extensions available that can help you with this.   In this article I’ll show how you can run and configure puppeteer inside a docker container, with an extension enabled. The goal is to use an extension to bypass GPDR cookie consent popups. So we’ll try and scrape a site (http://www.guardian.co.uk) that has a cookie consent banner like this one:      Then by using the correct extension, we should be able to scrape it, as if there was no banner:      To get here we need to take a couple of steps. The first one is to setup our project for this.   The project setup   We can write puppeteer code in different languages, but for this article we’ll just create a simple typescript node application. This script will pull in a web page and show the complete content. We’ll first just make it work, then add the extension, and finally we’ll run it inside a docker container using a headless X server. But first the project setup…   Start with creating a directory and add the following package.json. This contains a minimal configuration for our project   {   \"name\": \"puppeteer-headless\",   \"version\": \"1.0.0\",   \"devDependencies\": {     \"@types/node\": \"^13.7.0\",     \"@types/puppeteer\": \"^2.0.0\",     \"tslint\": \"^5.20.1\",     \"typescript\": \"^3.7.4\"   },   \"dependencies\": {     \"puppeteer\": \"^2.0.0\"   } }   Now run npm install, wait a bit, and we’re done with the basic setup.   $npm update  &gt; puppeteer@2.1.0 install /Users/jos/dev/git/smartjava/articles/puppeteer-headless/node_modules/puppeteer &gt; node install.js  Downloading Chromium r722234 - 116.4 Mb [====================] 100% 0.0s  Chromium downloaded to /Users/jos/dev/git/smartjava/articles/puppeteer-headless/node_modules/puppeteer/.local-chromium/mac-722234 npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN puppeteer-headless@1.0.0 No description npm WARN puppeteer-headless@1.0.0 No repository field. npm WARN puppeteer-headless@1.0.0 No license field.  + puppeteer@2.1.0 added 43 packages from 24 contributors and audited 51 packages in 41.003s found 0 vulnerabilities   And finally add a Typescript config file to the root with some sane defaults:   {     \"compilerOptions\": {         \"module\": \"commonjs\",         \"esModuleInterop\": true,         \"allowSyntheticDefaultImports\": true,         \"target\": \"es2017\",         \"noImplicitAny\": true,         \"moduleResolution\": \"node\",         \"sourceMap\": true,         \"outDir\": \"dist\",         \"baseUrl\": \".\",         \"paths\": {             \"*\": [                 \"node_modules/*\",                 \"src/types/*\"             ]         }     },     \"include\": [         \"src/**/*\"     ] }   With the project done, we need a simple puppeteer script that we’ll use for testing.   Puppeteer script to get a page   Create a src directory in the new project and in that directory create a file called scrape.ts. This file will be our simple scraper  that we’ll extend to run in normal mode in a docker container. For our initial test we’ll start with a script like this:   import puppeteer from 'puppeteer';  class Crawler {          async crawl(url: string, additionalWait: number = 0) : Promise&lt;string&gt; {         const browser = await puppeteer.launch({             headless: false,             args: []         });          // Wait for creating the new page.         const page = await browser.newPage();          // Don't load images         await page.setRequestInterception(true);         page.on('request', request =&gt; {           if (request.resourceType() === 'image')             request.abort();           else             request.continue();         });          // go to the page and wait for it to finish loading         await page.goto(url, { waitUntil: 'networkidle2' });         await page.waitFor(additionalWait);                  // now get all the current dom, and close the browser         const html = await page.content();         browser.close();         return html;     } }  const crawler = new Crawler();  let result = crawler.crawl(\"https://guardian.co.uk\"); console.log(result);   We compile the typescript to javascript and run the result. This will pop up a chrome browser window, load the site, wait for loading to be finished, and finally get the page content and dump that to the console. We can run this like so:   $ tsc &amp;&amp; node ./dist/scrape.js                     ... &lt;div class=\"ad_unit\" style=\"position: absolute; left: 0px; top: 0px; height: 10px; z-index:...   If you look closely (and are in the EU) you’ll see a quick popup of the cookie consent message in the browser:      Note that at this point we can change the headless: false property to true, and you won’t see any chrome windows appearing. But like  we mentioned before. If we go that route, we can’t use extensions to ignore popups, advertisements, or in this case the cookie consent  messages.   Now that we’ve got the basic setup done, we can look at adding the cookie consent blocker, rerun the scraper, and look at the result.   Adding extensions to the puppeteer chrome version   We’re going to use the following extension for this: I don’t care about cookies. Tnis extension blocks a large number of cookie consent popups, and should also block the consent cookies for the website we’re using as an example for this article. Before we can use an extension we need to download it and add it to where our puppeteer instance can find it. For this we use crxextractor, which allows us to download an extension. Use this site to download the extension:      Save the downloaded crx file somewhere (which is just a zip file), and extract it like this in your project directory:    $ mkdir -p extensions/cookieconsent                                       $ cd extensions/cookieconsent                                             $ cp ~/Downloads/extension_3_1_1_0.crx .                   $ unzip extension_3_1_1_0.crx                             Archive:  extension_3_1_1_0.crx warning [extension_3_1_1_0.crx]:  1320 extra bytes at beginning or within zipfile   (attempting to process anyway)   inflating: LICENSE                    inflating: manifest.json              ...  $ rm extension_3_1_1_0.crx      Now we can configure puppeteer to use these extenstions. For this we just need to replace the arguments like this:   const basePath = process.cwd(); const cookieIgnorePath = `${basePath}/extensions/cookieconsent`  class Crawler {          async crawl(url: string, additionalWait: number = 0) : Promise&lt;string&gt; {          // Wait for browser launching.         const browser = await puppeteer.launch({             headless: false,             args: [                 `--disable-extensions-except=${cookieIgnorePath}`,                 `--load-extension=${cookieIgnorePath}`,             ]         });          ...    This will load the extension from the provided path. We need to specify both the --disable-extensions-except and the --load-extension with   the path for the extension to be picked up correctly. Note that now headless needs to be set to false. If set to true the extensions won’t work. The final property we need to set is --no-sandbox, since else chromium refuses to load the extensions.   With this configuration change, the result from this same script now looks like this:      If puppeteer starts without complaining it usually means the extensions are correctly defined, and as you can see in the previous image, the consent popup we saw is now missing. Just what we were aiming for.   So at this point we’ve got a simple scraper, which uses puppeteer to launch a chrome instance with an extension. This needs to be launched with a full UI,  because if we don’t do this, the extensions won’t work. The next step is running this same configuration from a docker file. That way we can just run   this script inside a docker container, and have the docker environment provide the windowing system. In other words, we’ll get the same result as  we had in the last example, but this time, we won’t see a UI.   Setting up the docker container   We’ll start with the latest node docker container, and add an x11 server that’ll provide the UI. We’ll use xfvb for this. Quickly summarized xfvb provides this:      Xvfb is an X server that can run on machines with no display hardware and no physical input devices. It emulates a dumb framebuffer using virtual memory.    Which is great for use together with docker, where we (usually) don’t have access to real display hardware. Now lets look at the Dockerfile we’ll use for this:   FROM node:latest  # update and add all the steps for running with xvfb RUN apt-get update &amp;&amp;\\ apt-get install -yq gconf-service libasound2 libatk1.0-0 libc6 libcairo2 libcups2 libdbus-1-3 \\ libexpat1 libfontconfig1 libgcc1 libgconf-2-4 libgdk-pixbuf2.0-0 libglib2.0-0 libgtk-3-0 libnspr4 \\ libpango-1.0-0 libpangocairo-1.0-0 libstdc++6 libx11-6 libx11-xcb1 libxcb1 libxcomposite1 \\ libxcursor1 libxdamage1 libxext6 libxfixes3 libxi6 libxrandr2 libxrender1 libxss1 libxtst6 \\ ca-certificates fonts-liberation libappindicator1 libnss3 lsb-release xdg-utils wget \\ xvfb x11vnc x11-xkb-utils xfonts-100dpi xfonts-75dpi xfonts-scalable xfonts-cyrillic x11-apps  # add the required dependencies WORKDIR /app COPY node_modules /app/node_modules COPY extensions /app/extensions RUN npm i puppeteer  # Finally copy the build application COPY dist /app/dist  # make sure we can run without a UI ENV DISPLAY :99 CMD Xvfb :99 -screen 0 1024x768x16 &amp; node ./dist/simple.js   Note that this is a rather simple docker file, where we assume we’ve already got the app (or script) build before we start building this docker image. We could of course also add the tsc (and for a real project all the other steps) in this, or use a multi-stage docker build. For this example though, just make sure to run tsc before building the docker image.    $ tsc                                                                 $ docker build .                                                         Sending build context to Docker daemon  282.6MB Step 1/9 : FROM node:latest  ---&gt; f7756628c1ee Step 2/9 : RUN apt-get update &amp;&amp;apt-get install -yq gconf-service libasound2 libatk1.0-0 libc6 libcairo2 libcups2 libdbus-1-3 libexpat1 libfontconfig1 libgcc1 libgconf-2-4 libgdk-pixbuf2.0-0 libglib2.0-0 libgtk-3-0 libnspr4 libpango-1.0-0 libpangocairo-1.0-0 libstdc++6 libx11-6 libx11-xcb1 libxcb1 libxcomposite1 libxcursor1 libxdamage1 libxext6 libxfixes3 libxi6 libxrandr2 libxrender1 libxss1 libxtst6 ca-certificates fonts-liberation libappindicator1 libnss3 lsb-release xdg-utils wget xvfb x11vnc x11-xkb-utils xfonts-100dpi xfonts-75dpi xfonts-scalable xfonts-cyrillic x11-apps  ---&gt; Using cache  ---&gt; 5457211c1624 Step 3/9 : WORKDIR /app  ---&gt; Using cache  ---&gt; 2fdb73b87b0c Step 4/9 : COPY node_modules /app/node_modules  ---&gt; 9f46043b2738 Step 5/9 : COPY extensions /app/extensions  ---&gt; 2908ca5b2877 Step 6/9 : RUN npm i puppeteer  ---&gt; Running in 0b0b2487afd2  &gt; puppeteer@2.1.0 install /app/node_modules/puppeteer &gt; node install.js   Chromium downloaded to /app/node_modules/puppeteer/.local-chromium/linux-722234 npm WARN saveError ENOENT: no such file or directory, open '/app/package.json' npm notice created a lockfile as package-lock.json. You should commit this file. npm WARN enoent ENOENT: no such file or directory, open '/app/package.json' npm WARN app No description npm WARN app No repository field. npm WARN app No README data npm WARN app No license field.  + puppeteer@2.1.0 updated 1 package and audited 181 packages in 35.942s  1 package is looking for funding   run `npm fund` for details  found 0 vulnerabilities  Removing intermediate container 0b0b2487afd2  ---&gt; 5920816d3d53 Step 7/9 : COPY dist /app/dist  ---&gt; d461728772ab Step 8/9 : ENV DISPLAY :99  ---&gt; Running in 7fac3517846d Removing intermediate container 7fac3517846d  ---&gt; 9f34e25f83c8 Step 9/9 : CMD Xvfb :99 -screen 0 1024x768x16 &amp; node ./dist/scrape.js  ---&gt; Running in ccfe7c77ae45 Removing intermediate container ccfe7c77ae45  ---&gt; ced271b000e3 Successfully built ced271b000e3   And with that container build, we can now simply run the container, which will run our script, fire up a full-fledged chromium browser, with our extensions enabled, and output the HTML.   docker run ced271b000e3 | head &lt;!DOCTYPE html&gt;&lt;html id=\"js-context\" class=\"js-on is-modern id--signed-out svg has-flex has-flex-wrap has-fixed has-sticky should-kern\" lang=\"en\" data-page-path=\"/international\"&gt;&lt;head&gt; &lt;!--      __        __                      _     _      _      \\ \\      / /__    __ _ _ __ ___  | |__ (_)_ __(_)_ __   __ _       \\ \\ /\\ / / _ \\  / _` | '__/ _ \\ | '_ \\| | '__| | '_ \\ / _` |        \\ V  V /  __/ | (_| | | |  __/ | | | | | |  | | | | | (_| |         \\_/\\_/ \\___|  \\__,_|_|  \\___| |_| |_|_|_|  |_|_| |_|\\__, |                                                             |___/     Ever thought about joining us?     https://workforus.theguardian.com/careers/digital-development/   And apparently the site we’re using as an example is hiring :) (and no, I’m not affiliated in any way with the Guardian, this is pure coincidence.)   Conclusions   As you can see, once you’ve got a basic script up and running with puppeteer, using it from a docker container in headless mode is really easy. This provides a simple way of running Puppeteer combined with any extension you want, without having to deal with browser windows showing everywhere.  ","categories": ["puppeteer","typescript","posts"],
        "tags": [],
        "url": "http://www.smartjava.org/content/using-puppeteer-in-docker-copy-2/",
        "teaser":null},{
        "title": "Getting started with Rust - part 1",
        "excerpt":"I decided to look a bit into Rust. I’ve been hearing great things about it, the community seems to be (mostly) very inviting and open, and the language in itself seems to have a nice mix of modern features (lambdas, boxed types, extensions), while still allowing for low-level systems programming.  The initial idea was to work through the The rust programming languagebook and write a bit about that. But that quickly became very boring for me, so I just decided to start writing some small tools to see what the language can do and how to use the various features of the language. So for a first program, I wanted to write something useful.   At my current project we work with lots of branches, each feature/bug/ticket has its own branch and with reviews, PRs different projects I sometimes lose track of what I exactly named my branch I was working on. I could of course just check GitHub, or run some magical git command, but this would be a nice small tool to write with Rust. This simple tool should do this:      Look at the current list of git branches in a specific directory.   Determine the hash and date of the last commit of that specific branch.   Print out the list of last commits per branch, ordered by date.   That way I can quickly see on which branch I was working, and which branches were recently updated, without having to switch to each branch, and do git log and such.   Before we look at the code, a very big disclaimer. I haven’t looked at any optimizations yet, this was merely an exercise in Rust to get a working program. So any experienced Rust programmer will probably spot a whole lot of bad practices.   The final result   First, let’s see what the final program looks like:   $ ./rust-git ~/dev/git/some/project Using /Users/jos/dev/git/some/project as git repository to analyse. 2020-02-10 15:18:22 UTC : d3161dedcf7a99d3991b0492fb08285b245c93eb : dev#### 2020-02-10 11:42:47 UTC : 0b75ba1d2f745ea6c0950646b92ec8480684472d : set########################## 2020-02-10 07:09:51 UTC : cdf7bbc0aa090d87ca8bcdebbe0bb1df105ca1c2 : cha####################### 2020-02-07 13:32:56 UTC : d52cdd456b2a1c2fd406a46601bad2d785a32f1e : rem######################## 2020-02-06 12:36:11 UTC : edc540c8c79d9fff3e713bba4f6514d018a69df7 : sen################################ 2020-02-05 13:57:05 UTC : fbdf684693a1791fecccc37d0c894fab619831fa : fix###################### 2020-01-31 10:32:18 UTC : 8d35760cf0cc588b25d6566d4ef9771d172a23f4 : set################### 2020-01-27 10:22:35 UTC : 91ea1fabf9f7e6058a3e9f3e6b69fb87930786da : not############# ...   It actually uses color, so it looks much better when you actually run it. I’ve hashed the biggest part of the branches for this example, since I’m testing it on a work project. But you probably get the idea. It shows the branches, when was the last commit, and what is the hash of that commmit.      Nothing too special. But fun to create. So without further ado, the code.   The code   We’ll start with the Toml. Using crates wasn’t so different as using any of the package managers like npm, Gradle, Maven, SBT and such. So for this sample I ended up with this:   [package] name = \"rust-git\" version = \"0.1.0\" authors = [\"Jos Dirksen &lt;jos.dirksen@gmail.com&gt;\"] edition = \"2018\"  [dependencies] git2 = \"0.10\" chrono = \"0.4.10\" libc = \"0.2.66\" colored = \"1.9.2\"   Basically I used the dependencies for this:      git2: Provides access to git functionality by calling into libgit2.   chrono: For dealing with and converting timestamp.   libc: For an apparent issue with Rust, where you can’t pipe the output of a program to something like head or less   colored: Which provides extension functions (which I didn’t knew Rust has), to colorize output.   This doesn’t look like much, but when doing typescript or react you often get a large number of transitive dependencies as well. So I ran into cargo-tree, where you can create tree of these dependencies:   $ cargo-tree tree             rust-git v0.1.0 (/Users/jos/dev/git/smartjava/articles/rust-git) ├── chrono v0.4.10 │   ├── num-integer v0.1.42 │   │   └── num-traits v0.2.11 │   │       [build-dependencies] │   │       └── autocfg v1.0.0 │   │   [build-dependencies] │   │   └── autocfg v1.0.0 (*) │   ├── num-traits v0.2.11 (*) │   └── time v0.1.42 │       └── libc v0.2.66 ├── colored v1.9.2 │   ├── atty v0.2.14 │   │   └── libc v0.2.66 (*) │   └── lazy_static v1.4.0 ├── git2 v0.10.2 │   ├── bitflags v1.2.1 │   ├── libc v0.2.66 (*) │   ├── libgit2-sys v0.9.2 │   │   ├── libc v0.2.66 (*) │   │   ├── libssh2-sys v0.2.14 │   │   │   ├── libc v0.2.66 (*) │   │   │   ├── libz-sys v1.0.25 │   │   │   │   └── libc v0.2.66 (*) │   │   │   │   [build-dependencies] │   │   │   │   ├── cc v1.0.50 │   │   │   │   │   └── jobserver v0.1.21 │   │   │   │   │       └── libc v0.2.66 (*) │   │   │   │   └── pkg-config v0.3.17 │   │   │   └── openssl-sys v0.9.54 │   │   │       └── libc v0.2.66 (*) │   │   │       [build-dependencies] │   │   │       ├── autocfg v1.0.0 (*) │   │   │       ├── cc v1.0.50 (*) │   │   │       └── pkg-config v0.3.17 (*) │   │   │   [build-dependencies] │   │   │   ├── cc v1.0.50 (*) │   │   │   └── pkg-config v0.3.17 (*) │   │   ├── libz-sys v1.0.25 (*) │   │   └── openssl-sys v0.9.54 (*) │   │   [build-dependencies] │   │   ├── cc v1.0.50 (*) │   │   └── pkg-config v0.3.17 (*) │   ├── log v0.4.8 │   │   └── cfg-if v0.1.10 │   └── url v2.1.1 │       ├── idna v0.2.0 │       │   ├── matches v0.1.8 │       │   ├── unicode-bidi v0.3.4 │       │   │   └── matches v0.1.8 (*) │       │   └── unicode-normalization v0.1.12 │       │       └── smallvec v1.2.0 │       ├── matches v0.1.8 (*) │       └── percent-encoding v2.1.0 └── libc v0.2.66 (*)   which isn’t that shocking. Lot’s of build-dependencies and a reasonable amount of other crates. Anyway, cargo so far just worked, which was nice, and it feels really quick. But, of course, this is really just a very simple project, so that’s probably to be expected.   Now the main code… and like I already mentioned. This is my first step into Rust, so bare with me.   use git2::{Repository, BranchType, Branch}; use chrono::{Utc, TimeZone, DateTime}; use std::option::Option as Option; use colored::*; use std::env; use std::process::exit;  struct BranchCommitTime {     branch_name: String,     last_commit: DateTime&lt;Utc&gt;,     hash: String }  fn main() {      // to allow piping the result to other programs     //https://github.com/rust-lang/rust/issues/46016     unsafe {         libc::signal(libc::SIGPIPE, libc::SIG_DFL);     }      let args: Vec&lt;String&gt; = env::args().collect();     match args.len() {         1 =&gt; {             println!(\"{}\",\"Using current dir as git repository to analyse.\".green());             show_branches_and_time(env::current_dir().unwrap().to_str().unwrap());         },         2 =&gt; {             println!(\"{}{}{}\", \"Using \".green() ,args[1].green(), \" as git repository to analyse.\".green());             show_branches_and_time(args[1].as_str());         },         _ =&gt; {             println!(\"{}\", \"Unexpected number of arguments\".red());             exit(1);         }     };  }  fn show_branches_and_time(dir: &amp;str) {     let repo = match Repository::open(dir) {         Ok(repo) =&gt; repo,         Err(e) =&gt; panic!(\"failed to init: {}\", e),     };      // get all the local branches, and for each get the name and last commit time     // and return them as a new list of BranchCommitTime structs     let branches_list = get_all_local_branches(&amp;repo);     let bct_list = branches_list.filter_map(|branch| {          // first get the name, or ignore the field when the name can't be determined         return match branch {             Ok((branch, _)) =&gt; {                 let branch_name = get_branch_name(&amp;branch);                 let last_commit = get_branch_last_commit(&amp;branch, &amp;repo);                  // flatmap these options to create an Option&lt;BranchCommitTime&gt;                 let bct = branch_name                     .and_then(|n| last_commit.map(|t|                         BranchCommitTime { branch_name: n, last_commit: t.1, hash: t.0 }                     ));                  bct             }             Err(_) =&gt; None         };     });      // collect the iterator in a vector so we can sort it. This has to     // be a mutable one, since we do sorting in place. Finally print out     // the sorted list to console.     let mut bct_v: Vec&lt;_&gt; = bct_list.collect();     bct_v.sort_by(|a, b| b.last_commit.cmp(&amp;a.last_commit));     bct_v.iter().for_each(|bc| {         // for article purposes we'll hash the names         let hashed_name = hash_string(bc.branch_name.as_str());         println!(\"{} : {} : {}\", bc.last_commit.to_string().yellow(), bc.hash, hashed_name.blue());         }     ); }  /// replace all the characters in a string, except the first 3 fn hash_string(name: &amp;str) -&gt; String {     let mut hashed_name = String::from(\"\");     for (pos, e) in name.chars().enumerate() {         match pos {             0..=2 =&gt; hashed_name.push(e),             _ =&gt; hashed_name.push('#')         };     }     return hashed_name }  /// We get the name, and convert it to a string, so we don't /// run into ownership issues, or need to pass the lifetime around. fn get_branch_name(branch: &amp;Branch) -&gt; Option&lt;String&gt; {     let name = match branch.name() {         Ok(r) =&gt; r.map(|n| String::from(n)),         Err(_) =&gt; None     };      return name; }  /// get the last commit time from a branch, if it fails return none fn get_branch_last_commit(branch: &amp;Branch, repo: &amp;Repository) -&gt; Option&lt;(String,DateTime&lt;Utc&gt;)&gt; {     let p = branch.get().target().and_then(|oid| {         let commit = repo.find_commit(oid);          let t = commit.map(|c|             (c.id().to_string(), Utc.timestamp(c.time().seconds(), 0))         );          let res = t.ok();         res     });     return p; }  /// Get all the local branches for the passed in repository fn get_all_local_branches(repo: &amp;Repository) -&gt; git2::Branches {     let branches_list = match repo.branches(Option::Some(BranchType::Local)) {         Ok(br) =&gt; br,         Err(e) =&gt; panic!(\"failed to init: {}\", e),     };      return branches_list; }   Nothing to special, but it allowed me to learn a couple of interesting concepts from Rust. The basic flow is shown in the code. For the end of this article, I’d like to listen a couple of small conclusions I made so far regarding Rust.   The conclusions after one simple Rust project   Just a couple of points from my first project. In no particular order:      Rustc is very nice with exceptions and help how to solve it: I really like how the rustc compiler communicates errors. It not only tells you what is wrong, but also explains why it is, and what you might need to do to solve it. I don’t understand all the proposals yet, but it really helps with learning how the language, and its specific feature work (or should work).   IntelliJ as editor has some rough edges: I’ve used the Intellij Rust plugin and that doesn’t feel really polished yet. I’ve been slowly moving away from IntelliJ for a couple of languages (Scala, Typescript and Javascript) to Visual Studio Code, which feels much leaner and quicker. I probably need to dive into the options available for Rust though.   Option, Result feels natural: I really like that rust provides support for boxed types like Result and Option (and probably a lot more I haven’t looked at) out of the box. It really simplifies error handling, and feels much better than how it is done in Golang (at least for me).   Missing do or for-comprehension: But even though Result and Option are great, I do miss the for-comprehensions from Scala or Haskells do notation. After a quick search I already found a couple of libraries that add this syntactic sugar, so might look into that in the future as well. Kotlin, which I use daily at work, also doesn’t provide such a construct. But the Arrow-kt library provides a very nice extension for this to the core language.   Pattern matching: I’ve mostly used pattern matching in Scala, where I really like it. In Kotlin the support isn’t that great, but in Rust so far it feels quite nice. I’m still struggling a bit with the syntax at certain points, but so far I’m quite liking it.   Borrow, ownership, lifetimes: Apparently returning a &amp;str is something complex. I’ve avoided the Cow, Borrow, Into, lifetime annotation stuff so far. After reading a bit, I see the need for it, and it looks like Rust has chosen a great approach. So in a future project I’ll dive into it in more depth.   Odd lambda syntax: And finally, the lambda syntax takes some getting used to. It’s quite different from most languages I’ve used, but that’s probably just because all is new for me.   All in all, I’m really pleasantly suprised by this language. It’s been really fun creating this simple tool, and I never really felt too much constrained by the language. While I struggled with some parts, the documentation and compiler hints (and a good amount of trial-and-error) got me where I wanted. Now just to think of something to build for the next experiment….  ","categories": ["rust","git","posts"],
        "tags": [],
        "url": "http://www.smartjava.org/content/getting-started-with-rust-part-1/",
        "teaser":null},{
        "title": "Getting started with Rust - part 2",
        "excerpt":"In the previous article I started experimenting with Rust and build a very simple extension to git, which helps me remember the most recent branches I’ve been working on, or did PRs for. For this  second article, I wanted to do look at how you can do network programming in Rust.   (If you just want to skip to the code: https://github.com/josdirksen/dns-proxy-rusts)   For this, and the next article, since I’ve only done the basic so far, we’ll look at the steps needed to create a DNS proxy in Rust. This DNS proxy in the end will have the following features:      Proxy all incoming DNS requests to a real DNS server, and respond back to the client with the response from the real server.   Keep metrics of how often a specific host was queried.   Allow the use of /etc/hosts with wildcards as a source for A-Record queries.   And probably some other stuff, which I’ll think of when implementing.   One of reasons I want something simple like this is that at work we use wildcard certificates to connect specific domains to appications. So ui-dev.smartjava.org, would route to a specific instance, while ui-test.smartjava.org would route to another one. In some cases though, for instance when debugging, or testing k8s or kafka configs, it is useful to route these domains to localhost, or another host completely. We can do this by adding all the domains, including the subdomains to your /etc/hosts file, which solves part of the problem, but that only works when working directly on your local machine. If you start containers in docker, they won’t know about these changes. So with a simple DNS proxy, we can have docker instances use the information from   the docker host for routing. And of course I’m just interested to see what kind of DNs queries my machine is making.   For this first article we’ll focus on the first of these points, and try to get the basic application up and running. As a more technical / Rust goal I wanted to mostly learn about:      Network programming   async/await   Indirectly about the whole ownership model   Another change I made in regards to the previous article, was that I switched editors a couple of times. Before going into the details on the code I’ve written, a quick note on that.   Visual Studio Code as Rust Editor   For my day job I do a lot of Kotlin, and you don’t really have much choice of editor. You just use Intellij Idea. Which is a great editor, but can sometimes become slow and unresponsive, since it’s got so many features, checks and other smart things running in the background, analyzing your code. In the previous article, I used the Intellij Rust plugin to write the git extension, but for this DNS example I’ve switched to using Visual Studio Code.      Code has different plugins you can use for editing Rust, and I tried the following two:      The official RLS Extension:   Rust analyzer Extension:   I started off with the offical extension, and initially it worked great. I got code completion, could quickly skip through source code, and had nice inline comments. It also provided integration with the Code tasks system to easily run tasks ons cargo. However, when I started adding the async/await stuff, it just stopped working. I still had some code highlighting, and it showed the errors (the squiggly lines), but there was no code completion anymore, and jumping to implementations also pretty much stopped working. Apparently this is a know issue, and is caused by the macros from Tokio, which I used to add async support to my code.  At that point I, momentarily, gave up on Rust in Code and switched back to IntelliJ. But, unfortunately, with the Tokio stuff added, IntelliJ was giving the same kind of errors. So back to Visual Studio Code, and then I tried the Rust Analyzer extension. This one was actually working quite ok with the async stuff, so I’ve written the rest of the product with that extension.   ( There is also Another rust with RLS extension, on the marketplace, but I didn’t test that one.)   I’ll check back in the near future how the various extensions evolve, but at least I’ve now got a light weight solution to work with in Visual Studio Code, which works quite nice with the tasks in Cargo.   Quick introduction on UDP   For this first version we’re just going to create a DNS Proxy which works with UDP. You can also run DNS queries over TCP, so we’ll add that in the next article. UDP is a connection-less protocol so we need to take that into account when creating our service. The following image from this slidedeck (Introduction into DNS), provides a nice explanation of what we need to do:      So we need to write the following:      An UDP server which can receive datagrams from a DNS client (we’ll just use dig as the client)   When we receive an UDP request, we use an UDP client to forward the request to an external DNS server.   When that external server responds with a DNS answers, we forward it back to the initial UDP client (dig).   First of, lets look at the libraries used for this proxy   Cargo configuration   We use the following cargo.toml.   [package] name = \"rust-dns\" version = \"0.1.0\" authors = [\"Jos Dirksen &lt;jos.dirksen@gmail.com&gt;\"] edition = \"2018\"  [dependencies] dns-parser = \"0.8.0\" simplelog = \"^0.7.4\" log = \"0.4.8\" tokio = { version = \"0.2\", features = [\"full\"] }   We’ve got the following libraries in here:      dns-parser: This library can parse incoming byte streams to an UDP packet. We don’t do much with this yet, just use it for some logging to see what is happening.   simplelog and log: These two libraries provide logging functionality. The log library is an abstraction on top of the different logging libraries out there, and with simplelog we just get a simple logger, with colors.   tokio: Rust has gotten async/await support a couple of months ago, and to use this you need a library that provides the execution runtime. Tokio is the best known one for this.   Before I’ll show my simple code, a quick sidestep on the whole async/await stuff.   Async/await   With async/await we get a lot of syntactic sugar on how to work with futures. So we can write asynchronous code, which can be read as sequential code. Rust has a complete async handbook (which I’ve only skimmed through so far) explaining this concept.   Basically what it allows is to write code like this:   async fn function_1() { ... } async fn function_2() { ... }  #[tokio::main] async fn main() -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {     function_1().await?     function_1().await?     Ok(()) }   I hadn’t seen then question mark thingy ? at the end of a function call beforehand, but it appears to be some syntactic sugar for dealing with Result types. It allows us to just chain together functions that return a Result without having to and_then or pattern match the results. Basically a minimal version of Haskell’s do-notation or Scala’s for-comprehensions.   Now let’s look at the code   The code   The code in itself isn’t that large, so I’ll first show the complete project so far with all its comments, and below that explain some of the stuff I was struggling with, or found interesting.   use dns_parser::rdata::a::Record; use dns_parser::{Builder, Error as DNSError, Packet, RData, ResponseCode}; use dns_parser::{QueryClass, QueryType}; use io::Result as ioResult; use log::*; use simplelog::{Config, LevelFilter, TermLogger, TerminalMode}; use std::error::Error; use std::net::SocketAddr; use std::str; use tokio::net::UdpSocket; use tokio::prelude::*;  /// /// Simple dns proxy. For the first version, we'll just listen to /// an UDP socket, parse an incoming DNS query and write the output /// to the console. /// /// Next steps: ///   Support TCP protocol ///   Keep track of hosts (and subhosts queried) ///   Allow resolving through local host file ///   Add error handling /// #[tokio::main] async fn main() -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {     init_logging();      info!(\"Starting server, setting up listener for port 12345\");      // chain the await calls using '?'     let listen_socket = create_udp_socket_receiver(\"0.0.0.0:12345\").await?;     let sender_socket = create_udp_socket_sender().await?;      start_listening_udp(listen_socket, sender_socket).await?;     Ok(()) }  /// /// Create an async UDP socket. /// async fn create_udp_socket_receiver(host: &amp;str) -&gt; ioResult&lt;UdpSocket&gt; {     debug!(\"initializing listener udp socket on {}\", host);     let socket = UdpSocket::bind(&amp;host).await?;     return Ok(socket); }  /// /// Create the sender to forward the UDP request /// async fn create_udp_socket_sender() -&gt; ioResult&lt;UdpSocket&gt; {     let local_address = \"0.0.0.0:0\";     let socket = UdpSocket::bind(local_address).await?;     let socket_address: SocketAddr = \"8.8.8.8:53\"         .parse::&lt;SocketAddr&gt;()         .expect(\"Invalid forwarding address specified\");     socket.connect(&amp;socket_address).await?;     debug!(\"initializing listener udp socket on {}\", local_address);     return Ok(socket); }  /// /// Asynchronously run the handling of incoming messages. Whenever something comes in on the socket_rcv_from /// we try and convert it to a DNS query, and log it to the output. UDP is a connectionless protocol, so we /// can use this socket to send and receive messages to our client and other servers. /// async fn start_listening_udp(     mut listen_socket: UdpSocket,     mut sender_socket: UdpSocket, ) -&gt; ioResult&lt;()&gt; {     // 1. Wait for a request from a DNS client.     // 2. Then forward the request to a remote dns server     // 3. The response from the remote DNS server is then send back to the initial client.     loop {         let (request, peer) = receive_request(&amp;mut listen_socket).await?;         let forward_response = forward_request(&amp;mut sender_socket, &amp;request[..]).await?;         listen_socket.send_to(&amp;forward_response[..], &amp;peer).await?;     } }  /// /// Forward a request to the provided UDP socket, and wait for an answer. /// async fn forward_request(sender_socket: &amp;mut UdpSocket, request: &amp;[u8]) -&gt; ioResult&lt;Vec&lt;u8&gt;&gt; {     let mut buf = [0; 4096];     info!(\"Forwarding to target DNS\");     sender_socket.send(request).await?;     let (amt, _) = sender_socket.recv_from(&amp;mut buf).await?;     let filled_buf = &amp;mut buf[..amt];     // let answer_received = parse_incoming_stream(filled_buf).expect(\"Something went wrong\");      let v = Vec::from(filled_buf);     return Ok(v); }  /// /// Receive a request on the reference to the socket. We're not the owner, but we need a mutable /// reference. The result is a Vec, and the response address. Both are copies, which can be safely /// modified. /// async fn receive_request(from_socket: &amp;mut UdpSocket) -&gt; ioResult&lt;(Vec&lt;u8&gt;, SocketAddr)&gt; {     let mut buf = [0; 4096];      let (amt, peer) = from_socket.recv_from(&amp;mut buf).await?;     let filled_buf = &amp;mut buf[..amt];     // info!(\"Received length {}\", amt);     // let packet_received = parse_incoming_stream(filled_buf).expect(\"Something went wrong\");     // info!(\"Received package {:?}\", packet_received);      let v = Vec::from(filled_buf);     return Ok((v, peer)); }  /// /// Parse the packet and return it /// // fn parse_incoming_stream(incoming: &amp;[u8]) -&gt; Result&lt;Packet, DNSError&gt; { //     let pkt = Packet::parse(incoming)?; //     return Ok(pkt); // }  /// /// Initializes the logging library. We just simply log to console /// fn init_logging() {     TermLogger::init(LevelFilter::Debug, Config::default(), TerminalMode::Mixed).unwrap(); }   Let’s start with the main function. In our case this creates a client UDP socket, and a server UDP socket.   #[tokio::main] async fn main() -&gt; Result&lt;(), Box&lt;dyn Error&gt;&gt; {     init_logging();      info!(\"Starting server, setting up listener for port 12345\");      let listen_socket = create_udp_socket_receiver(\"0.0.0.0:12345\").await?;     let sender_socket = create_udp_socket_sender().await?;      start_listening_udp(listen_socket, sender_socket).await?;     Ok(()) }   I’ll skip the implementations of the two create functions, which internally use the Tokio provided UDP network sockets. We use the Tokio provided ones, since they already work with async and await, so we don’t have to create and implement our own Future implementation on top of setting up the connections. When both of the sockets have been created, we pass them into the start_listening_udp function, which will continuously wait for new datagrams. Tokio provides an enhanced main function, which we can define as async itself, so we don’t have to do any blocking here.   In the function above you can also see how await together with the ? function really allows for easily readable asynchronous code. The next interesting piece of code is the start_listening_udp function (in the future I want something similar for the TCP functionality):   async fn start_listening_udp(     mut listen_socket: UdpSocket,     mut sender_socket: UdpSocket, ) -&gt; ioResult&lt;()&gt; {     loop {         let (request, peer) = receive_request(&amp;mut listen_socket).await?;         let forward_response = forward_request(&amp;mut sender_socket, &amp;request[..]).await?;         listen_socket.send_to(&amp;forward_response[..], &amp;peer).await?;     } }   This function is a never-ending loop on top of three async functions. First we wait to receive a request on the socket our server is listening on. Once we get that request (e.g a message from dig), we call the forward_request function. This function will forward the request to a real DNS server, wait for a response, and return with the response. Finally we send the response back to our original client (dig) and wait for new requests to come in. The nice thing about this code, is that it is very easy to reason about. The whole await/async stuff is nicely hidden from view, and we can sequence calls by usinig the question mark. I was impressed that a low-level language like Rust had such advanced constructs.   Let’s look at one of the functions, since the others are pretty much the same:   async fn forward_request(sender_socket: &amp;mut UdpSocket, request: &amp;[u8]) -&gt; ioResult&lt;Vec&lt;u8&gt;&gt; {     let mut buf = [0; 4096];     info!(\"Forwarding to target DNS\");     sender_socket.send(request).await?;     let (amt, _) = sender_socket.recv_from(&amp;mut buf).await?;     let filled_buf = &amp;mut buf[..amt];     let v = Vec::from(filled_buf);     return Ok(v); }   Here we have the same couple of await? steps. This time to make a call, and wait for a response. Easy right?   So finally when we run this, we get the following when we make a call:      Just like the previous project, it’s been really a fun process of doing this.   Conclusions   So what was easy and what wasn’t?      I’m still struggling with ownership, and when Rust is cleaning up variables used in functions. I’m slowly starting to understand it, but it does cause some strange and error messages when developing.   On those error messages. Whiile they usually are really helpful, sometimes it still takes a lot of time to actually solve the problem. This isn’t so much a Rust issue, as it is an issue for me that I’m still learning about specific concepts.   And it is always good to finish with a positivbe :) The whole async/await feels nice, works great and combined with the ? thingy, results in compact and readable code.   ","categories": ["rust","posts","dns"],
        "tags": [],
        "url": "http://www.smartjava.org/content/getting-started-with-rust-part-2-copy/",
        "teaser":null},{
        "title": "Standalone Apollo Graphql Websocket Subscriptions",
        "excerpt":"This is just a quick article on how to create a simple standalone Apollo client to test Graphql subscriptions. I needed  something like this when we ran into some strange issues at work, where our Graphql backend (Kotlin) and Apollo React frontend  stopped working, or got into a refresh loop. To easily test this I wanted something I could script to easily run a couple of  scenarios without having to use a browser.   I couldn’t really find a good example, though, how to do this, so after looking through some samples, and libraries, I came up with this:   import { ApolloClient } from 'apollo-client'; import { InMemoryCache } from 'apollo-cache-inmemory'; import gql from 'graphql-tag'; import { WebSocketLink } from \"apollo-link-ws\"; import { SubscriptionClient } from \"subscriptions-transport-ws\"; import ws from 'ws';  let args = process.argv.slice(2); if (args.length != 1) {     console.log(`Expected a single arguments ${process.argv} as query`)     process.exit(1); } let query = args[0];  // the endpoint we're connecting to const GRAPHQL_ENDPOINT = \"wss://some/endpoint/for/subscriptions\";  // setup a subscription client for that endpoint const subscriptionClient = new SubscriptionClient(GRAPHQL_ENDPOINT, {     reconnect: true }, ws);  // using the subscription client to set up a websocket link // which can be passed into the apollo client const link = new WebSocketLink(subscriptionClient); let cc = new ApolloClient({     link: link,     cache: new InMemoryCache() });  // The subscription we want to follow, just parse it from the string const asGql = gql`${query}`  // subscribe const s = cc.subscribe({     query: asGql })  s.subscribe({     next: ({ data }) =&gt; console.log(data) });    And the following package.json:  {   \"name\": \"standalone-apollo-client\",   \"version\": \"1.0.0\",   \"description\": \"\",   \"main\": \"index.js\",   \"author\": \"\",   \"license\": \"ISC\",   \"devDependencies\": {     \"typescript\": \"^3.8.2\"   },   \"dependencies\": {     \"@oitmain/apollo-client-standalone\": \"^1.0.0\",     \"apollo-link-ws\": \"^1.0.19\",     \"subscriptions-transport-ws\": \"^0.9.16\"   } }   This small node script takes a graphql subscription as its argument, and by just changing the callback function, you can quickly test your subscriptions. In the end it really helped me determining what was wrong (which in the end wasn’t in the FE or the BE, with an obscure HAProxy configuration).  ","categories": ["typescript","apollo","websocket","graphql","node"],
        "tags": [],
        "url": "http://www.smartjava.org/content/standalone-apollo-client/",
        "teaser":null},{
        "title": "My 10 goto terminal commands",
        "excerpt":"There are a lot of useful terminal/shell commands which can make your live a lot easier. I do a lot of backend development, and there is a set of commands which I use a lot during development, testing or operations work. So maybe this list can introduce you to a command that can make your live a little bit easier.   jq   jq is a command which can be used to parse and transform json. I use it a lot in scripts to parse and extract information from APIs, or to quickly format a JSON file. The basic syntax is very easy, but it provides a very large set of operations and command to manipulate the JSON.      You can find more information about this tool here: JQ on GitHub   Autojump   A little known command autojump, which keeps tracks of your most used directories, so you can quickly jump between directories. There is a convenience wrapper called j, so I hardly have to use cd to jump between directories anymore, and can just use j instead.      You can find more information about this tool here: Autojump on GitHub   Date   Often during work I need to quickly get the current unix epoch timestamp, an ISO formatted date time or quickly determine what time a timestamp actually is. There are a number of websites out there that do this, but I usually just use the specific date commands (and since they are in my history, I usually don’t have to keep the exact parameters in mind). I often use the following:      date +%s: Print out the current unix epoch timestamp in seconds   date -u +\"%Y-%m-%dT%H:%M:%SZ\": Print out an iso-8601 timestamp as UTC   or date --iso-8601=seconds: Print out an iso-8601 timestamp using local timezone   date -d@1550841014: Convert epoch seconds back to a readable date      uuid-Gen   At my last couple of projects we used uuids as unique ids for entities. While this works great, it sometimes gets annoying when writing tests or preparing some data. Luckily we can just use another command line tool to quickly generate uuids.      uuidgen: Just creates a uuid with all uppercase characters. Not all tools though work correctly with upper case characters for the UUID strings.   uuidgen | tr \"[:upper:]\" \"[:lower:]\": Does the same as the previous command, but this time generates them as lower case characters.      generate passwords   Another tool which is regularly need, especially when generating users, is a simple password generator. You can use online tools for this, or tools like lastpass. Usually, though, it is quicker to just open a terminal and hit the following command a couple of times: openssl rand -base64 12:      ncal   Often I need to have quick access to a calendar. Either to plan something, look at what week it is, or just to determine on which day a specific date falls. For an actual agenda, I use google calendar, but I don’t like having to open gmail, or the Mac calendar application, for these simple things. Luckily there are of course CLI apps for that.      And this is of course a great reason to use lolcat. 😊   hub   Most developers use git to store their data, and often also work with github to store their code. While GitHub works great for most work, and with the normal git command you can do most of the daily jobs, there are some things missing. The biggest thing I miss when working with gitHub, though, is that you can’t simply create or manage PRs from the command line. You have to open the UI, and do it from there. Luckily, though, there is a command called hub which fills in this missing gap.      More information about hub can be found here: https://hub.github.com/   curl   This command is one we’ve already seen in the beginning of this article when we were discussing jq, and one which most people will already know. I still list it, since I use it so often, especially in combination with browsers like Firefox and Chrome allowing you to copy network requests as curl commands, makes this a very useful tool. I often use it together with Insomnia, which I use as a general REST client, and which also allows you to copy specific requests as curl commands.      htop   I develop on a mac, and sometimes need to determine why my system is slowing down to a crawl (answer: usually IntelliJ Idea), or when I’m running a large set of services, I need to monitor what is happening. For this you could use normal top, the Activity Monitor, or just ps, but I often have a simple terminal window open with htop:      vi   You either love or hate vi(m), and I tend to really like it. It’s present on most systems, once you’re used to it, provides a lot of quick commands for editing text files, and is often defined as the standard editor for git. There are thousands of extensions to customize it, use it as a complete IDE, or create an enhanced editor using SpaceVim      cd -, git checkout -   The last command I wanted to show isn’t a real command, but more of a way of navigating directories and branches. I had been doing linux and mac for a couple of years before I learned about this, and it really makes switching directories and switches branches a lot easier and quicker. All you need to remember is that you can quickly switch to the previous directory by just doing cd -, and switching to the previous git branch by doing git checkout -.      Honorable mention: terminalizer   As an honorable mention I’d like to add terminalizer, with this tool (get it from here), you can very quickly capture a terminals input and output, and generate the Gifs you see in this page. You can even use it to capture terminalizer itself in action.     ","categories": ["command line","bash","zsh","posts","top"],
        "tags": [],
        "url": "http://www.smartjava.org/content/my-10-goto-terminal-commands/",
        "teaser":null},{
        "title": "Running a minimal Kafka instance on K8S",
        "excerpt":"A couple of days ago I ran into an issue with our hosted Kafka environment, and we had to give a demo. So I was looking for a quick way to run a simple ephemeral Kafka instance on our kubernetes cluster. I looked around a bit, and couldn’t find one working out of the box, so I adapted one I found and which almost worked (https://dzone.com/articles/ultimate-guide-to-installing-kafka-docker-on-kuber) which used the standard docker images from here   Setup   The complete setup will consist out of:      A single deployment for zookeeper. This will create a single zookeeper pod, with the name zoo1   A single deployment for kafka. This will create a single kafka pod, with the name kafka-broker0   two services; one that exposes zookeeper, and one that exposes the kafka-broker   a final deployment that also adds kafkacat so we can quickly test whether our broker is working and accessible   --- apiVersion: v1 kind: Service metadata:   name: zoo1   labels:     app: zookeeper-1 spec:   ports:     - name: client       port: 2181       protocol: TCP     - name: follower       port: 2888       protocol: TCP     - name: leader       port: 3888       protocol: TCP   selector:     app: zookeeper-1 --- apiVersion: v1 kind: Service metadata:   name: kafka-service   labels:     name: kafka spec:   ports:     - port: 9092       name: kafka-port       protocol: TCP   selector:     app: kafka     id: \"0\" --- kind: Deployment apiVersion: extensions/v1beta1 metadata:   name: zookeeper-deployment-1 spec:   template:     metadata:       labels:         app: zookeeper-1     spec:       containers:         - name: zoo1           image: digitalwonderland/zookeeper           ports:             - containerPort: 2181           env:             - name: ZOOKEEPER_ID               value: \"1\"             - name: ZOOKEEPER_SERVER_1               value: zoo1 --- kind: Deployment apiVersion: extensions/v1beta1 metadata:   name: kafka-broker0 spec:   template:     metadata:       labels:         app: kafka         id: \"0\"     spec:       containers:         - name: kafka           image: wurstmeister/kafka           ports:             - containerPort: 9092           env:             - name: KAFKA_ADVERTISED_PORT               value: \"9092\"             - name: KAFKA_ADVERTISED_HOST_NAME               value: kafka-service             - name: KAFKA_ZOOKEEPER_CONNECT               value: zoo1:2181             - name: KAFKA_BROKER_ID               value: \"0\"             - name: KAFKA_CREATE_TOPICS               value: sample.topic:1:1 --- kind: Deployment apiVersion: extensions/v1beta1 metadata:   name: kafka-cat spec:   template:     metadata:       labels:         app: kafka-cat     spec:       containers:         - name: kafka-cat           image: confluentinc/cp-kafkacat           command: [\"/bin/sh\"]           args: [\"-c\", \"trap : TERM INT; sleep infinity &amp; wait\"]    Starting Kafka   To deploy it simple save this configuration as kubernetes-kafka-ephemeral.yml:   $ kubectl apply -f ./kubernetes-kafka-ephemeral.yml service/zoo1 created service/kafka-service created deployment.extensions/zookeeper-deployment-1 created deployment.extensions/kafka-broker0 created deployment.extensions/kafka-cat created   And looking at the resources you’ll see that the correct items have been created (the kafka broker might restart if zookeeper isn’t up yet, but that’ll resolve itself):     $ kubectl get services NAME            TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE kafka-service   ClusterIP   10.105.141.140   &lt;none&gt;        9092/TCP                     72s kubernetes      ClusterIP   10.96.0.1        &lt;none&gt;        443/TCP                      49m zoo1            ClusterIP   10.108.158.62    &lt;none&gt;        2181/TCP,2888/TCP,3888/TCP   72s   $ kubectl get pods  NAME                                      READY   STATUS    RESTARTS   AGE kafka-broker0-677f5b7ccf-gsstk            1/1     Running   0          76s kafka-cat-84b9cfdf45-x9wk7                1/1     Running   0          76s zookeeper-deployment-1-684478678c-d8m5m   1/1     Running   0          76s   $ kubectl get deployments NAME                     READY   UP-TO-DATE   AVAILABLE   AGE kafka-broker0            1/1     1            1           80s kafka-cat                1/1     1            1           80s zookeeper-deployment-1   1/1     1            1           80s   Using Kafka   kafkacat runs in a pod which won’t exit, so you can just exec into the pod, and see if everything works. Easiest way to test is to just open two shells and run the kafkacat commands (kafkacat -P -b kafka-service:9092 -t test and kafkacat -b kafka-service:9092 -t test):     ","categories": ["kafka","kubernetes","posts","k8s"],
        "tags": [],
        "url": "http://www.smartjava.org/content/minimal-kafka-instance-for-k8s/",
        "teaser":null}]
